<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Debugging · RxInfer.jl</title><meta name="title" content="Debugging · RxInfer.jl"/><meta property="og:title" content="Debugging · RxInfer.jl"/><meta property="twitter:title" content="Debugging · RxInfer.jl"/><meta name="description" content="Julia package for automated Bayesian inference on a factor graph with reactive message passing"/><meta property="og:description" content="Julia package for automated Bayesian inference on a factor graph with reactive message passing"/><meta property="twitter:description" content="Julia package for automated Bayesian inference on a factor graph with reactive message passing"/><meta property="og:url" content="https://docs.rxinfer.com/stable/manuals/debugging/"/><meta property="twitter:url" content="https://docs.rxinfer.com/stable/manuals/debugging/"/><link rel="canonical" href="https://docs.rxinfer.com/stable/manuals/debugging/"/><script async src="https://www.googletagmanager.com/gtag/js?id=G-X4PH160GMF"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-X4PH160GMF', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/theme.css" rel="stylesheet" type="text/css"/><link href="../../assets/header.css" rel="stylesheet" type="text/css"/><script src="../../assets/header.js"></script><script src="../../assets/chat.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><meta name="keywords" content="Julia, Bayesian inference, factor graph, message passing, probabilistic programming, reactive programming, RxInfer">
<link rel="sitemap" type="application/xml" title="Sitemap" href="https://docs.rxinfer.com/stable/sitemap.xml"></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="RxInfer.jl logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="RxInfer.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">User guide</span><ul><li><a class="tocitem" href="../getting-started/">Getting started</a></li><li><a class="tocitem" href="../comparison/">RxInfer.jl vs. Others</a></li><li><a class="tocitem" href="../how-to-use-rxinfer-from-python/">Using RxInfer from Python</a></li><li><a class="tocitem" href="../model-specification/">Model specification</a></li><li><a class="tocitem" href="../constraints-specification/">Constraints specification</a></li><li><a class="tocitem" href="../meta-specification/">Meta specification</a></li><li><input class="collapse-toggle" id="menuitem-2-7" type="checkbox"/><label class="tocitem" for="menuitem-2-7"><span class="docs-label">Inference specification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../inference/overview/">Overview</a></li><li><a class="tocitem" href="../inference/static/">Static inference</a></li><li><a class="tocitem" href="../inference/streamlined/">Streamline inference</a></li><li><a class="tocitem" href="../inference/static-vs-streamlined/">Static vs. Streamlined</a></li><li><a class="tocitem" href="../inference/initialization/">Initialization</a></li><li><a class="tocitem" href="../inference/autoupdates/">Auto-updates</a></li><li><a class="tocitem" href="../inference/delta-node/">Deterministic nodes</a></li><li><a class="tocitem" href="../inference/nonconjugate/">Non-conjugate inference</a></li><li><a class="tocitem" href="../inference/undefinedrules/">Undefined message update rules</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-8" type="checkbox"/><label class="tocitem" for="menuitem-2-8"><span class="docs-label">Inference customization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../customization/custom-node/">Defining a custom node and rules</a></li><li><a class="tocitem" href="../customization/postprocess/">Inference results postprocessing</a></li></ul></li><li><a class="tocitem" href="../performance-tips/">Performance Tips</a></li><li><a class="tocitem" href="../faq/">FAQ</a></li><li class="is-active"><a class="tocitem" href>Debugging</a><ul class="internal"><li><a class="tocitem" href="#Getting-Help-from-the-Community"><span>Getting Help from the Community</span></a></li><li><a class="tocitem" href="#Requesting-a-trace-of-messages"><span>Requesting a trace of messages</span></a></li><li><a class="tocitem" href="#user-guide-debugging-callbacks"><span>Using <code>callbacks</code> in the <code>infer</code> function</span></a></li><li><a class="tocitem" href="#Using-LoggerPipelineStage"><span>Using <code>LoggerPipelineStage</code></span></a></li><li><a class="tocitem" href="#user-guide-debugging-benchmark-callbacks"><span>Using <code>RxInferBenchmarkCallbacks</code> for Performance Analysis</span></a></li></ul></li><li><a class="tocitem" href="../session_summary/">Session summary</a></li><li><a class="tocitem" href="../telemetry/">Sharing sessions &amp; telemetry</a></li><li><a class="tocitem" href="../migration-guide-v2-v3/">Migration from v2 to v3</a></li><li><input class="collapse-toggle" id="menuitem-2-15" type="checkbox"/><label class="tocitem" for="menuitem-2-15"><span class="docs-label">Sharp bits of RxInfer</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../sharpbits/overview/">Overview</a></li><li><a class="tocitem" href="../sharpbits/rule-not-found/">Rule Not Found Error</a></li><li><a class="tocitem" href="../sharpbits/stack-overflow-inference/">Stack Overflow in Message Computations</a></li><li><a class="tocitem" href="../sharpbits/usage-colon-equality/">Using <code>=</code> instead of <code>:=</code> for deterministic nodes</a></li></ul></li></ul></li><li><span class="tocitem">Library</span><ul><li><a class="tocitem" href="../../library/model-construction/">Model construction</a></li><li><a class="tocitem" href="../../library/bethe-free-energy/">Bethe Free Energy</a></li><li><a class="tocitem" href="../../library/functional-forms/">Functional form constraints</a></li><li><a class="tocitem" href="../../library/exported-methods/">Exported methods</a></li></ul></li><li><a class="tocitem" href="../../examples/overview/">Examples</a></li><li><span class="tocitem">Contributing</span><ul><li><a class="tocitem" href="../../contributing/guide/">Contribution guide</a></li><li><a class="tocitem" href="../../contributing/guidelines/">Contribution guidelines</a></li><li><a class="tocitem" href="../../contributing/new-documentation/">Contributing to the documentation</a></li><li><a class="tocitem" href="../../contributing/new-example/">Contributing to the examples</a></li><li><a class="tocitem" href="../../contributing/new-release/">Publishing a new release</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">User guide</a></li><li class="is-active"><a href>Debugging</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Debugging</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/ReactiveBayes/RxInfer.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/ReactiveBayes/RxInfer.jl/blob/main/docs/src/manuals/debugging.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="user-guide-debugging"><a class="docs-heading-anchor" href="#user-guide-debugging">Debugging</a><a id="user-guide-debugging-1"></a><a class="docs-heading-anchor-permalink" href="#user-guide-debugging" title="Permalink"></a></h1><p>Debugging inference in <code>RxInfer</code> can be quite challenging, mostly due to the reactive nature of the inference, undefined order of computations, the use of observables, and generally hard-to-read stack traces in Julia. Below we discuss ways to help you find problems in your model that prevents you from getting the results you want. </p><h2 id="Getting-Help-from-the-Community"><a class="docs-heading-anchor" href="#Getting-Help-from-the-Community">Getting Help from the Community</a><a id="Getting-Help-from-the-Community-1"></a><a class="docs-heading-anchor-permalink" href="#Getting-Help-from-the-Community" title="Permalink"></a></h2><p>When you encounter issues that are difficult to debug, the RxInfer community is here to help. To get the most effective support:</p><ol><li><p><strong>Share Session Data</strong>: For complex issues, you can share your session data to help us understand exactly what&#39;s happening in your model. See <a href="../telemetry/#manual-session-sharing">Session Sharing</a> to learn how.</p></li><li><p><strong>Join Community Meetings</strong>: We discuss common issues and solutions in our regular community meetings. See <a href="../sharpbits/overview/#getting-help">Getting Help with Issues</a> for more information.</p></li></ol><h2 id="Requesting-a-trace-of-messages"><a class="docs-heading-anchor" href="#Requesting-a-trace-of-messages">Requesting a trace of messages</a><a id="Requesting-a-trace-of-messages-1"></a><a class="docs-heading-anchor-permalink" href="#Requesting-a-trace-of-messages" title="Permalink"></a></h2><p><code>RxInfer</code> provides a way that allows to save the history of the computations leading up to the computed messages and marginals in the inference procedure. This history is added on top of messages and marginals and is referred to as a <em>Memory Addon</em>. Below is an example explaining how you can extract this history and use it to fix a bug.</p><div class="admonition is-info" id="Note-deefa442a1e05510"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-deefa442a1e05510" title="Permalink"></a></header><div class="admonition-body"><p>Addons is a feature of <code>ReactiveMP.</code> Read more about implementing custom addons in the corresponding section of <code>ReactiveMP</code> package.</p></div></div><p>We show the application of the Memory Addon on the coin toss example from <a href="../getting-started/#user-guide-getting-started-coin-flip-simulation">earlier</a> in the documentation. We model the binary outcome <span>$x$</span> (heads or tails) using a <code>Bernoulli</code> distribution, with a parameter <span>$\theta$</span> that represents the probability of landing on heads. We have a <code>Beta</code> prior distribution for the <span>$\theta$</span> parameter, with a known shape <span>$\alpha$</span> and rate <span>$\beta$</span> parameter.</p><p class="math-container">\[\theta \sim \mathrm{Beta}(a, b)\]</p><p class="math-container">\[x_i \sim \mathrm{Bernoulli}(\theta)\]</p><p>where <span>$x_i \in {0, 1}$</span> are the binary observations (heads = 1, tails = 0). This is the corresponding RxInfer model:</p><pre><code class="language-julia hljs">using RxInfer, Random, Plots

n = 4
θ_real = 0.3
dataset = float.(rand(Bernoulli(θ_real), n))

@model function coin_model(x)
    θ  ~ Beta(4, huge)
    x .~ Bernoulli(θ)
end

result = infer(
    model = coin_model(),
    data  = (x = dataset, ),
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Inference results:
  Posteriors       | available for (θ)
</code></pre><p>The model will run without errors. But when we plot the posterior distribution for <span>$\theta$</span>, something&#39;s wrong. The posterior seems to be a flat distribution:</p><pre><code class="language-julia hljs">rθ = range(0, 1, length = 1000)

plot(rθ, (rvar) -&gt; pdf(result.posteriors[:θ], rvar), label=&quot;Infered posterior&quot;)
vline!([θ_real], label=&quot;Real θ&quot;, title = &quot;Inference results&quot;)</code></pre><img src="31498e92.svg" alt="Example block output"/><p>We can figure out what&#39;s wrong by tracing the computation of the posterior with the Memory Addon.  To obtain the trace, we have to add <code>addons = (AddonMemory(),)</code> as an argument to the inference function.  Note, that the argument to the <code>addons</code> keyword argument must be a tuple, because multiple addons can be activated  at the same time. Here, we create a tuple with a single element however.</p><pre><code class="language-julia hljs">result = infer(
    model = coin_model(),
    data  = (x = dataset, ),
    addons = (AddonMemory(),)
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Inference results:
  Posteriors       | available for (θ)
</code></pre><p>Now we have access to the messages that led to the marginal posterior:</p><pre><code class="language-julia hljs">RxInfer.ReactiveMP.getaddons(result.posteriors[:θ])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(AddonMemory(Product memory:
 Message mapping memory:
    At the node: Beta
    Towards interface: Val{:out}()
    With local constraint: Marginalisation()
    With addons: (AddonMemory(nothing),)
    With input marginals on Val{(:a, :b)}() edges: (PointMass{Int64}(4), PointMass{TinyHugeNumbers.HugeNumber}(huge))
    With the result: Beta{Float64}(α=4.0, β=1.0e12)
 Message mapping memory:
    At the node: Bernoulli
    Towards interface: Val{:p}()
    With local constraint: Marginalisation()
    With addons: (AddonMemory(nothing),)
    With input marginals on Val{(:out,)}() edges: (PointMass{Float64}(0.0),)
    With the result: Beta{Float64}(α=1.0, β=2.0)
 Message mapping memory:
    At the node: Bernoulli
    Towards interface: Val{:p}()
    With local constraint: Marginalisation()
    With addons: (AddonMemory(nothing),)
    With input marginals on Val{(:out,)}() edges: (PointMass{Float64}(0.0),)
    With the result: Beta{Float64}(α=1.0, β=2.0)
 Message mapping memory:
    At the node: Bernoulli
    Towards interface: Val{:p}()
    With local constraint: Marginalisation()
    With addons: (AddonMemory(nothing),)
    With input marginals on Val{(:out,)}() edges: (PointMass{Float64}(1.0),)
    With the result: Beta{Float64}(α=2.0, β=1.0)
 Message mapping memory:
    At the node: Bernoulli
    Towards interface: Val{:p}()
    With local constraint: Marginalisation()
    With addons: (AddonMemory(nothing),)
    With input marginals on Val{(:out,)}() edges: (PointMass{Float64}(0.0),)
    With the result: Beta{Float64}(α=1.0, β=2.0)
),)</code></pre><p><img src="../../assets/img/debugging_messages.png" alt="Addons_messages"/></p><p>The messages in the factor graph are marked in color. If you&#39;re interested in the mathematics behind these results, consider verifying them manually using the general equation for sum-product messages:</p><p class="math-container">\[\underbrace{\overrightarrow{\mu}_{θ}(θ)}_{\substack{ \text{outgoing}\\ \text{message}}} = \sum_{x_1,\ldots,x_n} \underbrace{\overrightarrow{\mu}_{X_1}(x_1)\cdots \overrightarrow{\mu}_{X_n}(x_n)}_{\substack{\text{incoming} \\ \text{messages}}} \cdot \underbrace{f(θ,x_1,\ldots,x_n)}_{\substack{\text{node}\\ \text{function}}}\]</p><p><img src="../../assets/img/debugging_graph.png" alt="Graph"/></p><p>Note that the posterior (yellow) has a rate parameter on the order of <code>1e12</code>. Our plot failed because a Beta distribution with such a rate parameter cannot be accurately depicted using the range of <span>$\theta$</span> we used in the code block above. So why does the posterior have this rate parameter?</p><p>All the observations (purple, green, pink, blue) have much smaller rate parameters. It seems the prior distribution (red) has an unusual rate parameter, namely <code>1e12</code>. If we look back at the model, the parameter was set to <code>huge</code> (which is a reserved keyword meaning <code>1e12</code>). Reducing the prior rate parameter will ensure the posterior has a reasonable rate parameter as well.</p><pre><code class="language-julia hljs">@model function coin_model(x)
    θ  ~ Beta(4, 100)
    x .~ Bernoulli(θ)
end

result = infer(
    model = coin_model(),
    data  = (x = dataset, ),
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Inference results:
  Posteriors       | available for (θ)
</code></pre><pre><code class="language-julia hljs">rθ = range(0, 1, length = 1000)

plot(rθ, (rvar) -&gt; pdf(result.posteriors[:θ], rvar), fillalpha = 0.4, fill = 0, label=&quot;Infered posterior&quot;)
vline!([θ_real], label=&quot;Real θ&quot;, title = &quot;Inference results&quot;)</code></pre><img src="11a59c0d.svg" alt="Example block output"/><p>Now the posterior has much more sensible shape thus confirming that we have identified the original issue correctly.  We can run the model with more observations, to get an even better posterior:</p><pre><code class="language-julia hljs">result = infer(
    model = coin_model(),
    data  = (x = float.(rand(Bernoulli(θ_real), 1000)), ),
)

rθ = range(0, 1, length = 1000)
plot(rθ, (rvar) -&gt; pdf(result.posteriors[:θ], rvar), fillalpha = 0.4, fill = 0, label=&quot;Infered posterior (1000 observations)&quot;)
vline!([θ_real], label=&quot;Real θ&quot;, title = &quot;Inference results&quot;)</code></pre><img src="407dd01e.svg" alt="Example block output"/><h2 id="user-guide-debugging-callbacks"><a class="docs-heading-anchor" href="#user-guide-debugging-callbacks">Using <code>callbacks</code> in the <code>infer</code> function</a><a id="user-guide-debugging-callbacks-1"></a><a class="docs-heading-anchor-permalink" href="#user-guide-debugging-callbacks" title="Permalink"></a></h2><p>Another way to inspect the inference procedure is to use the <code>callbacks</code> or <code>events</code> from the <a href="../inference/overview/#RxInfer.infer"><code>infer</code></a> function. Read more about callbacks in the documentation to the <a href="../inference/overview/#RxInfer.infer"><code>infer</code></a> function. Here, we show a simple application of callbacks to a simple IID inference problem. We start with model specification:</p><pre><code class="language-julia hljs">using RxInfer

@model function iid_normal(y)
    μ  ~ Normal(mean = 0.0, variance = 100.0)
    γ  ~ Gamma(shape = 1.0, rate = 1.0)
    y .~ Normal(mean = μ, precision = γ)
end</code></pre><p>Next, let us define a syntehtic dataset:</p><pre><code class="language-julia hljs">dataset = rand(NormalMeanPrecision(3.1415, 30.0), 100)</code></pre><p>Now, we can use the <code>callbacks</code> argument of the <code>infer</code> function to track the order of posteriors computation and their intermediate values for each variational iteration:</p><pre><code class="language-julia hljs"># A callback that will be called every time before a variational iteration starts
function before_iteration_callback(model, iteration)
    println(&quot;Starting iteration &quot;, iteration)
end

# A callback that will be called every time after a variational iteration finishes
function after_iteration_callback(model, iteration)
    println(&quot;Iteration &quot;, iteration, &quot; has been finished&quot;)
end

# A callback that will be called every time a posterior is updated
function on_marginal_update_callback(model, variable_name, posterior)
    println(&quot;Latent variable &quot;, variable_name, &quot; has been updated. Estimated mean is &quot;, mean(posterior), &quot; with standard deviation &quot;, std(posterior))
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">on_marginal_update_callback (generic function with 1 method)</code></pre><p>After we have defined all callbacks of interest, we can call the <a href="../inference/overview/#RxInfer.infer"><code>infer</code></a> function passing them in the <code>callback</code> argument as a named tuple:</p><pre><code class="language-julia hljs">init = @initialization begin
    q(μ) = vague(NormalMeanVariance)
end

result = infer(
    model = iid_normal(),
    data  = (y = dataset, ),
    constraints = MeanField(),
    iterations = 5,
    initialization = init,
    returnvars = KeepLast(),
    callbacks = (
        on_marginal_update = on_marginal_update_callback,
        before_iteration   = before_iteration_callback,
        after_iteration    = after_iteration_callback
    )
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Starting iteration 1
Latent variable γ has been updated. Estimated mean is 1.0199999999897615e-12 with standard deviation 1.4282856856942332e-13
Latent variable μ has been updated. Estimated mean is 3.2225092507548384e-8 with standard deviation 9.999999948999998
Iteration 1 has been finished
Starting iteration 2
Latent variable γ has been updated. Estimated mean is 0.009269535623001923 with standard deviation 0.0012979946121077764
Latent variable μ has been updated. Estimated mean is 3.125603729880277 with standard deviation 1.0330966817933893
Iteration 2 has been finished
Starting iteration 3
Latent variable γ has been updated. Estimated mean is 0.9066819854201038 with standard deviation 0.12696087267307457
Latent variable μ has been updated. Estimated mean is 3.158974416705157 with standard deviation 0.10501432813866265
Iteration 3 has been finished
Starting iteration 4
Latent variable γ has been updated. Estimated mean is 15.092465380738634 with standard deviation 2.1133678691530524
Latent variable μ has been updated. Estimated mean is 3.159301894140231 with standard deviation 0.02574058821667667
Iteration 4 has been finished
Starting iteration 5
Latent variable γ has been updated. Estimated mean is 17.8266080665775 with standard deviation 2.496224424140154
Latent variable μ has been updated. Estimated mean is 3.159305104701491 with standard deviation 0.023684511067341145
Iteration 5 has been finished</code></pre><p>We can see that the callback has been correctly executed for each intermediate variational iteration.</p><pre><code class="language-julia hljs">println(&quot;Estimated mean: &quot;, mean(result.posteriors[:μ]))
println(&quot;Estimated precision: &quot;, mean(result.posteriors[:γ]))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Estimated mean: 3.159305104701491
Estimated precision: 17.8266080665775</code></pre><h2 id="Using-LoggerPipelineStage"><a class="docs-heading-anchor" href="#Using-LoggerPipelineStage">Using <code>LoggerPipelineStage</code></a><a id="Using-LoggerPipelineStage-1"></a><a class="docs-heading-anchor-permalink" href="#Using-LoggerPipelineStage" title="Permalink"></a></h2><p><code>ReactiveMP</code> inference engine allows attaching extra computations to the default computational pipeline of message passing.  Read more about pipelines in the corresponding section of <code>ReactiveMP</code>. Here we show how to use <code>LoggerPipelineStage</code> to trace the order of message passing updates for debugging purposes. We start with model specification:</p><pre><code class="language-julia hljs">using RxInfer

@model function iid_normal_with_pipeline(y)
    μ  ~ Normal(mean = 0.0, variance = 100.0)
    γ  ~ Gamma(shape = 1.0, rate = 1.0)
    y .~ Normal(mean = μ, precision = γ) where { pipeline = LoggerPipelineStage() }
end</code></pre><p>Next, let us define a syntehtic dataset:</p><pre><code class="language-julia hljs"># We use less data points in the dataset to reduce the amount of text printed
# during the inference
dataset = rand(NormalMeanPrecision(3.1415, 30.0), 5)</code></pre><p>Now, we can call the <a href="../inference/overview/#RxInfer.infer"><code>infer</code></a> function. We combine the pipeline logger stage with the callbacks, which were introduced in the <a href="#user-guide-debugging-callbacks">previous section</a>:</p><pre><code class="language-julia hljs">result = infer(
    model = iid_normal_with_pipeline(),
    data  = (y = dataset, ),
    constraints = MeanField(),
    iterations = 5,
    initialization = init,
    returnvars = KeepLast(),
    callbacks = (
        on_marginal_update = on_marginal_update_callback,
        before_iteration   = before_iteration_callback,
        after_iteration    = after_iteration_callback
    )
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Starting iteration 1
[Log]: [NormalMeanPrecision][τ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][τ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][τ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][τ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][τ]: DeferredMessage([ use `as_message` to compute the message ])
Latent variable γ has been updated. Estimated mean is 1.3999999999852607e-12 with standard deviation 7.483314773469098e-13
[Log]: [NormalMeanPrecision][μ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][μ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][μ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][μ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][μ]: DeferredMessage([ use `as_message` to compute the message ])
Latent variable μ has been updated. Estimated mean is 2.218450862194554e-9 with standard deviation 9.9999999965
Iteration 1 has been finished
Starting iteration 2
[Log]: [NormalMeanPrecision][τ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][τ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][τ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][τ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][τ]: DeferredMessage([ use `as_message` to compute the message ])
Latent variable γ has been updated. Estimated mean is 0.01266647491622983 with standard deviation 0.006770515633528312
[Log]: [NormalMeanPrecision][μ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][μ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][μ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][μ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][μ]: DeferredMessage([ use `as_message` to compute the message ])
Latent variable μ has been updated. Estimated mean is 2.7370441168271196 with standard deviation 3.692768868892191
Iteration 2 has been finished
Starting iteration 3
[Log]: [NormalMeanPrecision][τ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][τ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][τ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][τ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][τ]: DeferredMessage([ use `as_message` to compute the message ])
Latent variable γ has been updated. Estimated mean is 0.0978516249537536 with standard deviation 0.052303893616577925
[Log]: [NormalMeanPrecision][μ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][μ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][μ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][μ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][μ]: DeferredMessage([ use `as_message` to compute the message ])
Latent variable μ has been updated. Estimated mean is 3.105737023030407 with standard deviation 1.415263901362041
Iteration 3 has been finished
Starting iteration 4
[Log]: [NormalMeanPrecision][τ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][τ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][τ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][τ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][τ]: DeferredMessage([ use `as_message` to compute the message ])
Latent variable γ has been updated. Estimated mean is 0.5620089600618966 with standard deviation 0.30040642526410516
[Log]: [NormalMeanPrecision][μ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][μ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][μ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][μ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][μ]: DeferredMessage([ use `as_message` to compute the message ])
Latent variable μ has been updated. Estimated mean is 3.157977345302737 with standard deviation 0.5954866040551655
Iteration 4 has been finished
Starting iteration 5
[Log]: [NormalMeanPrecision][τ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][τ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][τ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][τ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][τ]: DeferredMessage([ use `as_message` to compute the message ])
Latent variable γ has been updated. Estimated mean is 1.6690656514178905 with standard deviation 0.8921531176626301
[Log]: [NormalMeanPrecision][μ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][μ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][μ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][μ]: DeferredMessage([ use `as_message` to compute the message ])
[Log]: [NormalMeanPrecision][μ]: DeferredMessage([ use `as_message` to compute the message ])
Latent variable μ has been updated. Estimated mean is 3.1654224723833133 with standard deviation 0.3459539085822944
Iteration 5 has been finished</code></pre><p>We can see the order of message update events. Note that <code>ReactiveMP</code> may decide to compute messages lazily, in which case the actual computation of the value of a message will be deferred until later moment. In this case, <code>LoggerPipelineStage</code> will report <em>DeferredMessage</em>.</p><h2 id="user-guide-debugging-benchmark-callbacks"><a class="docs-heading-anchor" href="#user-guide-debugging-benchmark-callbacks">Using <code>RxInferBenchmarkCallbacks</code> for Performance Analysis</a><a id="user-guide-debugging-benchmark-callbacks-1"></a><a class="docs-heading-anchor-permalink" href="#user-guide-debugging-benchmark-callbacks" title="Permalink"></a></h2><p><code>RxInfer</code> provides a built-in benchmarking callback structure called <a href="#RxInfer.RxInferBenchmarkCallbacks"><code>RxInferBenchmarkCallbacks</code></a> that helps collect timing information during the inference procedure. This structure aggregates timing information across multiple runs, allowing you to track performance statistics (min/max/average/etc.) of your model&#39;s creation and inference procedure.</p><p>Here&#39;s how to use it:</p><pre><code class="language-julia hljs">using RxInfer


# Create a benchmark callbacks instance to track performance
benchmark_callbacks = RxInferBenchmarkCallbacks()

# Run inference multiple times to gather statistics
for i in 1:3  # Usually you&#39;d want more runs for better statistics
    infer(
        model = iid_normal(),
        data = (y = dataset, ),
        constraints = MeanField(),
        iterations = 5,
        initialization = init,
        callbacks = benchmark_callbacks
    )
end</code></pre><p>In order to nicely display the statistics, you may want to install <code>PrettyTables.jl</code> package.  It is not bundled with <code>RxInfer</code> by default, but, if installed manually, it makes the output more readable.</p><pre><code class="language-julia hljs">using PrettyTables

# Display the benchmark statistics in a nicely formatted table
PrettyTables.pretty_table(benchmark_callbacks)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">RxInfer inference benchmark statistics: 3 evaluations
╭────────────────┬────────────┬────────────┬────────────┬────────────┬───────────╮
│      Operation │        Min │        Max │       Mean │     Median │       Std │
├────────────────┼────────────┼────────────┼────────────┼────────────┼───────────┤
│ Model creation │ 276.725 μs │ 423.775 μs │ 336.446 μs │ 308.837 μs │ 77.315 μs │
│      Inference │  45.374 μs │  80.532 μs │  58.618 μs │  49.947 μs │ 19.116 μs │
│      Iteration │   3.477 μs │  26.977 μs │   7.169 μs │   4.156 μs │  6.578 μs │
╰────────────────┴────────────┴────────────┴────────────┴────────────┴───────────╯</code></pre><p>The <code>RxInferBenchmarkCallbacks</code> structure collects timestamps at various stages of the inference process:</p><ul><li>Before and after model creation</li><li>Before and after inference starts/ends</li><li>Before and after each iteration</li><li>Before and after autostart (for streaming inference)</li></ul><article><details class="docstring" open="true"><summary id="RxInfer.RxInferBenchmarkCallbacks"><a class="docstring-binding" href="#RxInfer.RxInferBenchmarkCallbacks"><code>RxInfer.RxInferBenchmarkCallbacks</code></a> — <span class="docstring-category">Type</span></summary><div><pre><code class="language-julia hljs">RxInferBenchmarkCallbacks(; capacity = RxInfer.DEFAULT_BENCHMARK_CALLBACKS_BUFFER_CAPACITY)</code></pre><p>A callback structure for collecting timing information during the inference procedure. This structure collects timestamps for various stages of the inference process and aggregates them across multiple runs, allowing you to track performance statistics (min/max/average/etc.) of your model&#39;s creation and inference procedure. The structure supports pretty printing by default, displaying timing statistics in a human-readable format.</p><p>The structure uses circular buffers with a default capacity of 1000 entries to store timestamps, which helps to limit memory usage in long-running applications. Use <code>RxInferBenchmarkCallbacks(; capacity = N)</code> to change the buffer capacity. See also <a href="#RxInfer.get_benchmark_stats"><code>RxInfer.get_benchmark_stats(callbacks)</code></a>.</p><p><strong>Fields</strong></p><ul><li><code>before_model_creation_ts</code>: CircularBuffer of timestamps before model creation</li><li><code>after_model_creation_ts</code>: CircularBuffer of timestamps after model creation</li><li><code>before_inference_ts</code>: CircularBuffer of timestamps before inference starts</li><li><code>after_inference_ts</code>: CircularBuffer of timestamps after inference ends</li><li><code>before_iteration_ts</code>: CircularBuffer of vectors of timestamps before each iteration</li><li><code>after_iteration_ts</code>: CircularBuffer of vectors of timestamps after each iteration</li><li><code>before_autostart_ts</code>: CircularBuffer of timestamps before autostart</li><li><code>after_autostart_ts</code>: CircularBuffer of timestamps after autostart</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs"># Create a callbacks instance to track performance
callbacks = RxInferBenchmarkCallbacks()

# Run inference multiple times to gather statistics
for _ in 1:10
    infer(
        model = my_model(),
        data = my_data,
        callbacks = callbacks
    )
end

# Display the timing statistics (you need to install `PrettyTables.jl` to use `pretty_table` function)
using PrettyTables

PrettyTables.pretty_table(callbacks)</code></pre><a class="docs-sourcelink" target="_blank" href="https://github.com/ReactiveBayes/RxInfer.jl/blob/ae5dc78095e01a060264e193291a99fb61368121/src/inference/benchmarkcallbacks.jl#L13-L55">source</a></div></details></article><article><details class="docstring" open="true"><summary id="RxInfer.get_benchmark_stats"><a class="docstring-binding" href="#RxInfer.get_benchmark_stats"><code>RxInfer.get_benchmark_stats</code></a> — <span class="docstring-category">Function</span></summary><div><pre><code class="language-julia hljs">get_benchmark_stats(callbacks::RxInferBenchmarkCallbacks)</code></pre><p>Returns a matrix containing benchmark statistics for different operations in the inference process. The matrix contains the following columns:</p><ol><li>Operation name (String)</li><li>Minimum time (Float64)</li><li>Maximum time (Float64)</li><li>Mean time (Float64)</li><li>Median time (Float64)</li><li>Standard deviation (Float64)</li></ol><p>Each row represents a different operation (model creation, inference, iteration, autostart). Times are in nanoseconds.</p><a class="docs-sourcelink" target="_blank" href="https://github.com/ReactiveBayes/RxInfer.jl/blob/ae5dc78095e01a060264e193291a99fb61368121/src/inference/benchmarkcallbacks.jl#L107-L121">source</a></div></details></article><article><details class="docstring" open="true"><summary id="RxInfer.DEFAULT_BENCHMARK_CALLBACKS_BUFFER_CAPACITY"><a class="docstring-binding" href="#RxInfer.DEFAULT_BENCHMARK_CALLBACKS_BUFFER_CAPACITY"><code>RxInfer.DEFAULT_BENCHMARK_CALLBACKS_BUFFER_CAPACITY</code></a> — <span class="docstring-category">Constant</span></summary><div><pre><code class="language-julia hljs">DEFAULT_BENCHMARK_CALLBACKS_BUFFER_CAPACITY</code></pre><p>The default capacity of the circular buffers used to store timestamps in the <code>RxInferBenchmarkCallbacks</code> structure.</p><a class="docs-sourcelink" target="_blank" href="https://github.com/ReactiveBayes/RxInfer.jl/blob/ae5dc78095e01a060264e193291a99fb61368121/src/inference/benchmarkcallbacks.jl#L6-L10">source</a></div></details></article><div class="admonition is-info" id="Note-d99dbc9015391ca1"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-d99dbc9015391ca1" title="Permalink"></a></header><div class="admonition-body"><p>By default, the <code>RxInferBenchmarkCallbacks</code> structure uses a circular buffer with a limited capacity to store timestamps. This helps limit memory usage in long-running applications. You can change the buffer capacity by passing a different value to the <code>capacity</code> keyword argument of the <code>RxInferBenchmarkCallbacks</code> constructor.</p></div></div><p>This information can be used to:</p><ul><li>Track performance statistics (min/max/average) of your inference procedure</li><li>Identify performance variability across runs</li><li>Monitor the time spent in different stages of inference</li><li>Establish performance baselines for your models</li><li>Detect performance regressions</li></ul><p>The timestamps are collected using <code>time_ns()</code> for high precision timing measurements and are automatically formatted into human-readable durations when displayed.</p><div class="admonition is-info" id="Note-89ab06ca2ac0ec0d"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-89ab06ca2ac0ec0d" title="Permalink"></a></header><div class="admonition-body"><p>The timing measurements include all overhead from the Julia runtime and may vary between runs. For more precise benchmarking of specific code sections, consider using the <code>BenchmarkTools.jl</code> package. When gathering performance statistics, consider running multiple iterations to get more reliable metrics.</p></div></div><script type="module">import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.esm.min.mjs';
mermaid.initialize({
    startOnLoad: true,
    theme: "neutral"
});
</script></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../faq/">« FAQ</a><a class="docs-footer-nextpage" href="../session_summary/">Session summary »</a><div class="flexbox-break"></div><p class="footer-message">Created in <a href="https://biaslab.github.io/">BIASlab</a>, maintained by <a href="https://github.com/ReactiveBayes">ReactiveBayes</a>, powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.0 on <span class="colophon-date" title="Thursday 20 November 2025 16:48">Thursday 20 November 2025</span>. Using Julia version 1.12.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>

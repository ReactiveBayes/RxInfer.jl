var documenterSearchIndex = {"docs":
[{"location":"manuals/model-specification/#user-guide-model-specification","page":"Model specification","title":"Model Specification","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"RxInfer largely depends on GraphPPL for model specification. Read extensive documentation regarding the model specification in the corresponding section of GraphPPL documentation. Here we outline only a small portion of model specification capabilities for beginners.","category":"page"},{"location":"manuals/model-specification/#@model-macro","page":"Model specification","title":"@model macro","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"The RxInfer.jl package exports the @model macro for model specification. This @model macro accepts the model specification itself in a form of regular Julia function. ","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"For example: ","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"@model function model_name(model_arguments...)\n    # model specification here\nend","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"where model_arguments... may include both hypeparameters and data. ","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"note: Note\nmodel_arguments are converted to keyword arguments. Positional arguments in the model specification are not supported.  Thus it is not possible to use Julia's multiple dispatch for the model arguments.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"The @model macro returns a regular Julia function (in this example model_name()) which can be executed as usual. The only difference here is that all arguments of the model function are treated as keyword arguments. Upon calling, the model function returns a so-called model generator object, e.g:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"using RxInfer #hide\n@model function my_model(observation, hyperparameter)\n    observations ~ Normal(0.0, hyperparameter)\nend","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"model = my_model(hyperparameter = 3)\nnothing #hide","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"The model generator is not a real model (yet). For example, in the code above, we haven't specified anything for the observation.  The generator object allows us to iteratively add extra properties to the model, condition on data, and/or assign extra metadata information without actually materializing the entire graph structure. Read extra information about model generator here.","category":"page"},{"location":"manuals/model-specification/#A-state-space-model-example","page":"Model specification","title":"A state space model example","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Here we give an example of a probabilistic model before presenting the details of the model specification syntax. The model below is a simple state space model with latent random variables x and noisy observations y.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"using RxInfer #hide\n\n@model function state_space_model(y, trend, variance)\n    x[1] ~ Normal(mean = 0.0, variance = 100.0)\n    y[1] ~ Normal(mean = x[1], variance = variance)\n    for i in 2:length(y)\n       x[i] ~ Normal(mean = x[i - 1] + trend, variance = 1.0)\n       y[i] ~ Normal(mean = x[i], variance = variance)\n    end\nend","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"In this model we assign a prior distribution over latent state x[1]. All subsequent states x[i] depend on x[i - 1] and trend and are modelled  as a simple Gaussian random walk. Observations y are modelled with the Gaussian distribution as well with a  prespecified variance hyperparameter.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"note: Note\nlength(y) can be called only if y has an associated data with it. This is not always the case, for example it is possible to instantiate the  model lazily before the data becomes available. In such situations, length(y) will throw an error.","category":"page"},{"location":"manuals/model-specification/#user-guide-model-specification-hyperparameters","page":"Model specification","title":"Hyperparameters","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Any constant passed to a model as a model argument will be automatically converted to a corresponding constant node in the model's graph.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"model = state_space_model(trend = 3.0, variance = 1.0)\nnothing #hide","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"In this example we instantiate a model generator with trend and variance parameters clamped to 3.0 and 1.0 respectively. That means  that no inference will be performed for those parameters and some of the expressions within the model structure might be simplified and compiled-out.","category":"page"},{"location":"manuals/model-specification/#user-guide-model-specification-conditioning","page":"Model specification","title":"Conditioning on data","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"To fully complete model specification we need to specify y. In this example, y is playing a role of observations. RxInfer provides a convenient mechanism to pass data values to the model with the | operator.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"conditioned = model | (y = [ 0.0, 1.0, 2.0 ], )","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"note: Note\nThe conditioning on data is a feature of RxInfer, not GraphPPL.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"In the example above we conditioned on data in a form of the NamedTuple, but it is also possible to  condition on a dictionary where keys represent names of the corresponding model arguments:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"data        = Dict(:y => [ 0.0, 1.0, 2.0 ])\nconditioned = model | data","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Sometimes it might be useful to indicate that some arguments are data (thus condition on them) before the actual data becomes available. This situation may occur during reactive inference, when data becomes available after model creation. RxInfer provides a special structure called RxInfer.DefferedDataHandler, which can be used instead of the real data.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"For the example above, however, we cannot simply do the following:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"model | (y = RxInfer.DefferedDataHandler(), )","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"because we use length(y) in the model and this is only possible if y has an associated data.  We could adjust the model specification a bit, by adding the extra n parameter to the list of arguments:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"@model function state_space_model_with_n(y, n, trend, variance)\n    x[1] ~ Normal(mean = 0.0, variance = 100.0)\n    y[1] ~ Normal(mean = x[1], variance = variance)\n    for i in 2:n\n       x[i] ~ Normal(mean = x[i - 1] + trend, variance = 1.0)\n       y[i] ~ Normal(mean = x[i], variance = variance)\n    end\nend","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"For such model, we can safely condition on y without providing actual data for it, but using the RxInfer.DefferedDataHandler instead:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"state_space_model_with_n(trend = 3.0, variance = 1.0, n = 10) | (\n    y = RxInfer.DefferedDataHandler(), \n)","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Read more information about condition on data in this section of the documentation.","category":"page"},{"location":"manuals/model-specification/#user-guide-model-specification-random-variables","page":"Model specification","title":"Latent variables","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Latent variables are being created with the ~ operator and can be read as is distributed as.  For example, to create a latent variable y which is modeled by a Normal distribution,  where its mean and variance are controlled by the random variables m and v respectively, we define","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"y ~ Normal(mean = m, variance = v)","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"In the example above","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"x[1] ~ Normal(mean = 0.0, variance = 100.0)","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"indicates that x₁ is distributed as Normal distribution. ","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"note: Note\nThe RxInfer.jl package uses the ~ operator for modelling both stochastic and deterministic relationships between random variables. However, GraphPPL.jl also allows to use := operator for deterministic relationships.","category":"page"},{"location":"manuals/model-specification/#user-guide-model-specification-node-creation","page":"Model specification","title":"Relationships between variables","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"In probabilistic models based on graphs, factor nodes are used to define a relationship between random variables and/or constants and data variables. A factor node defines a probability distribution over selected latent or data variables. The ~ operator not only creates a latent variable but also  defines a functional relatinship of it with other variables and creates a factor node as a result.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"In the example above","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"x[1] ~ Normal(mean = 0.0, variance = 100.0)","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"not only creates a latent variable x₁ but also a factor node Normal.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"note: Note\nGenerally it is not necessary to label all the arguments with their names, as mean = ... or variance = ... and many factor nodes  do not require it explicitly. However, for nodes, which have many different useful parametrizations (e.g. Normal) labeling the arguments  is a requirement that helps to avoid any possible confusion. Read more about Distributions compatibility here.","category":"page"},{"location":"manuals/model-specification/#user-guide-model-specification-node-creation-control-flow","page":"Model specification","title":"Control flow statements","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"In general, it is possible to use any Julia code within model specification function, including control flow statements, such as for, while and if statements. However, it is not possible to use any latent states within such statements. This is due to the fact that it is necessary to know exactly the structure of the graph before the inference. Thus it is not possible to write statements like:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"c ~ Categorical([ 1/2, 1/2 ])\n# This is NOT possible in `RxInfer`'s model specification language\nif c > 1\n# ...\nend","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"since c must be statically known upon graph creation.","category":"page"},{"location":"manuals/model-specification/#user-guide-model-specification-node-creation-anonymous","page":"Model specification","title":"Anonymous factor nodes and latent variables","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"The @model macro automatically resolves any inner function calls into anonymous factor nodes and latent variables.  For example the following:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"y ~ Normal(\n    mean = Normal(mean = 0.0, variance = 1.0), \n    precision = Gamma(shape = 1.0, rate = 1.0)\n)","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"is equivalent to","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"tmp1 ~ Normal(mean = 0.0, variance = 1.0)\ntmp2 ~ Gamma(shape = 1.0, rate = 1.0)\ny    ~ Normal(mean = tmp1, precision = tmp2)","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"The inference backend still performs inference for anonymous latent variables, however, there it does not provide an easy way to obtain posteriors for them. Note that the inference backend will try to optimize deterministic function calls in the case where all arguments are known in advance. For example:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"y ~ Normal(mean = 0.0, variance = inv(2.0))","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"should not create an extra factor node for the inv, since inv is a deterministic function and all arguments are known in advance. The same situation applies in case of complex initializations involving different types, as in:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"y ~ MvNormal(mean = zeros(3), covariance = Matrix(Diagonal(ones(3))))","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"In this case, the expression Matrix(Diagonal(ones(3))) can (and will) be precomputed upon model creation and does not require to perform probabilistic inference.","category":"page"},{"location":"manuals/model-specification/#user-guide-model-specification-node-creation-indexing","page":"Model specification","title":"Indexing operations","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"The ref expressions, such as x[i], are handled in a special way. Technically, in Julia, the x[i] call is translated to a function call getindex(x, i). Thus the @model macro should create a factor node for the getindex function, but this won't happen in practice because this case is treated separately. This means that the model parser will not create unnecessary nodes when only simple indexing is involved. That also means that all expressions inside x[...] list are left untouched during model parsing. ","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"warning: Warning\nIt is not allowed to use latent variables within square brackets in the model specification or for control flow statements such as if, for or while.","category":"page"},{"location":"manuals/model-specification/#user-guide-model-specification-node-creation-broadcasting","page":"Model specification","title":"Broadcasting syntax","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"GraphPPL support broadcasting for ~ operator in the exact same way as Julia itself.  A user is free to write an expression of the following form:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"m  ~ Normal(mean = 0.0, precision = 0.0001)\nt  ~ Gamma(shape = 1.0, rate = 1.0)\ny .~ Normal(mean = m, precision = t)","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"More complex expressions are also allowed:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"w         ~ Wishart(3, diageye(2))\nx[1]      ~ MvNormal(mean = zeros(2), precision = diageye(2))\nx[2:end] .~ A .* x[1:end-1] # <- State-space model with transition matrix A\ny        .~ MvNormal(mean = x, precision = w) # <- Observations with unknown precision matrix","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Note, however, that shapes of all variables that take part in the broadcasting operation must be defined in advance. That means that it is not possible to  use broadcasting with deffered data. Read more about how broadcasting machinery works in Julia in the official documentation.","category":"page"},{"location":"manuals/model-specification/#user-guide-model-specification-distributions","page":"Model specification","title":"Distributions.jl compatibility","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"For some factor nodes we rely on the syntax from Distributions.jl to make it easy to adopt RxInfer.jl for these users. These nodes include for example the Beta and Wishart distributions. These nodes can be created using the ~ syntax with the arguments as specified in Distributions.jl. Unfortunately, we RxInfer.jl is not yet compatible with all possible distributions to be used as factor nodes. If you feel that you would like to see another node implemented, please file an issue.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"note: Note\nTo quickly check the list of all available factor nodes that can be used in the model specification language call ?ReactiveMP.is_predefined_node or Base.doc(ReactiveMP.is_predefined_node).","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Specifically for the Gaussian/Normal case we have custom implementations that yield a higher computational efficiency and improved stability in comparison to Distributions.jl as these are optimized for sampling operations. Our aliases for these distributions therefore do not correspond to the implementations from Distributions.jl. However, our model specification language is compatible with syntax from Distributions.jl for normal distributions, which will be automatically converted. RxInfer has its own implementation because of the following 3 reasons:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Distributions.jl constructs normal distributions by saving the corresponding covariance matrices in a PDMat object from PDMats.jl. This construction always computes the Cholesky decompositions of the covariance matrices, which is very convenient for sampling-based procedures. However, in RxInfer.jl we mostly base our computations on analytical expressions which do not always need to compute the Cholesky decomposition. In order to reduce the overhead that Distributions.jl introduces, we therefore have custom implementations.\nDepending on the update rules, we might favor different parameterizations of the normal distributions. ReactiveMP.jl has quite a variety in parameterizations that allow us to efficient computations where we convert between parameterizations as little as possible.\nIn certain situations we value stability a lot, especially when inverting matrices. PDMats.jl, and hence Distributions.jl, is not capable to fulfill all needs that we have here. Therefore we use PositiveFactorizations.jl to cope with the corner-cases.","category":"page"},{"location":"manuals/model-specification/#user-guide-model-specification-visualization","page":"Model specification","title":"Model structure visualisation","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"It is also possible to visualize the model structure after conditioning on data. For that we need two extra packages installed: Cairo and GraphPlot. Note, that those packages are not included in the RxInfer package and must be installed separately.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"using Cairo, GraphPlot\n\n# `Create` the actual graph of the model conditioned on the data\nmodel = RxInfer.create_model(conditioned)\n\n# Call `gplot` function from `GraphPlot` to visualise the structure of the graph\nGraphPlot.gplot(RxInfer.getmodel(model))","category":"page"},{"location":"manuals/model-specification/#user-guide-model-specification-node-creation-options","page":"Model specification","title":"Node creation options","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"GraphPPL allows to pass optional arguments to the node creation constructor with the where { options...  } options specification syntax.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Example:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"y ~ Normal(mean = y_mean, var = y_var) where { meta = ... }","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"A list of the available options specific to the ReactiveMP inference engine is presented below.","category":"page"},{"location":"manuals/model-specification/#Metadata-option","page":"Model specification","title":"Metadata option","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Is is possible to pass any extra metadata to a factor node with the meta option. Metadata can be later accessed in message computation rules.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"z ~ f(x, y) where { meta = Linearization() }\nd ~ g(a, b) where { meta = Unscented() }","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"This option might be useful to change message passing rules around a specific factor node. Read more about this feature in Meta Specification section.","category":"page"},{"location":"manuals/model-specification/#Dependencies-option","page":"Model specification","title":"Dependencies option","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"A user can modify default computational pipeline of a node with the dependencies options.  Read more about different options in the ReactiveMP.jl documentation.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"y[k - 1] ~ Probit(x[k]) where {\n    # This specification indicates that in order to compute an outbound message from the `in` interface\n    # We need an inbound message from the same edge initialized to `NormalMeanPrecision(0.0, 1.0)`\n    dependencies = RequireMessageFunctionalDependencies(in = NormalMeanPrecision(0.0, 1.0))\n}","category":"page"},{"location":"manuals/model-specification/#Read-also","page":"Model specification","title":"Read also","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Constraints specification\nMeta specification\nInference execution\nDebugging inference","category":"page"},{"location":"examples/overview/#examples-overview","page":"Overview","title":"Examples overview","text":"","category":"section"},{"location":"examples/overview/","page":"Overview","title":"Overview","text":"This section contains a set of examples for Bayesian Inference with RxInfer package in various probabilistic models.","category":"page"},{"location":"examples/overview/","page":"Overview","title":"Overview","text":"note: Note\nAll examples have been pre-generated automatically from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/overview/","page":"Overview","title":"Overview","text":"Basic examples: Basic examples contain \"Hello World!\" of Bayesian inference in RxInfer.\nAdvanced examples: Advanced examples contain more complex inference problems.\nProblem specific: Problem specific examples contain specialized models and inference for various domains.\nExternal examples: Featured examples from the community.","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/#examples-assessing-people’s-skills","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"","category":"section"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"# Activate local environment, see `Project.toml`\nimport Pkg;\nPkg.activate(\"..\");\nPkg.instantiate();","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"The goal of this demo is to demonstrate the use of the @node and @rule macros, which allow the user to define custom factor nodes and associated update rules respectively. We will introduce these macros in the context of a root cause analysis on a student's test results. This demo is inspired by Chapter 2 of \"Model-Based Machine Learning\" by Winn et al.","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/#Problem-Statement","page":"Assessing People’s Skills","title":"Problem Statement","text":"","category":"section"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"We consider a student who takes a test that consists of three questions. Answering each question correctly requires a combination of skill and attitude. More precisely, has the student studied for the test, and have they partied the night before?","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"We model the result for question i as a continuous variable r_iin01, and skill/attitude as a binary variable s_i in 0 1, where s_1 represents whether the student has partied, and s_2 and s_3 represent whether the student has studied the chapters for the corresponding questions.","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"We assume the following logic:","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"If the student is alert (has not partied), then they will score on the first question;\nIf the student is alert or has studied chapter two, then they will score on question two;\nIf the student can answer question two and has studied chapter three, then they will score on question three.","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/#Generative-Model-Definition","page":"Assessing People’s Skills","title":"Generative Model Definition","text":"","category":"section"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"To model the probability for correct answers, we assume a latent state variable t_i in 01. The dependencies between the variables can then be modeled by the following Bayesian network:","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"(s_1)   (s_2)   (s_3)\n  |       |       |\n  v       v       v\n(t_1)-->(t_2)-->(t_3)\n  |       |       |\n  v       v       v\n(r_1)   (r_2)   (r_3)","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"As prior beliefs, we assume that a student is equally likely to study/party or not: s_i sim Ber(05) for all i. Next, we model the domain logic as beginaligned   t_1 = s_1\n  t_2 = t_1  s_2\n  t_3 = t_2  s_3 endaligned For the scoring results we might not have a specific forward model in mind. However, we can define a backward mapping, from continuous results to discrete latent variables, as  t_i sim Ber(s_i) for all i.","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/#Custom-Nodes-and-Rules","page":"Assessing People’s Skills","title":"Custom Nodes and Rules","text":"","category":"section"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"The backward mapping from results to latents is quite specific to our application. Moreover, it does not define a proper generative forward model. In order to still define a full generative model for our application, we can define a custom Score node and define an update rule that implements the backward mapping from scores to latents as a message.","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"In RxInfer, the @node macro defines a factor node. This macro accepts the new node type, an indicator for a stochastic or deterministic relationship, and a list of interfaces.","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"using RxInfer, Random\n\n# Create Score node\nstruct Score end\n\n@node Score Stochastic [out, in]","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"We can now define the backward mapping as a sum-product message through the @rule macro. This macro accepts the node type, the (outbound) interface on which the message is sent, any relevant constraints, and the message/distribution types on the remaining (inbound) interfaces.","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"# Adding update rule for the Score node\n@rule Score(:in, Marginalisation) (q_out::PointMass,) = begin\n    return Bernoulli(mean(q_out))\nend","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/#Generative-Model-Specification","page":"Assessing People’s Skills","title":"Generative Model Specification","text":"","category":"section"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"We can now build the full generative model.","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"# GraphPPL.jl exports the `@model` macro for model specification\n# It accepts a regular Julia function and builds an FFG under the hood\n@model function skill_model(r)\n\n    local s\n    # Priors\n    for i in eachindex(r)\n        s[i] ~ Bernoulli(0.5)\n    end\n\n    # Domain logic\n    t[1] ~ ¬s[1]\n    t[2] ~ t[1] || s[2]\n    t[3] ~ t[2] && s[3]\n\n    # Results\n    for i in eachindex(r)\n        r[i] ~ Score(t[i])\n    end\nend","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/#Inference-Specification","page":"Assessing People’s Skills","title":"Inference Specification","text":"","category":"section"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"Let us assume that a student scored very low on all questions and set up and execute an inference algorithm.","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"test_results = [0.1, 0.1, 0.1]\ninference_result = infer(\n    model=skill_model(),\n    data=(r=test_results,)\n)","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"Inference results:\n  Posteriors       | available for (s, t)","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/#Results","page":"Assessing People’s Skills","title":"Results","text":"","category":"section"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"# Inspect the results\nmap(params, inference_result.posteriors[:s])","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"3-element Vector{Tuple{Float64}}:\n (0.9872448979591837,)\n (0.06377551020408162,)\n (0.4719387755102041,)","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"These results suggest that this particular student was very likely out on the town last night.","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/#examples-predicting-bike-rental-demand","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"","category":"section"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Important Note: This notebook primarily aims to show how to manage missing data and generate predictions using a model. It does not serve as a comprehensive guide to building the most advanced model for this dataset or showcase the best practices in feature engineering. To keep explanations clear, we will use simplified assumptions and straightforward models. For applications in the real world, you should employ more sophisticated approaches and feature engineering methods. However, we will present a more complex model towards the end of this notebook for you to explore, albeit with less detailed guidance.","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"using RxInfer","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/#Preamble:-Enabling-Predictions","page":"Predicting Bike Rental Demand","title":"Preamble: Enabling Predictions","text":"","category":"section"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"RxInfer.jl facilitates predictions in two primary ways. ","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Implicit Prediction: By adding missing instances directly into the data, which are then treated as regular observations during inference.\nExplicit Prediction: By defining a separate data variable in the model. This approach doesn't necessitate passing missing instances as the data variable but does require specifying the predictvar argument in the inference function.","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/#Example","page":"Predicting Bike Rental Demand","title":"Example","text":"","category":"section"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Consider the following model:","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"@model function example_model(y)\n\n    h ~ NormalMeanPrecision(0, 1.0)\n    x ~ NormalMeanPrecision(h, 1.0)\n    y ~ NormalMeanPrecision(x, 10.0)\nend","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# Implicit Prediction\nresult = infer(model = example_model(), data = (y = missing,))","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Inference results:\n  Posteriors       | available for (h, x)\n  Predictions      | available for (y)","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# Explicit Prediction\nresult = infer(model = example_model(), predictvars = (y = KeepLast(),))","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Inference results:\n  Posteriors       | available for (h, x)\n  Predictions      | available for (y)","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Both approaches yield the same results, but the choice depends on personal preferences and the model's structure. In scenarios with a clear distinction between observed and predicted variables, the explicit method is preferable. However, our subsequent example will not differentiate between observations and predictions, as it utilizes a state space representation.","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"using CSV, DataFrames, Plots","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/#Objective","page":"Predicting Bike Rental Demand","title":"Objective","text":"","category":"section"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"This example aims to simultaneously learn the dynamics of the feature space and predict hourly bike rental demand through reactive message passing, a signature approach of RxInfer.jl.","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/#Dataset-Source","page":"Predicting Bike Rental Demand","title":"Dataset Source","text":"","category":"section"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Data for this example study is sourced from the Kaggle Bike Count Prediction Dataset. For the purpose of this example, the original dataset from Kaggle has been adapted by removing categorical variables such as season, holiday, and working day. Additionally we take only 500 entries. This modification allows us to focus on modeling the continuous variables without additional complexities of handling categorical data. Nevertheless, this extension is feasible within RxInfer.jl.","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# Load the data\ndf = CSV.read(\"../data/bike_count/modified_bicycle.csv\", DataFrame)\ndf[1:10, :]","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"10×6 DataFrame\n Row │ datetime            temp     atemp    humidity  windspeed  count\n     │ String31            Float64  Float64  Float64   Float64    Int64\n─────┼──────────────────────────────────────────────────────────────────\n   1 │ 2011-01-01 0:00:00     9.84   14.395      81.0     0.0        16\n   2 │ 2011-01-01 1:00:00     9.02   13.635      80.0     0.0        40\n   3 │ 2011-01-01 2:00:00     9.02   13.635      80.0     0.0        32\n   4 │ 2011-01-01 3:00:00     9.84   14.395      75.0     0.0        13\n   5 │ 2011-01-01 4:00:00     9.84   14.395      75.0     0.0         1\n   6 │ 2011-01-01 5:00:00     9.84   12.88       75.0     6.0032      1\n   7 │ 2011-01-01 6:00:00     9.02   13.635      80.0     0.0         2\n   8 │ 2011-01-01 7:00:00     8.2    12.88       86.0     0.0         3\n   9 │ 2011-01-01 8:00:00     9.84   14.395      75.0     0.0         8\n  10 │ 2011-01-01 9:00:00    13.12   17.425      76.0     0.0        14","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# we reserve few samples for prediction\nn_future = 24\n\n# `x` is a sequence of observed features\nX = Union{Vector{Float64}, Missing}[[row[i] for i in 2:(ncol(df))-1] for row in eachrow(df)][1:end-n_future]\n\n# `y` is a sequence of \"count\" bicycles\ny = Union{Float64, Missing}[df[:, \"count\"]...][1:end-n_future]\n\nstate_dim = length(X[1]); # dimensionality of feature space","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/#Generative-Model-with-Priors","page":"Predicting Bike Rental Demand","title":"Generative Model with Priors","text":"","category":"section"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"We present a generative model that delineates the latent dynamics of feature evolution, represented by mathbfh_t, and their link to the bike rental counts, mathbfy_t.","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/#Equations-and-Priors","page":"Predicting Bike Rental Demand","title":"Equations and Priors","text":"","category":"section"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Feature Dynamics with Prior:\nPrior: mathbfa sim mathcalN(mathbf0 mathbfI)\nDynamics: mathbfh_t sim mathcalN(mathbfA h_t-1 mathbfQ)\nmathbfA\nis the transition matrix, reshaped from the prior vector mathbfa, and mathbfQ represents process noise.\nNoisy Observations:\nmathbfx_t sim mathcalN(mathbfh_t mathbfP)\nRepresents the observed noisy state of the features.\nCount Prediction with Prior:\nPrior: boldsymboltheta sim mathcalN(mathbf0 mathbfI)\nPrediction: y_t sim mathcalN(textsoftplus(boldsymboltheta^topmathbfh_t) sigma^2)\nModels the bike rental count as influenced by a non-linear transformation of the hidden state.","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/#Interpretation","page":"Predicting Bike Rental Demand","title":"Interpretation","text":"","category":"section"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"This framework aims to simultaneously infer the transition matrix mathbfA and the regression parameters boldsymboltheta, providing a comprehensive view of the feature space dynamics and the count prediction.\nBy employing Gaussian priors on both mathbfa and boldsymboltheta, we incorporate beliefs about their distributions.\nThe inference process aims to discover these underlying dynamics, enabling predictions of both features mathbfx_t and counts y_t.","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# # We augument the dataset with missing entries for 24 hours ahead\nappend!(X, repeat([missing], n_future))\nappend!(y, repeat([missing], n_future));","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# Function to perform the state transition in the model.\n# It reshapes vector `a` into a matrix and multiplies it with vector `x` to simulate the transition.\nfunction transition(a, x)\n    nm, n = length(a), length(x)\n    m = nm ÷ n  # Calculate the number of rows for reshaping 'a' into a matrix\n    A = reshape(a, (m, n))  \n    return A * x\nend","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"transition (generic function with 1 method)","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# The dotsoftplus function combines a dot product and softplus transformation.\n# While useful for ensuring positivity, it may not be the optimal choice for all scenarios,\n# especially if the data suggests other forms of relationships or distributions.\nimport StatsFuns: softplus\ndotsoftplus(a, x) = softplus(dot(a, x))","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"dotsoftplus (generic function with 1 method)","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# model definction\n@model function bicycle_ssm(x, y, h0, θ0, a0, Q, s)\n\n    a ~ a0\n    θ ~ θ0\n    h_prior ~ h0\n\n    h_prev = h_prior\n    for i in eachindex(y)\n        \n        h[i] ~ MvNormal(μ=transition(a, h_prev), Σ=Q)\n        x[i] ~ MvNormal(μ=h[i], Σ=diageye(state_dim))\n        y[i] ~ Normal(μ=dotsoftplus(θ, h[i]), σ²=s)\n        h_prev = h[i]\n    end\n\nend","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# In this example, we opt for a basic Linearization approach for the transition and dotsoftplus functions.\n# However, alternative methods like Unscented or CVI approximations can also be considered.\nbicycle_ssm_meta = @meta begin \n    transition() -> Linearization()\n    dotsoftplus() -> Linearization()\nend","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Meta: \n  transition() -> Linearization()\n  dotsoftplus() -> Linearization()","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# prior_h: Based on first observation, assuming initial state is similar with equal variance.\nprior_h = MvNormalMeanCovariance(X[1], diageye(state_dim))\n# prior_θ, prior_a: No initial bias, parameters independent with equal uncertainty.\nprior_θ = MvNormalMeanCovariance(zeros(state_dim), diageye(state_dim))\nprior_a = MvNormalMeanCovariance(zeros(state_dim^2), diageye(state_dim^2));","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# the deterministic relationsships (transition) and (dotsoftplus) will induce loops in the graph representation of our model, this necessiates the initialization of the messages\nimessages = @initialization begin\n    μ(h) = prior_h\n    μ(a) = prior_a\n    μ(θ) = prior_θ\nend\n# Assumptions about the model parameters:\n# Q: Process noise based on observed features' variance, assuming process variability reflects observed features variability.\n# s: Observation noise based on observed data variance, directly estimating variance in the data, important for predictions\nbicycle_model = bicycle_ssm(h0=prior_h, θ0=prior_θ, a0=prior_a, Q=var(filter(!ismissing, X)).*diageye(state_dim), s=var(filter(!ismissing, y)))\n\nresult = infer(\n    model = bicycle_model,\n    data  = (x=X, y=y), \n    options = (limit_stack_depth = 500, ), \n    returnvars = KeepLast(),\n    predictvars = KeepLast(),\n    initialization = imessages,\n    meta = bicycle_ssm_meta,\n    iterations = 20,\n    showprogress=true,\n)","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Inference results:\n  Posteriors       | available for (a, h, h_prior, θ)\n  Predictions      | available for (y, x)","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# For a sake of this example, we extract only predictions\nmean_y, cov_y = mean.(result.predictions[:y]), cov.(result.predictions[:y])\nmean_x, cov_x = mean.(result.predictions[:x]), var.(result.predictions[:x])\n\nmean_x1, cov_x1 = getindex.(mean_x, 1), getindex.(cov_x, 1)\nmean_x2, cov_x2 = getindex.(mean_x, 2), getindex.(cov_x, 2)\nmean_x3, cov_x3 = getindex.(mean_x, 3), getindex.(cov_x, 3)\nmean_x4, cov_x4 = getindex.(mean_x, 4), getindex.(cov_x, 4);","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"slice = (300, length(y))\ndata = df[:, \"count\"][length(y)-n_future:length(y)]\n\np = scatter(y, \n            color=:darkblue, \n            markerstrokewidth=0,\n            label=\"Observed Count\", \n            alpha=0.6)\n\n# Plotting the mean prediction with variance ribbon\nplot!(mean_y, ribbon=sqrt.(cov_y), \n      color=:orange, \n      fillalpha=0.3,\n      label=\"Predicted Mean ± Std Dev\")\n\n# Adding a vertical line to indicate the start of the future prediction\nvline!([length(y)-n_future], \n       label=\"Prediction Start\", \n       linestyle=:dash, \n       linecolor=:green)\n\n# Future (unobserved) data\nplot!(length(y)-n_future:length(y), data, label=\"Future Count\")\n\n# Adjusting the limits\nxlims!(slice)\n\n# Enhancing the plot with titles and labels\ntitle!(\"Bike Rental Demand Prediction\")\nxlabel!(\"Time\")\nylabel!(\"Bike Count\")\n\n# Adjust the legend\nlegend=:topright\n\n# Show the final plot\ndisplay(p)","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"using Plots\n\n# Define a color palette\npalette = cgrad(:viridis)\n\n# Plot the hidden states with observations\np1 = plot(mean_x1, ribbon=sqrt.(cov_x1), color=palette[1], label=\"Hidden State 1\", legend=:topleft)\nplot!(df[!, :temp], color=:grey, label=\"Temperature\")\nvline!([length(y)-n_future], linestyle=:dash, color=:red, label=\"Prediction Start\")\nxlabel!(\"Time\")\nylabel!(\"Value\")\ntitle!(\"Temperature vs Hidden State 1\")\n\np2 = plot(mean_x2, ribbon=sqrt.(cov_x2), color=palette[2], label=\"Hidden State 2\", legend=:topleft)\nplot!(df[!, :atemp], color=:grey, label=\"Feels-Like Temp\")\nvline!([length(y)-n_future], linestyle=:dash, color=:red, label=\"\")\nxlabel!(\"Time\")\nylabel!(\"Value\")\ntitle!(\"Feels-Like Temp vs Hidden State 2\")\n\np3 = plot(mean_x3, ribbon=sqrt.(cov_x3), color=palette[3], label=\"Hidden State 3\", legend=:topleft)\nplot!(df[!, :humidity], color=:grey, label=\"Humidity\")\nvline!([length(y)-n_future], linestyle=:dash, color=:red, label=\"Prediction Start\")\nxlabel!(\"Time\")\nylabel!(\"Value\")\ntitle!(\"Humidity vs Hidden State 3\")\n\np4 = plot(mean_x4, ribbon=sqrt.(cov_x4), color=palette[4], label=\"Hidden State 4\", legend=:topleft)\nplot!(df[!, :windspeed], color=:grey, label=\"Windspeed\")\nvline!([length(y)-n_future], linestyle=:dash, color=:red, label=\"Prediction Start\")\nxlabel!(\"Time\")\nylabel!(\"Value\")\ntitle!(\"Windspeed vs Hidden State 4\")\n\nfor p in [p1, p2, p3, p4]\n    xlims!(p, first(slice), last(slice))\nend\n\nplot(p1, p2, p3, p4, layout=(2, 2), size=(800, 400))","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/#Improving-the-model","page":"Predicting Bike Rental Demand","title":"Improving the model","text":"","category":"section"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"While our current model's predictions may not closely match real-world data, it's important to recognize that certain assumptions and simplifications were made that might have affected the results. The model is essentially a theoretical framework, highlighting the ability to simultaneously deduce states, parameters, and predictions, with an emphasis on the analysis's predictive aspect.","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"To enhance the model and refine its predictions, we can employ variational message passing. This method enables us to eliminate loops within the model by substituting initial messages with initial marginal distributions. This is achieved by utilizing ContinuousTransition (also referred to as CTransition) and SoftDot (aka softdot) nodes. These nodes facilitate the variational approximation of the transition matrix and the dot product, respectively.","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"transformation = a -> reshape(a, state_dim, state_dim)","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"#31 (generic function with 1 method)","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# model definction\n@model function bicycle_ssm_advanced(x, y, h0, θ0, a0, P0, γ0)\n\n    a ~ a0\n    θ ~ θ0\n    h_prior ~ h0\n    P ~ P0\n    γ ~ γ0\n\n    h_prev = h_prior\n    for i in eachindex(y)\n        \n        h[i] ~ CTransition(h_prev, a, P)\n        x[i]  ~ MvNormal(μ=h[i], Λ=diageye(state_dim))\n        _y[i] ~ softdot(θ, h[i], γ)\n        y[i] ~ Normal(μ=softplus(_y[i]), γ=1e4)\n        h_prev = h[i]\n    end\n\nend","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"bicycle_ssm_advanced_meta = @meta begin \n    softplus() -> Linearization()\n    CTransition() -> CTMeta(transformation)\nend","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Meta: \n  log1pexp() -> Linearization()\n  ContinuousTransition() -> ContinuousTransitionMeta{Main.var\"##WeaveSandBo\nx#225\".var\"#31#32\"}(Main.var\"##WeaveSandBox#225\".var\"#31#32\"())","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"bicycle_ssm_advanced_constraints = @constraints begin\n    q(h_prior, h, a, P, γ, _y, y, θ) = q(h_prior, h)q(a)q(P)q(γ)q(_y, y)q(θ)\nend","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Constraints: \n  q(h_prior, h, a, P, γ, _y, y, θ) = q(h_prior, h)q(a)q(P)q(γ)q(_y, y)q(θ)","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"prior_P = ExponentialFamily.WishartFast(state_dim+2, inv.(var(filter(!ismissing, X))) .* diageye(state_dim))\nprior_a = MvNormalMeanPrecision(ones(state_dim^2), diageye(state_dim^2));\n\nprior_γ = GammaShapeRate(1.0, var(filter(!ismissing, y)))\nprior_h = MvNormalMeanPrecision(X[1], diageye(state_dim))\nprior_θ = MvNormalMeanPrecision(ones(state_dim), diageye(state_dim))\n\nimarginals = @initialization begin \n    q(h) = prior_h\n    q(a) = prior_a\n    q(P) = prior_P\n    q(γ) = prior_γ\n    q(θ) = prior_θ\nend\n\nbicycle_model_advanced = bicycle_ssm_advanced(h0=prior_h, θ0=prior_θ, a0=prior_a, P0=prior_P, γ0=prior_γ)\n\nresult_advanced = infer(\n    model = bicycle_model_advanced,\n    data  = (x=X, y=y), \n    options = (limit_stack_depth = 500, ), \n    returnvars = KeepLast(),\n    predictvars = KeepLast(),\n    initialization = imarginals,\n    constraints = bicycle_ssm_advanced_constraints,\n    meta = bicycle_ssm_advanced_meta,\n    iterations = 10,\n    showprogress=true,\n)","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Inference results:\n  Posteriors       | available for (a, P, _y, γ, h, h_prior, θ)\n  Predictions      | available for (y, x)","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# For a sake of this example, we extract only predictions\nmean_y, cov_y = mean.(result_advanced.predictions[:y]), cov.(result_advanced.predictions[:y])","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"([16.000001019202035, 39.999999977143176, 32.000000296906734, 13.0000012305\n67127, 1.0000017352030168, 1.0000019345942128, 2.0000015515351452, 3.000001\n41495801, 8.000001680638599, 14.00000184505535  …  30.833408925476057, 30.4\n9819166408451, 30.18915358149313, 29.900739393219943, 29.628352363186636, 2\n9.3682245646492, 29.117313865316248, 28.873223915428518, 28.634139368844966\n, 28.400036139088527], [0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0\n001, 0.0001, 0.0001, 0.0001  …  0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0\n001, 0.0001, 0.0001, 0.0001, 0.0001])","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"slice = (300, length(y))\ndata = df[:, \"count\"][length(y)-n_future:length(y)]\n\npa = scatter(y, \n            color=:darkblue, \n            markerstrokewidth=0,\n            label=\"Observed Count\", \n            alpha=0.6)\n\n# Plotting the mean prediction with variance ribbon\nplot!(mean_y, ribbon=sqrt.(cov_y), \n      color=:orange, \n      fillalpha=0.3,\n      label=\"Predicted Mean ± Std Dev\")\n\n# Adding a vertical line to indicate the start of the future prediction\nvline!([length(y)-n_future], \n       label=\"Prediction Start\", \n       linestyle=:dash, \n       linecolor=:green)\n\n# Future (unobserved) data\nplot!(length(y)-n_future:length(y), data, label=\"Future Count\")\n\n# Adjusting the limits\nxlims!(slice)\n\n# Enhancing the plot with titles and labels\ntitle!(\"Advanced model\")\nxlabel!(\"Time\")\nylabel!(\"Bike Count\")\n\n# Adjust the legend\nlegend=:topright\n\n# Show the final plot\nplot(pa, p, size=(800, 400))","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"","category":"page"},{"location":"manuals/comparison/#comparison","page":"RxInfer.jl vs. Others","title":"Comparison to other packages","text":"","category":"section"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"Nowadays there's plenty of probabilistic programming languages and packages available. Although all are based on Bayesian inference, their methodologies vary. This section compares RxInfer.jl against other renowned probabilistic programming languages and packages. The goal is to enlighten potential users about the nuances and guide them in choosing the package that best suits their requirements.","category":"page"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"warning: Warning\nThis comparison is not exhaustive and mirrors the author's hands-on experience with the packages. Others may have undergone more rigorous testing. If you're an author of one of these packages and believe this comparison does not do justice, please reach out, and we will be more than willing to make corrections.\nThe comparison is more qualitative than quantitative, considering the intricacies of upkeeping benchmarking code for perpetually evolving packages.","category":"page"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"Toolbox Universality Efficiency Expressiveness Debugging & Visualization Modularity Inference Engine Language Community & Ecosystem\nRxInfer.jl ~ ✓ ✓ ~ ✓ Message-passing Julia ✗\nForneyLab.jl ✗ ~ ✗ ~ ✗ Message-passing Julia ✗\nInfer.net ~ ✓ ✗ ✓ ✗ Message-passing C# ✗\nPGMax ✗ ✓ ✗ ✓ ✗ Message-passing Python ✗\nTuring.jl ✓ ✗ ✓ ~ ✗ Sampling Julia ✓\nPyMC ✓ ✗ ✓ ✓ ✗ Sampling Python ✓\nNumPyro ✓ ✓ ~ ✓ ✗ Sampling Python ✓\nTensorFlow Probability ✓ ✗ ~ ✓ ✗ Sampling Python ✓\nStan ✓ ✗ ✓ ✓ ✗ Sampling Stan ✓","category":"page"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"(Date of creation: 20/10/2023)","category":"page"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"Legend","category":"page"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"✓ : Full capability or feature is present.\n~ : Partial capability or feature is present.\n✗ : No capability or feature.","category":"page"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"Notes:","category":"page"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"Universality: Denotes the capability to depict a vast array of probabilistic models.\nEfficiency: Highlights computational competence. A \"~\" in this context suggests perceived slowness.\nExpressiveness: Assesses the ability to concisely formulate intricate probabilistic models.\nDebugging & Visualization: Evaluates the suite of tools for model debugging and visualization.\nModularity: Reflects the potential to create models by integrating smaller models.\nInference Engines: Pinpoints the primary inference strategy employed by the toolbox.\nLanguage: Identifies the programming language integral to the toolbox.\nCommunity & Ecosystem: Signifies the vibrancy of the ecosystem, inclusive of tools, libraries, and community backing.","category":"page"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"","category":"page"},{"location":"manuals/comparison/#RxInfer.jl-breakdown","page":"RxInfer.jl vs. Others","title":"RxInfer.jl breakdown","text":"","category":"section"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"Universality: RxInfer.jl shines in formulating models derived from the exponential family distributions. The package encompasses not only commonly used distributions such as Gaussian or Bernoulli, but also specialized stochastic nodes that represents prevalent probabilistic models like Autoregressive models, Gamma Mixture models, among others. Furthermore, RxInfer.jl proficiently manages deterministic transformations of variables from the exponential family, see Delta node. Nevertheless, for models outside the exponential family, RxInfer.jl might not be the good choice. Such models would require the creation of novel nodes and corresponding rules, as illustrated in this section.\nEfficiency: RxInfer.jl distinguishes itself with its inference engine rooted in reactive message passing. This approach is supremely efficient, facilitating real-time propagation of updates across the system, supporting parallelization, interruptibility, and more. \nModularity: Broadly, the toolboxes in the table aren't modular in the truest sense. They don't offer the fusion of models by integrating smaller models. RxInfer.jl on the other hand provides a way to compose different models:","category":"page"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"using RxInfer #hide\n\n@model function inner_inner(τ, y, x)\n    y ~ Normal(τ[1], τ[2] + x)\nend\n\n@model function inner(θ, α)\n    β ~ Normal(0, 1)\n    α ~ Gamma(β, 1)\n    α ~ inner_inner(τ = θ, x = 3)\nend\n\n@model function outer()\n    local w\n    for i = 1:5\n        w[i] ~ inner(θ = Gamma(1, 1))\n    end\n    y ~ inner(θ = w[2:3])\nend","category":"page"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"Expressiveness: RxInfer.jl empowers users to elegantly and concisely craft models, closely mirroring probabilistic notation, thanks to Julia's macro capabilities. To illustrate this, let's consider the following model:","category":"page"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"beginaligned\n x  sim mathrmNormal(00 10)\n w  sim mathrmInverseGamma(10 10)\n y  sim mathrmNormal(x w)\nendaligned","category":"page"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"The model then is expressed in RxInfer.jl as follows:","category":"page"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"using RxInfer #hide\n\n@model function example_model()\n    x ~ Normal(mean = 0.0, var = 1.0)\n    w ~ InverseGamma(α = 1, θ = 1)\n    y ~ Normal(mean = x, var = w)\nend","category":"page"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"Debugging & Visualization: RxInfer.jl does provide a mechanism to debug the inference procedure and visualise the graph structure, even though not as seamlessly as some other packages.","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/#examples-infinite-data-stream","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"","category":"section"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"This example shows the capabilities of RxInfer to perform Bayesian inference on real-time signals. As usual, first, we start with importing necessary packages:","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"using RxInfer, Plots, Random, StableRNGs","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"For demonstration purposes we will create a synthetic environment that has a hidden underlying signal, which we cannot observer directly. Instead, we will observe a noised realisation of this hidden signal:","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"mutable struct Environment\n    rng                   :: AbstractRNG\n    current_state         :: Float64\n    observation_precision :: Float64\n    history               :: Vector{Float64}\n    observations          :: Vector{Float64}\n    \n    Environment(current_state, observation_precision; seed = 123) = begin \n         return new(StableRNG(seed), current_state, observation_precision, [], [])\n    end\nend\n\nfunction getnext!(environment::Environment)\n    environment.current_state = environment.current_state + 1.0\n    nextstate  = 10sin(0.1 * environment.current_state)\n    observation = rand(NormalMeanPrecision(nextstate, environment.observation_precision))\n    push!(environment.history, nextstate)\n    push!(environment.observations, observation)\n    return observation\nend\n\nfunction gethistory(environment::Environment)\n    return environment.history\nend\n\nfunction getobservations(environment::Environment)\n    return environment.observations\nend","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"getobservations (generic function with 1 method)","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/#Model-specification","page":"Infinite Data Stream","title":"Model specification","text":"","category":"section"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"We assume that we don't know the shape of our signal in advance. So we try to fit a simple gaussian random walk with unknown observation noise:","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"@model function kalman_filter(x_prev_mean, x_prev_var, τ_shape, τ_rate, y)\n    x_prev ~ Normal(mean = x_prev_mean, variance = x_prev_var)\n    τ ~ Gamma(shape = τ_shape, rate = τ_rate)\n\n    # Random walk with fixed precision\n    x_current ~ Normal(mean = x_prev, precision = 1.0)\n    y ~ Normal(mean = x_current, precision = τ)\n    \nend\n\n# We assume the following factorisation between variables \n# in the variational distribution\n@constraints function filter_constraints()\n    q(x_prev, x_current, τ) = q(x_prev, x_current)q(τ)\nend","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"filter_constraints (generic function with 1 method)","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/#Prepare-environment","page":"Infinite Data Stream","title":"Prepare environment","text":"","category":"section"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"initial_state         = 0.0\nobservation_precision = 0.1","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"0.1","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"After we have created the environment we can observe how our signal behaves:","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"testenvironment = Environment(initial_state, observation_precision);\n\nanimation = @animate for i in 1:100\n    getnext!(testenvironment)\n    \n    history = gethistory(testenvironment)\n    observations = getobservations(testenvironment)\n    \n    p = plot(size = (1000, 300))\n    \n    p = plot!(p, 1:i, history[1:i], label = \"Hidden signal\")\n    p = scatter!(p, 1:i, observations[1:i], ms = 4, alpha = 0.7, label = \"Observation\")\nend\n\ngif(animation, \"../pics/infinite-data-stream.gif\", fps = 24, show_msg = false);","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/#Filtering-on-static-dataset","page":"Infinite Data Stream","title":"Filtering on static dataset","text":"","category":"section"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"RxInfer is flexible and allows for running inference both on real-time and static datasets. In the next section we show how to perform the filtering procedure on a static dataset. We also will verify our inference procedure by checking on the Bethe Free Energy values:","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"n                  = 300\nstatic_environment = Environment(initial_state, observation_precision);\n\nfor i in 1:n\n    getnext!(static_environment)\nend\n\nstatic_history      = gethistory(static_environment)\nstatic_observations = getobservations(static_environment);\nstatic_datastream   = from(static_observations) |> map(NamedTuple{(:y,), Tuple{Float64}}, (d) -> (y = d, ));","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"function run_static(environment, datastream)\n    \n    # `@autoupdates` structure specifies how to update our priors based on new posteriors\n    # For example, every time we have updated a posterior over `x_current` we update our priors\n    # over `x_prev`\n    autoupdates = @autoupdates begin \n        x_prev_mean, x_prev_var = mean_var(q(x_current))\n        τ_shape = shape(q(τ))\n        τ_rate = rate(q(τ))\n    end\n    \n    init = @initialization begin\n        q(x_current) = NormalMeanVariance(0.0, 1e3) \n        q(τ) = GammaShapeRate(1.0, 1.0)\n    end\n\n    engine = infer(\n        model          = kalman_filter(),\n        constraints    = filter_constraints(),\n        datastream     = datastream,\n        autoupdates    = autoupdates,\n        returnvars     = (:x_current, ),\n        keephistory    = 10_000,\n        historyvars    = (x_current = KeepLast(), τ = KeepLast()),\n        initialization = init,\n        iterations     = 10,\n        free_energy    = true,\n        autostart      = true,\n    )\n    \n    return engine\nend","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"run_static (generic function with 1 method)","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"result = run_static(static_environment, static_datastream);","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"static_inference = @animate for i in 1:n\n    estimated = result.history[:x_current]\n    p = plot(1:i, mean.(estimated[1:i]), ribbon = var.(estimated[1:n]), label = \"Estimation\")\n    p = plot!(static_history[1:i], label = \"Real states\")    \n    p = scatter!(static_observations[1:i], ms = 2, label = \"Observations\")\n    p = plot(p, size = (1000, 300), legend = :bottomright)\nend\n\ngif(static_inference, \"../pics/infinite-data-stream-inference.gif\", fps = 24, show_msg = false);","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"plot(result.free_energy_history, label = \"Bethe Free Energy (averaged)\")","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/#Filtering-on-realtime-dataset","page":"Infinite Data Stream","title":"Filtering on realtime dataset","text":"","category":"section"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"Next lets create a \"real\" infinite stream. We use timer() observable from Rocket.jlto emulate real-world scenario. In our example we are going to generate a new data point every ~41ms (24 data points per second). For demonstration purposes we force stop after n data points, but there is no principled limitation to run inference indefinite:","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"function run_and_plot(environment, datastream)\n    \n    # `@autoupdates` structure specifies how to update our priors based on new posteriors\n    # For example, every time we have updated a posterior over `x_current` we update our priors\n    # over `x_prev`\n    autoupdates = @autoupdates begin \n        x_prev_mean, x_prev_var = mean_var(q(x_current))\n        τ_shape = shape(q(τ))\n        τ_rate = rate(q(τ))\n    end\n    \n    posteriors = []\n    \n    plotfn = (q_current) -> begin \n        IJulia.clear_output(true)\n        \n        push!(posteriors, q_current)\n\n        p = plot(mean.(posteriors), ribbon = var.(posteriors), label = \"Estimation\")\n        p = plot!(gethistory(environment), label = \"Real states\")    \n        p = scatter!(getobservations(environment), ms = 2, label = \"Observations\")\n        p = plot(p, size = (1000, 300), legend = :bottomright)\n\n        display(p)\n    end\n    \n    init = @initialization begin\n        q(x_current) = NormalMeanVariance(0.0, 1e3)\n        q(τ) = GammaShapeRate(1.0, 1.0)\n    end\n\n    engine = infer(\n        model         = kalman_filter(),\n        constraints   = filter_constraints(),\n        datastream    = datastream,\n        autoupdates   = autoupdates,\n        returnvars    = (:x_current, ),\n        initialization = init,\n        iterations    = 10,\n        autostart     = false,\n    )\n    \n    qsubscription = subscribe!(engine.posteriors[:x_current], plotfn)\n    \n    RxInfer.start(engine)\n    \n    return engine\nend","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"run_and_plot (generic function with 1 method)","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"# This example runs in our documentation pipeline, which does not support \"real-time\" execution context\n# We skip this code if run not in Jupyter notebook (see below an example with gif)\nengine = nothing \nif isdefined(Main, :IJulia)\n    timegen      = 41 # 41 ms\n    environment  = Environment(initial_state, observation_precision);\n    observations = timer(timegen, timegen) |> map(Float64, (_) -> getnext!(environment)) |> take(n) # `take!` automatically stops after `n` observations\n    datastream   = observations |> map(NamedTuple{(:y,), Tuple{Float64}}, (d) -> (y = d, ));\n    engine = run_and_plot(environment, datastream)\nend;","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"The plot above is fully interactive and we can stop and unsubscribe from our datastream before it ends:","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"if !isnothing(engine) && isdefined(Main, :IJulia)\n    RxInfer.stop(engine)\n    IJulia.clear_output(true)\nend;","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/#examples-universal-mixtures","page":"Universal Mixtures","title":"Universal Mixtures","text":"","category":"section"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"using RxInfer, Distributions, Random, Plots","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"John and Jane are having a coin toss competition. Before they start, they both have the feeling that something is not right. The coin is unbalanced and favors one side over the other. However, both John and Jane do not know which side is being favored. John thinks that the coin favors heads and Jane thinks tails. Coincidentally, both John and Jane have a strong mathematics background and are aware of the appropriate likelihood function for this experiment","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(y_i mid theta) = mathrmBer(y_i mid theta)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"where y_i in 01 are the outcomes of the coin tosses, i.e. heads or tails, and where theta is the coin parameter. They express their gut feeling about the fairness of the coin in terms of a prior distribution over the coin parameter theta, which represents the probability of the coin landing on heads. Based on their gut feeling and the support of thetain01 they come up with the prior beliefs:","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(theta mid textJohn) = mathrmBeta(theta mid 7 2)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(theta mid textJane) = mathrmBeta(theta mid 2 7)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"rθ = range(0, 1, length = 1000)\np = plot(title = \"prior beliefs\")\nplot!(rθ, (x) -> pdf(Beta(7.0, 2.0), x), fillalpha=0.3, fillrange = 0, label=\"P(θ) John\", c=1)\nplot!(rθ, (x) -> pdf(Beta(2.0, 7.0), x), fillalpha=0.3, fillrange = 0, label=\"p(θ) Jane\", c=3,)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"John and Jane really want to clear the odds and decide to perform a lengthy experiment. They toss the unbalanced coin N = 10 times, because their favorite TV show is cancelled anyway and therefore they have plenty of time. ","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"true_coin = Bernoulli(0.25)\nnr_throws = 10\ndataset = Int.(rand(MersenneTwister(42), true_coin, nr_throws))\nnr_heads, nr_tails = sum(dataset), nr_throws-sum(dataset)\nprintln(\"experimental outcome: \\n - heads: \", nr_heads, \"\\n - tails: \", nr_tails);","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"experimental outcome: \n - heads: 3\n - tails: 7","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"For computing the posterior beliefs p(theta mid y) about the parameter theta, they will perform probabilistic inference in the model based on Bayes' rule. Luckily everything is tractable and therefore they can resort to exact inference. They decide to outsource these tedious computations using RxInfer.jl and specify the following models:","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/#John's-model:","page":"Universal Mixtures","title":"John's model:","text":"","category":"section"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(y theta mid textJohn) = p(theta mid textJohn) prod_i=1^N p(y_i mid theta)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"@model function beta_model_john(y)\n\n    # specify John's prior model over θ\n    θ ~ Beta(7.0, 2.0)\n\n    # create likelihood models\n    y .~ Bernoulli(θ)\n    \nend","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/#Jane's-model:","page":"Universal Mixtures","title":"Jane's model:","text":"","category":"section"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(y theta mid textJane) = p(theta mid textJane) prod_i=1^N p(y_i mid theta)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"@model function beta_model_jane(y)\n\n    # specify Jane's prior model over θ\n    θ ~ Beta(2.0, 7.0)\n\n    # create likelihood models\n    y .~ Bernoulli(θ)\n    \nend","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"Now it is time to figure out whose prior belief was the best and who was actually right. They perform probabilistic inference automatically and compute the Bethe free energy to compare eachothers models. For acyclic models, the Bethe free energy mathrmF_B bounds the model evidence p(y) as ","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"mathrmF_Bpq geq - ln p(y)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"result_john = infer(\n    model = beta_model_john(), \n    data  = (y = dataset, ),\n    free_energy = true,\n)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"Inference results:\n  Posteriors       | available for (θ)\n  Free Energy:     | Real[8.96366]","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"result_jane = infer(\n    model = beta_model_jane(), \n    data  = (y = dataset, ),\n    free_energy = true\n)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"Inference results:\n  Posteriors       | available for (θ)\n  Free Energy:     | Real[6.63988]","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"From these results, they agree that Jane her gut feeling was right all along, as her Bethe free energy is lower and therefore her model evidence is higher. Nonetheless, after the 10 throws, they now have a better idea about the underlying theta parameter. They formulate this through the posterior distributions p(theta mid y textJohn) and p(theta mid y textJane):","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"rθ = range(0, 1, length = 1000)\np = plot(title = \"posterior beliefs\")\nplot!(rθ, (x) -> pdf(result_john.posteriors[:θ], x), fillalpha=0.3, fillrange = 0, label=\"P(θ|y) John\", c=1)\nplot!(rθ, (x) -> pdf(result_jane.posteriors[:θ], x), fillalpha=0.3, fillrange = 0, label=\"p(θ|y) Jane\", c=3,)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"What John and Jane did not know, was that Mary, their neighbour, was overhearing their conversation. She was also curious, but could not see the coin. She did not really know how to formulate a prior distribution over theta, so instead she combined both John and Jane their prior beliefs. She had the feeling that John his assessment was more correct, as he was often going to the casino. As a result, she mixed the prior beliefs of John and Jane with proportions 0.7 and 0.3, respectively. Her model for the environment is specified as","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(y theta c mid textMary) = p(c mid textMary)  p(theta mid textJohn)^c p(theta mid textJane)^1-c prod_i=1^N p(y_i mid theta)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"where c describes the probability of John being correct as ","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(c mid textMary) = mathrmBer(c mid 07)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"The predictive distribution p(theta mid textMary) for theta (similar to the prior beliefs of John and Jane) she obtained from the marginalisation over c as","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(theta mid textMary) = sum_cin01 p(cmidtextMary) p(theta mid textJohn)^c p(theta mid textJane)^1-c = 07 cdot p(theta mid textJohn) + 03 cdot p(theta mid textJane)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"rθ = range(0, 1, length = 1000)\np = plot(title = \"prior belief\")\nplot!(rθ, (x) -> pdf(MixtureDistribution([Beta(2.0, 7.0), Beta(7.0, 2.0)], [ 0.3, 0.7 ]), x), fillalpha=0.3, fillrange = 0, label=\"P(θ) Mary\", c=1)\nplot!(rθ, (x) -> 0.7*pdf(Beta(7.0, 2.0), x), c=3, label=\"\")\nplot!(rθ, (x) -> 0.3*pdf(Beta(2.0, 7.0), x), c=3, label=\"\")","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"She was also interested in the results and used the new Mixture node and addons in ReactiveMP.jl. She specified her model as follows and performed inference in this model:","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"@model function beta_model_mary(y)\n\n    # specify John's and Jane's prior models over θ\n    θ_jane ~ Beta(2.0, 7.0)\n    θ_john ~ Beta(7.0, 2.0)\n\n    # specify initial guess as to who is right\n    john_is_right ~ Bernoulli(0.7) \n\n    # specify mixture prior Distribution\n    θ ~ Mixture(switch = john_is_right, inputs = [θ_jane, θ_john])\n\n    # create likelihood models\n    y .~ Bernoulli(θ)\n    \nend","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"This Mixture node updates the belief over c on the performance of the individual models of both John and Jane using so-called scale factors, as introduced in Nguyen et al.. The specific update rules for this node can be found here.","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"result_mary = infer(\n    model = beta_model_mary(), \n    data  = (y = dataset, ),\n    returnvars = (θ = KeepLast(), θ_john = KeepLast(), θ_jane = KeepLast(), john_is_right = KeepLast()),\n    addons = AddonLogScale(),\n    postprocess = UnpackMarginalPostprocess(),\n)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"Inference results:\n  Posteriors       | available for (john_is_right, θ_john, θ, θ_jane)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"Mary was happy, with her mixture prior, she beat John in terms of performance. However, it was not the best decision to think that John was right. In fact, after the experiment there was only a minor possibility remaining that John was right. Her posterior distribution over theta also changed, and as expected the estimate from Jane was more prominent.","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"rθ = range(0, 1, length = 1000)\np = plot(title = \"posterior belief\")\nplot!(rθ, (x) -> pdf(result_mary.posteriors[:θ], x), fillalpha=0.3, fillrange = 0, label=\"P(θ|y) Mary\", c=1)\nplot!(rθ, (x) -> result_mary.posteriors[:θ].weights[1] * pdf(component(result_mary.posteriors[:θ], 1), x), label=\"\", c=3)\nplot!(rθ, (x) -> result_mary.posteriors[:θ].weights[2] * pdf(component(result_mary.posteriors[:θ], 2), x), label=\"\", c=3)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/overview/#examples-basic_examples-overview","page":"Overview","title":"Basic examples","text":"","category":"section"},{"location":"examples/basic_examples/overview/","page":"Overview","title":"Overview","text":"This section contains a set of examples for Bayesian Inference with RxInfer package in various probabilistic models.","category":"page"},{"location":"examples/basic_examples/overview/","page":"Overview","title":"Overview","text":"note: Note\nAll examples have been pre-generated automatically from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/basic_examples/overview/","page":"Overview","title":"Overview","text":"Basic examples contain \"Hello World!\" of Bayesian inference in RxInfer.","category":"page"},{"location":"examples/basic_examples/overview/","page":"Overview","title":"Overview","text":"Coin toss model (Beta-Bernoulli): An example of Bayesian inference in Beta-Bernoulli model with IID observations.\nBayesian Linear Regression Tutorial: An extensive tutorial on Bayesian linear regression with RxInfer with a lot of examples, including multivariate and hierarchical linear regression.\nKalman filtering and smoothing: In this demo, we are interested in Bayesian state estimation in different types of State-Space Models, including linear, nonlinear, and cases with missing observations\nPredicting Bike Rental Demand: An illustrative guide to implementing prediction mechanisms within RxInfer.jl, using bike rental demand forecasting as a contextual example.\nHow to train your Hidden Markov Model: An example of structured variational Bayesian inference in Hidden Markov Model with unknown transition and observational matrices.","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/#examples-global-parameter-optimisation","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"","category":"section"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"This notebook demonstrates how to optimize parameters in state space models using external optimization packages, such as Optim.jl and Flux.jl. We utilize RxInfer.jl, a powerful package for inference in probabilistic models.","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"By the end of this notebook, you will have practical knowledge of global parameter optimization in state space models. You will learn how to optimize parameters in both univariate and multivariate state space models, and harness the power of external optimization packages such as Optim.jl and Flux.jl.","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/#Univariate-State-Space-Model","page":"Global Parameter Optimisation","title":"Univariate State Space Model","text":"","category":"section"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Let us try use the following simple state space model:","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"beginaligned\n    x_t = x_t-1 + c \n    y_t sim mathcalNleft(x_t p right) \nendaligned","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"with prior x_0 sim mathcalN(m_x_0 v_x_0). Our goal is to optimize parameters c and m_x_0.","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"using RxInfer, BenchmarkTools, Random, LinearAlgebra, Plots","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"@model function smoothing(y, x0, c, P)\n    \n    x_prior ~ Normal(mean = mean(x0), var = var(x0)) \n    x_prev = x_prior\n\n    for i in eachindex(y)\n        x[i] ~ x_prev + c\n        y[i] ~ Normal(mean = x[i], var = P)\n        x_prev = x[i]\n    end\nend","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"rng = MersenneTwister(42)\n\nP      = 1.0\nn      = 250\nc_real = -5.0\ndata   = c_real .+ collect(1:n) + rand(rng, Normal(0.0, sqrt(P)), n);","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# c[1] is C\n# c[2] is μ0\nfunction f(c)\n    x0_prior = NormalMeanVariance(c[2], 100.0)\n    result = infer(\n        model = smoothing(x0 = x0_prior, c = c[1], P = P), \n        data  = (y = data,), \n        free_energy = true\n    )\n    return result.free_energy[end]\nend","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"f (generic function with 1 method)","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"using Optim","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"res = optimize(f, ones(2), GradientDescent(), Optim.Options(g_tol = 1e-3, iterations = 100, store_trace = true, show_trace = true, show_every = 10))","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Iter     Function value   Gradient norm \n     0     3.651509e+02     1.001412e+03\n * time: 0.023235082626342773\n * Status: success\n\n * Candidate solution\n    Final objective value:     3.645772e+02\n\n * Found with\n    Algorithm:     Gradient Descent\n\n * Convergence measures\n    |x - x'|               = 1.18e-06 ≰ 0.0e+00\n    |x - x'|/|x'|          = 2.30e-07 ≰ 0.0e+00\n    |f(x) - f(x')|         = 9.13e-07 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 2.51e-09 ≰ 0.0e+00\n    |g(x)|                 = 2.56e-06 ≤ 1.0e-03\n\n * Work counters\n    Seconds run:   17  (vs limit Inf)\n    Iterations:    9\n    f(x) calls:    67\n    ∇f(x) calls:   67","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"res.minimizer # Real values are indeed (c = 1.0 and μ0 = -5.0)","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"2-element Vector{Float64}:\n  1.0007749243942134\n -5.14368311177876","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"println(\"Real value vs Optimized\")\nprintln(\"Real:      \", [ 1.0, c_real ])\nprintln(\"Optimized: \", res.minimizer)","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Real value vs Optimized\nReal:      [1.0, -5.0]\nOptimized: [1.0007749243942134, -5.14368311177876]","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/#Multivariate-state-space-model","page":"Global Parameter Optimisation","title":"Multivariate state space model","text":"","category":"section"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Let us consider the multivariate state space model:","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"beginaligned\n    mathbfx_t sim mathcalNleft(mathbfAx_t-1 mathbfQ right) \n    mathbfy_t sim mathcalNleft(mathbfx_t mathbfP right) \nendaligned","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"with prior ","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"beginaligned\nmathbfx_0 sim mathcalN(mathbfm_x_0 mathbfV_x_0)\nendaligned","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"and transition matrix ","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"beginaligned\nmathbfA = beginbmatrix costheta  -sintheta  sintheta  costheta endbmatrix\nendaligned","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Covariance matrices mathbfV_x_0, mathbfP and mathbfQ are known. Our goal is to optimize parameters mathbfm_x_0 and theta.","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"using RxInfer, BenchmarkTools, Random, LinearAlgebra, Plots","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"@model function rotate_ssm(y, θ, x0, Q, P)\n    \n    x_prior ~ MvNormal(mean = mean(x0), cov = cov(x0))\n    x_prev = x_prior\n    \n    A = [ cos(θ) -sin(θ); sin(θ) cos(θ) ]\n    \n    for i in eachindex(y)\n        x[i] ~ MvNormal(mean = A * x_prev, covariance = Q)\n        y[i] ~ MvNormal(mean = x[i], covariance = P)\n        x_prev = x[i]\n    end\n    \nend","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# Generate data\nfunction generate_rotate_ssm_data()\n    rng = MersenneTwister(1234)\n\n    θ = π / 8\n    A = [ cos(θ) -sin(θ); sin(θ) cos(θ) ]\n    Q = Matrix(Diagonal(1.0 * ones(2)))\n    P = Matrix(Diagonal(1.0 * ones(2)))\n\n    n = 300\n\n    x_prev = [ 10.0, -10.0 ]\n\n    x = Vector{Vector{Float64}}(undef, n)\n    y = Vector{Vector{Float64}}(undef, n)\n\n    for i in 1:n\n        \n        x[i] = rand(rng, MvNormal(A * x_prev, Q))\n        y[i] = rand(rng, MvNormal(x[i], Q))\n        \n        x_prev = x[i]\n    end\n\n    return θ, A, Q, P, n, x, y\nend","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"generate_rotate_ssm_data (generic function with 1 method)","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"θ, A, Q, P, n, x, y = generate_rotate_ssm_data();","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"px = plot()\n\npx = plot!(px, getindex.(x, 1), ribbon = diag(Q)[1] .|> sqrt, fillalpha = 0.2, label = \"real₁\")\npx = plot!(px, getindex.(x, 2), ribbon = diag(Q)[2] .|> sqrt, fillalpha = 0.2, label = \"real₂\")\n\nplot(px, size = (1200, 450))","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"function f(θ)\n    x0 = MvNormalMeanCovariance([ θ[2], θ[3] ], Matrix(Diagonal(0.01 * ones(2))))\n    result = infer(\n        model = rotate_ssm(θ = θ[1], x0 = x0, Q = Q, P = P), \n        data  = (y = y,), \n        free_energy = true\n    )\n    return result.free_energy[end]\nend","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"f (generic function with 1 method)","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"res = optimize(f, zeros(3), LBFGS(), Optim.Options(f_tol = 1e-14, g_tol = 1e-12, show_trace = true, show_every = 10))","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Iter     Function value   Gradient norm \n     0     2.192003e+04     9.032537e+04\n * time: 7.510185241699219e-5\n * Status: success (objective increased between iterations)\n\n * Candidate solution\n    Final objective value:     1.161372e+03\n\n * Found with\n    Algorithm:     L-BFGS\n\n * Convergence measures\n    |x - x'|               = 2.94e-09 ≰ 0.0e+00\n    |x - x'|/|x'|          = 2.59e-10 ≰ 0.0e+00\n    |f(x) - f(x')|         = 6.82e-13 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 5.87e-16 ≤ 1.0e-14\n    |g(x)|                 = 9.39e-08 ≰ 1.0e-12\n\n * Work counters\n    Seconds run:   46  (vs limit Inf)\n    Iterations:    9\n    f(x) calls:    58\n    ∇f(x) calls:   58","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"println(\"Real value vs Optimized\")\nprintln(\"Real:      \", θ)\nprintln(\"Optimized: \", res.minimizer[1])\n\n@show sin(θ), sin(res.minimizer[1])\n@show cos(θ), cos(res.minimizer[1])","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Real value vs Optimized\nReal:      0.39269908169872414\nOptimized: 0.39293324813955327\n(sin(θ), sin(res.minimizer[1])) = (0.3826834323650898, 0.382899763452979)\n(cos(θ), cos(res.minimizer[1])) = (0.9238795325112867, 0.9237898955648155)\n(0.9238795325112867, 0.9237898955648155)","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"x0 = MvNormalMeanCovariance([ res.minimizer[2], res.minimizer[3] ], Matrix(Diagonal(100.0 * ones(2))))\n\nresult = infer(\n    model = rotate_ssm(θ = res.minimizer[1], x0 = x0, Q = Q, P = P), \n    data  = (y = y,), \n    free_energy = true\n)\n\nxmarginals = result.posteriors[:x]\n\npx = plot()\n\npx = plot!(px, getindex.(x, 1), ribbon = diag(Q)[1] .|> sqrt, fillalpha = 0.2, label = \"real₁\")\npx = plot!(px, getindex.(x, 2), ribbon = diag(Q)[2] .|> sqrt, fillalpha = 0.2, label = \"real₂\")\npx = plot!(px, getindex.(mean.(xmarginals), 1), ribbon = getindex.(var.(xmarginals), 1) .|> sqrt, fillalpha = 0.5, label = \"inf₁\")\npx = plot!(px, getindex.(mean.(xmarginals), 2), ribbon = getindex.(var.(xmarginals), 2) .|> sqrt, fillalpha = 0.5, label = \"inf₂\")\n\nplot(px, size = (1200, 450))","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/#Learning-Kalman-filter-with-LSTM-driven-dynamic","page":"Global Parameter Optimisation","title":"Learning Kalman filter with LSTM driven dynamic","text":"","category":"section"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"In this example, our focus is on Bayesian state estimation in a Nonlinear State-Space Model. Specifically, we will utilize the time series generated by the Lorenz system as an example. ","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Our objective is to compute the marginal posterior distribution of the latent (hidden) state x_k at each time step k, considering the history of measurements up to that time step:","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"\np(x_k  y_1k)\n\n\nThe above expression represents the probability distribution of the latent state x_k given the measurements y_1k up to time step k\n\njulia\nusing RxInfer BenchmarkTools Flux ReverseDiff Random Plots LinearAlgebra ProgressMeter JLD StableRNGs\n\n\n\n\n Generate data\n\njulia\n Lorenz system equations to be used to generate dataset\nBasekwdef mutable struct Lorenz\n    dtFloat64\n    σFloat64\n    ρFloat64\n    βFloat64\n    xFloat64\n    yFloat64\n    zFloat64\nend\n\nfunction step(lLorenz)\n    dx = lσ * (ly - lx)         lx += ldt * dx\n    dy = lx * (lρ - lz) - ly   ly += ldt * dy\n    dz = lx * ly - lβ * lz     lz += ldt * dz\nend\n\n\n\n\njulia\n Dataset\nrng = StableRNG(999)\n\nordered_dataset = \nordered_parameters = \nfor σ = 1115\n    for ρ = 2327\n        for β_nom = 69\n            attractor = Lorenz(002 σ ρ β_nom30 1 1 1)\n            noise_free_data = 10 10 10\n            for i=199\n                step(attractor)\n                push(noise_free_data attractorx attractory attractorz)\n            end\n            push(ordered_dataset noise_free_data)\n            push(ordered_parameters σ ρ β_nom30)\n        end\n    end\nend\n\nnew_order = collect(1100)\nshuffle(rngnew_order)\n\ndataset =   noisy dataset\nnoise_free_dataset =   noise free dataset\nlorenz_parameters = \n\nfor i in new_order\n    local data = \n    push(noise_free_dataset ordered_dataseti)\n    push(lorenz_parameters ordered_parametersi)\n    for nfd in ordered_dataseti\n        push(datanfd+randn(rng3))\n    end\n    push(dataset data)\nend\n\ntrainset = dataset160\nvalidset = dataset6180\ntestset = dataset81end\n\nnoise_free_trainset = noise_free_dataset160\nnoise_free_validset = noise_free_dataset6180\nnoise_free_testset = noise_free_dataset81end\n\n\n\n\n\n Data visualization\n\njulia\none_nonoise=noise_free_trainset1\none=trainset1\ngx gy gz = zeros(100) zeros(100) zeros(100)\nrx ry rz = zeros(100) zeros(100) zeros(100)\nfor i=1100\n    rxi ryi rzi = onei1 onei2 onei3\n    gxi gyi gzi = one_nonoisei1 one_nonoisei2 one_nonoisei3\nend\np1=plot(rxrylabel=Noise observations)\np1=plot(gxgylabel=True state)\nxlabel(x)\nylabel(y)\np2=plot(rxrzlabel=Noise observations)\np2=plot(gxgzlabel=True state)\nxlabel(x)\nylabel(z)\np3=plot(ryrzlabel=Noise observations)\np3=plot(gygzlabel=True state)\nxlabel(y)\nylabel(z)\nplot(p1 p2 p3 size = (800 200)layout=(13))\n\n\n(assetsexamplesGlobal Parameter Optimisation_22_1png)\n\n\n Inference\n\n\nWe use the following state-space model representation\n\nbeginaligned\nx_k sim p(x_k  x_k-1) \ny_k sim p(y_k  x_k)\nendaligned","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"where x_k sim p(x_k  x_k-1) represents the hidden dynamics of our system.  The hidden dynamics of the Lorenz system exhibit nonlinearities and hence cannot be solved in the closed form. One manner of solving this problem is by introducing a neural network to approximate the transition matrix of the Lorenz system. ","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"beginaligned\nA_k-1=NN(y_k-1) \np(x_k  x_k-1)=mathcalN(x_k  A_k-1x_k-1 Q) \np(y_k  x_k)=mathcalN(y_k  Bx_k R)\nendaligned","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"where NN is the neural network. The input is the observation y_k-1, and output is the trasition matrix A_k-1. B denote distortion or measurment matrix. Q and R are covariance matrices. Note that the hidden state x_k comprises three coordinates, i.e. x_k = (rx_k ry_k rz_k)","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"By employing this state-space model representation and utilizing the neural network approximation, we can estimate the hidden dynamics and perform inference in the Lorenz system.","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# Neural Network model\nmutable struct NN\n    InputLayer\n    OutputLater\n    g\n    params\n    function NN(W1,b1,W2_1,W2_2,b2,s2_1,W3,b3)\n        InputLayer = Dense(W1, b1, relu)\n        Lstm = LSTM(W2_1,W2_2,b2,s2_1)\n        OutputLayer = Dense(W3, b3)\n        g = Chain(InputLayer, OutputLayer);\n        new(InputLayer, OutputLayer, g, (W1,b1,W2_1,W2_2,b2,s2_1,W3,b3))\n    end\nend","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/#Model-specification","page":"Global Parameter Optimisation","title":"Model specification","text":"","category":"section"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Note that we treat the trasition matrix A_k-1 as time-varying.","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"#State Space Model\n@model function ssm(y, As, Q, B, R)\n    \n    x_prior_mean = zeros(3)\n    x_prior_cov  = Matrix(Diagonal(ones(3)))\n    \n    x[1] ~ MvNormal(mean = x_prior_mean, cov = x_prior_cov)\n    y[1] ~ MvNormal(mean = B * x[1], cov = R)\n    \n    for i in 2:length(y)\n        x[i] ~ MvNormal(mean = As[i - 1] * x[i - 1], cov = Q) \n        y[i] ~ MvNormal(mean = B * x[i], cov = R)\n    end\nend","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"We set distortion matrix B and the covariance matrices Q and R as identity matrix.","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Q = Matrix(Diagonal(ones(3)))*2\nB = Matrix(Diagonal(ones(3)))\nR = Matrix(Diagonal(ones(3)))\n;","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"We use the inference function in the RxInfer.jl. Before that, we need to bulid a function to get the matrix A output by the neural network. And the A is treated as a datavar in the inference function.","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"function get_matrix_AS(data,W1,b1,W2_1,W2_2,b2,s2_1,W3,b3)\n    n = length(data)\n    neural = NN(W1,b1,W2_1,W2_2,b2,s2_1,W3,b3)\n    Flux.reset!(neural)\n    As  = map((d) -> Matrix(Diagonal(neural.g(d))), data[1:end-1])\n    return As\nend","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"get_matrix_AS (generic function with 1 method)","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"The weights of neural network NN are initialized as follows:","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# Initial model parameters\nW1, b1 = randn(5, 3)./100, randn(5)./100\nW2_1, W2_2, b2, s2_1, s2_2 = randn(5 * 4, 5) ./ 100, randn(5 * 4, 5) ./ 100, randn(5*4) ./ 100, zeros(5), zeros(5)\nW3, b3 = randn(3, 5) ./ 100, randn(3) ./ 100\n;","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Before network training, we show the inference results for the hidden states:","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# Performance on an instance from the testset before training\nindex = 1\ndata=testset[index]\nn=length(data)\nresult = infer(\n    model = ssm(As = get_matrix_AS(data,W1,b1,W2_1,W2_2,b2,s2_1,W3,b3),Q = Q,B = B,R = R), \n    data  = (y = data, ), \n    returnvars = (x = KeepLast(), ),\n    free_energy = true\n)\nx_est=result.posteriors[:x]\nrx, ry, rz = zeros(100), zeros(100), zeros(100)\nrx_est_m, ry_est_m, rz_est_m = zeros(100), zeros(100), zeros(100)\nrx_est_var, ry_est_var, rz_est_var = zeros(100), zeros(100), zeros(100)\n\nfor i=1:100\n    rx[i], ry[i], rz[i] = testset[index][i][1], testset[index][i][2], testset[index][i][3]\n    rx_est_m[i], ry_est_m[i], rz_est_m[i] = mean(x_est[i])[1], mean(x_est[i])[2], mean(x_est[i])[3]\n    rx_est_var[i], ry_est_var[i], rz_est_var[i] = var(x_est[i])[1], var(x_est[i])[2], var(x_est[i])[3]\nend\n\np1 = plot(rx,label=\"Hidden state rx\")\np1 = plot!(rx_est_m,label=\"Inferred states\", ribbon=rx_est_var)\np1 = scatter!(first.(testset[index]), label=\"Observations\", markersize=1.0)\n\np2 = plot(ry,label=\"Hidden state ry\")\np2 = plot!(ry_est_m,label=\"Inferred states\", ribbon=ry_est_var)\np2 = scatter!(getindex.(testset[index], 2), label=\"Observations\", markersize=1.0)\n\np3 = plot(rz,label=\"Hidden state rz\")\np3 = plot!(rz_est_m,label=\"Inferred states\", ribbon=rz_est_var)\np3 = scatter!(last.(testset[index]), label=\"Observations\", markersize=1.0)\n\n\nplot(p1, p2, p3, size = (1000, 300))","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/#Training-network","page":"Global Parameter Optimisation","title":"Training network","text":"","category":"section"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"In this part, we use the Free Energy as the objective function to optimize the weights of network.","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# free energy objective to be optimized during training\nfunction fe_tot_est(W1,b1,W2_1,W2_2,b2,s2_1,W3,b3)\n    fe_ = 0\n    for train_instance in trainset\n        result = infer(\n            model = ssm(n, get_matrix_AS(train_instance,W1,b1,W2_1,W2_2,b2,s2_1,W3,b3),Q,B,R), \n            data  = (y = train_instance, ), \n            returnvars = (x = KeepLast(), ),\n            free_energy = true\n        )\n        fe_ += result.free_energy[end]\n    end\n    return fe_\nend","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"fe_tot_est (generic function with 1 method)","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/#Training","page":"Global Parameter Optimisation","title":"Training","text":"","category":"section"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# Training is a computationally expensive procedure, for the sake of an example we load pre-trained weights\n# Uncomment the following code to train the network manually\n# opt = Flux.Optimise.RMSProp(0.006, 0.95)\n# params = (W1,b1,W2_1,W2_2,b2,s2_1,W3,b3)\n# @showprogress for epoch in 1:800\n#     grads = ReverseDiff.gradient(fe_tot_est, params);\n#     for i=1:length(params)\n#         Flux.Optimise.update!(opt,params[i],grads[i])\n#     end\n# end","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/#Test","page":"Global Parameter Optimisation","title":"Test","text":"","category":"section"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Import the weights of neural network that we have trained.","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"W1a, b1a, W2_1a, W2_2a, b2a, s2_1a, W3, b3a = load(\"../data/nn_prediction/weights.jld\")[\"data\"];","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# Performance on an instance from the testset after training\nindex = 1\ndata = testset[index]\nn = length(data)\nresult = infer(\n    model = ssm(As=get_matrix_AS(data,W1a,b1a,W2_1a,W2_2a,b2a,s2_1a,W3,b3a),Q=Q,B=B,R=R), \n    data  = (y = data, ), \n    returnvars = (x = KeepLast(), ),\n    free_energy = true\n)\nx_est=result.posteriors[:x]\n\ngx, gy, gz = zeros(100), zeros(100), zeros(100)\nrx, ry, rz = zeros(100), zeros(100), zeros(100)\nrx_est_m, ry_est_m, rz_est_m = zeros(100), zeros(100), zeros(100)\nrx_est_var, ry_est_var, rz_est_var = zeros(100), zeros(100), zeros(100)\n\nfor i=1:100\n    gx[i], gy[i], gz[i] = noise_free_testset[index][i][1], noise_free_testset[index][i][2], noise_free_testset[index][i][3]\n    rx[i], ry[i], rz[i] = testset[index][i][1], testset[index][i][2], testset[index][i][3]\n    rx_est_m[i], ry_est_m[i], rz_est_m[i] = mean(x_est[i])[1], mean(x_est[i])[2], mean(x_est[i])[3]\n    rx_est_var[i], ry_est_var[i], rz_est_var[i] = var(x_est[i])[1], var(x_est[i])[2], var(x_est[i])[3]\nend\n\np1 = plot(rx,label=\"Hidden state rx\")\np1 = plot!(rx_est_m,label=\"Inferred states\", ribbon=rx_est_var)\np1 = scatter!(first.(testset[index]), label=\"Observations\", markersize=1.0)\n\np2 = plot(ry,label=\"Hidden state ry\")\np2 = plot!(ry_est_m,label=\"Inferred states\", ribbon=ry_est_var)\np2 = scatter!(getindex.(testset[index], 2), label=\"Observations\", markersize=1.0)\n\np3 = plot(rz,label=\"Hidden state rz\")\np3 = plot!(rz_est_m,label=\"Inferred states\", ribbon=rz_est_var)\np3 = scatter!(last.(testset[index]), label=\"Observations\", markersize=1.0)\n\nplot(p1, p2, p3, size = (1000, 300))","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/#Prediction","page":"Global Parameter Optimisation","title":"Prediction","text":"","category":"section"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"In the above instances, the observations during whole time are available. For prediction task, we can only access to the  observations untill k and estimate the future state at time k+1, k+2, dots,k+T.","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"We can still solve this problem by the trained neural network to approximate the transition matrix. And we can get the one-step prediction in the future. Then, the predicted results are feed into the neural network to generate the transition matrix for the next step, and roll into the future to get the multi-step prediction.","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"beginaligned\nA_k=NN(x_k) \np(x_k+1  x_k)=mathcalN(x_k+1  A_kx_k Q) \nendaligned","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"#Define the prediction function\nmultiplyGaussian(A,m,V) = (A * m, A * V * transpose(A))\nsumGaussians(m1,m2,V1,V2) = (m1 + m2, V1 + V2)\n\nfunction runForward(A,B,Q,R,mh_old,Vh_old)\n    mh_1, Vh_1 = multiplyGaussian(A,mh_old,Vh_old)\n    mh_pred, Vh_pred = sumGaussians(mh_1, zeros(length(mh_old)), Vh_1, Q)\nend\n\nfunction g_predict(mh_old,Vh_old,Q)\n    neural = NN(W1a,b1a,W2_1a,W2_2a,b2a,s2_1a,W3,b3a)\n    # Flux.reset!(neural)\n    As  = map((d) -> Matrix(Diagonal(neural.g(d))), [mh_old])\n    As = As[1]\n    return runForward(As,B,Q,R,mh_old,Vh_old), As\nend","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"g_predict (generic function with 1 method)","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"After k=75, the observations are not available, and we predict the future state from k=76 to the end","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"tt = 75\nmh = mean(x_est[tt])\nVh = cov(x_est[tt])\nmo_list, Vo_list, A_list = [], [], [] \ninv_Q = inv(Q)\nfor t=1:100-tt\n    (mo, Vo), A_t = g_predict(mh,Vh,inv_Q)\n    push!(mo_list, mo)\n    push!(Vo_list, Vh)\n    push!(A_list, A_t)\n    global mh = mo\n    global Vh = Vo\nend","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# Prediction visualization\nrx, ry, rz = zeros(100), zeros(100), zeros(100)\nrx_est_m, ry_est_m, rz_est_m = zeros(100), zeros(100), zeros(100)\nrx_est_var, ry_est_var, rz_est_var = zeros(100), zeros(100), zeros(100)\nfor i=1:tt\n    rx[i], ry[i], rz[i] = testset[index][i][1], testset[index][i][2], testset[index][i][3]\n    rx_est_m[i], ry_est_m[i], rz_est_m[i] = mean(x_est[i])[1], mean(x_est[i])[2], mean(x_est[i])[3]\n    rx_est_var[i], ry_est_var[i], rz_est_var[i] = var(x_est[i])[1], var(x_est[i])[2], var(x_est[i])[3]\nend\nfor i=tt+1:100\n    ii=i-tt\n    rx[i], ry[i], rz[i] = testset[index][i][1], testset[index][i][2], testset[index][i][3]\n    rx_est_m[i], ry_est_m[i], rz_est_m[i] = mo_list[ii][1], mo_list[ii][2], mo_list[ii][3]\n    rx_est_var[i], ry_est_var[i], rz_est_var[i] = Vo_list[ii][1,1], Vo_list[ii][2,2], Vo_list[ii][3,3]\nend\np1 = plot(rx,label=\"Ground truth rx\")\np1 = plot!(rx_est_m,label=\"Inffered state rx\",ribbon=rx_est_var)\np1 = scatter!(first.(testset[index][1:tt]), label=\"Observations\", markersize=1.0)\n\np2 = plot(ry,label=\"Ground truth ry\")\np2 = plot!(ry_est_m,label=\"Inferred states\", ribbon=ry_est_var)\np2 = scatter!(getindex.(testset[index][1:tt], 2), label=\"Observations\", markersize=1.0)\n\np3 = plot(rz,label=\"Ground truth rz\")\np3 = plot!(rz_est_m,label=\"Inferred states\", ribbon=rz_est_var)\np3 = scatter!(last.(testset[index][1:tt]), label=\"Observations\", markersize=1.0)\n\n\nplot(p1, p2, p3, size = (1000, 300),legend=:bottomleft)","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/#examples-autoregressive-models","page":"Autoregressive Models","title":"Autoregressive Models","text":"","category":"section"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"In this example we are going to perform an automated Variational Bayesian Inference for a latent autoregressive model that can be represented as following:","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"beginaligned\np(gamma) = mathrmGamma(gammaa b)\np(mathbftheta) = mathcalN(mathbfthetamathbfmu Sigma)\np(x_tmathbfx_t-1t-k) = mathcalN(x_tmathbftheta^Tmathbfx_t-1t-k gamma^-1)\np(y_tx_t) = mathcalN(y_tx_t tau^-1)\nendaligned","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"where x_t is a current state of our system, mathbfx_t-1t-k is a sequence of k previous states, k is an order of autoregression process, mathbftheta is a vector of transition coefficients, gamma is a precision of state transition process, y_k is a noisy observation of x_k with precision tau.","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"For a more rigorous introduction to Bayesian inference in Autoregressive models we refer to Albert Podusenko, Message Passing-Based Inference for Time-Varying Autoregressive Models.","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"We start with importing all needed packages:","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"using RxInfer, Distributions, LinearAlgebra, Random, Plots, BenchmarkTools, Parameters","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Let's generate some synthetic dataset, we use a predefined sets of coeffcients for k = 1, 3 and 5 respectively:","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# The following coefficients correspond to stable poles\ncoefs_ar_1 = [-0.27002517200218096]\ncoefs_ar_2 = [0.4511170798064709, -0.05740081602446657]\ncoefs_ar_5 = [0.10699399235785655, -0.5237303489793305, 0.3068897071844715, -0.17232255282458891, 0.13323964347539288];","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"function generate_ar_data(rng, n, θ, γ, τ)\n    order        = length(θ)\n    states       = Vector{Vector{Float64}}(undef, n + 3order)\n    observations = Vector{Float64}(undef, n + 3order)\n    \n    γ_std = sqrt(inv(γ))\n    τ_std = sqrt(inv(τ))\n    \n    states[1] = randn(rng, order)\n    \n    for i in 2:(n + 3order)\n        states[i]       = vcat(rand(rng, Normal(dot(θ, states[i - 1]), γ_std)), states[i-1][1:end-1])\n        observations[i] = rand(rng, Normal(states[i][1], τ_std))\n    end\n    \n    return states[1+3order:end], observations[1+3order:end]\nend","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"generate_ar_data (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# Seed for reproducibility\nseed = 123\nrng  = MersenneTwister(seed)\n\n# Number of observations in synthetic dataset\nn = 500\n\n# AR process parameters\nreal_γ = 1.0\nreal_τ = 0.5\nreal_θ = coefs_ar_5\n\nstates, observations = generate_ar_data(rng, n, real_θ, real_γ, real_τ);","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Let's plot our synthetic dataset:","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"plot(first.(states), label = \"Hidden states\")\nscatter!(observations, label = \"Observations\")","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Next step is to specify probabilistic model, inference constraints and run inference procedure with RxInfer. We will specify two different models for Multivariate AR with order k > 1 and for Univariate AR (reduces to simple State-Space-Model) with order k = 1.","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"@model function lar_model(T, y, order, c, τ)\n    \n    \n    # Prior for first state\n    if T === Multivariate\n        γ  ~ Gamma(α = 1.0, β = 1.0)\n        θ  ~ MvNormal(μ = zeros(order), Λ = diageye(order))\n        x0 ~ MvNormal(μ = zeros(order), Λ = diageye(order))\n    else\n        γ  ~ Gamma(α = 1.0, β = 1.0)\n        θ  ~ Normal(μ = 0.0, γ = 1.0)\n        x0 ~ Normal(μ = 0.0, γ = 1.0)\n    end\n    \n    x_prev = x0\n    \n    for i in eachindex(y)\n        \n        x[i] ~ AR(x_prev, θ, γ) \n        \n        if T === Multivariate\n            y[i] ~ Normal(μ = dot(c, x[i]), γ = τ)\n        else\n            y[i] ~ Normal(μ = c * x[i], γ = τ)\n        end\n        \n        x_prev = x[i]\n    end\n\nend","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"@constraints function ar_constraints() \n    q(x0, x, θ, γ) = q(x0, x)q(θ)q(γ)\nend","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"ar_constraints (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"@meta function ar_meta(artype, order, stype)\n    AR() -> ARMeta(artype, order, stype)\nend","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"ar_meta (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"@initialization function ar_init(morder)\n    q(γ) = GammaShapeRate(1.0, 1.0)\n    q(θ) = MvNormalMeanPrecision(zeros(morder), diageye(morder))\nend","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"ar_init (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"morder  = 5\nmartype = Multivariate\nmc      = ReactiveMP.ar_unit(martype, morder)\nmconstraints = ar_constraints()\nmmeta        = ar_meta(martype, morder, ARsafe())\n\nmoptions = (limit_stack_depth = 100, )\n\nmmodel          = lar_model(T=martype, order=morder, c=mc, τ=real_τ)\nmdata           = (y = observations, )\nminitialization = ar_init(morder)\nmreturnvars     = (x = KeepLast(), γ = KeepEach(), θ = KeepEach())\n\n# First execution is slow due to Julia's initial compilation \n# Subsequent runs will be faster (benchmarks are below)\nmresult = infer(\n    model = mmodel, \n    data  = mdata,\n    constraints   = mconstraints,\n    meta          = mmeta,\n    options       = moptions,\n    initialization = minitialization,\n    returnvars    = mreturnvars,\n    free_energy   = true,\n    iterations    = 50, \n    showprogress  = false\n);","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"@unpack x, γ, θ = mresult.posteriors","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Dict{Symbol, Vector} with 3 entries:\n  :γ => GammaShapeRate{Float64}[GammaShapeRate{Float64}(a=251.0, b=47.5918)\n, Ga…\n  :θ => MvNormalWeightedMeanPrecision{Float64, Vector{Float64}, Matrix{Floa\nt64}…\n  :x => MvNormalWeightedMeanPrecision{Float64, Vector{Float64}, Matrix{Floa\nt64}…","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"We will use different initial marginals depending on type of our AR process","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"p1 = plot(first.(states), label=\"Hidden state\")\np1 = scatter!(p1, observations, label=\"Observations\")\np1 = plot!(p1, first.(mean.(x)), ribbon = first.(std.(x)), label=\"Inferred states\", legend = :bottomright)\n\np2 = plot(mean.(γ), ribbon = std.(γ), label = \"Inferred transition precision\", legend = :topright)\np2 = plot!([ real_γ ], seriestype = :hline, label = \"Real transition precision\")\n\np3 = plot(mresult.free_energy, label = \"Bethe Free Energy\")\n\nplot(p1, p2, p3, layout = @layout([ a; b c ]))","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Let's also plot a subrange of our results:","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"subrange = div(n,5):(div(n, 5) + div(n, 5))\n\nplot(subrange, first.(states)[subrange], label=\"Hidden state\")\nscatter!(subrange, observations[subrange], label=\"Observations\")\nplot!(subrange, first.(mean.(x))[subrange], ribbon = sqrt.(first.(var.(x)))[subrange], label=\"Inferred states\", legend = :bottomright)","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"It is also interesting to see where our AR coefficients converge to:","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"let\n    pθ = plot()\n\n    θms = mean.(θ)\n    θvs = var.(θ)\n    \n    l = length(θms)\n\n    edim(e) = (a) -> map(r -> r[e], a)\n\n    for i in 1:length(first(θms))\n        pθ = plot!(pθ, θms |> edim(i), ribbon = θvs |> edim(i) .|> sqrt, label = \"Estimated θ[$i]\")\n    end\n    \n    for i in 1:length(real_θ)\n        pθ = plot!(pθ, [ real_θ[i] ], seriestype = :hline, label = \"Real θ[$i]\")\n    end\n    \n    plot(pθ, legend = :outertopright, size = (800, 300))\nend","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"println(\"$(length(real_θ))-order AR inference Bethe Free Energy: \", last(mresult.free_energy))","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"5-order AR inference Bethe Free Energy: 1024.077566129808","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"We can also run a 1-order AR inference on 5-order AR data:","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"uorder  = 1\nuartype = Univariate\nuc      = ReactiveMP.ar_unit(uartype, uorder)\nuconstraints = ar_constraints()\numeta        = ar_meta(uartype, uorder, ARsafe())\n\nuoptions = (limit_stack_depth = 100, )\numodel         = lar_model(T=uartype, order=uorder, c=uc, τ=real_τ)\nudata          = (y = observations, )\ninitialization = @initialization begin\n    q(γ) = GammaShapeRate(1.0, 1.0)\n    q(θ) = NormalMeanPrecision(0.0, 1.0)\nend\nureturnvars    = (x = KeepLast(), γ = KeepEach(), θ = KeepEach())\n\nuresult = infer(\n    model = umodel, \n    data  = udata,\n    meta  = umeta,\n    constraints    = uconstraints,\n    initialization = initialization,\n    returnvars     = ureturnvars,\n    free_energy    = true,\n    iterations     = 50, \n    showprogress   = false\n);","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"println(\"1-order AR inference Bethe Free Energy: \", last(uresult.free_energy))","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"1-order AR inference Bethe Free Energy: 1025.8792949319945","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"if uresult.free_energy[end] > mresult.free_energy[end]\n    println(\"We can see that, according to final Bethe Free Energy value, in this example 5-order AR process can describe data better than 1-order AR.\")\nelse\n    error(\"AR-1 performs better than AR-5...\") \nend","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"We can see that, according to final Bethe Free Energy value, in this exampl\ne 5-order AR process can describe data better than 1-order AR.","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/#Autoregressive-Moving-Average-Model","page":"Autoregressive Models","title":"Autoregressive Moving Average Model","text":"","category":"section"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Bayesian ARMA model can be effectively implemeted in RxInfer.jl. For theoretical details on Varitional Inference for ARMA model, we refer the reader to the following paper.  The Bayesian ARMA model can be written as follows:","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"beginaligned\ne_t sim mathcalN(0 gamma^-1) quad\ntheta sim mathcalMN(mathbf0 mathbfI) quad\neta sim mathcalMN(mathbf0 mathbfI) \nmathbfh_0 sim mathcalMNleft(beginbmatrix\ne_-1 \ne_-2\nendbmatrix mathbfIright) \nmathbfh_t = mathbfSmathbfh_t-1 + mathbfc e_t-1 \nmathbfx_t = boldsymboltheta^topmathbfx_t-1 + boldsymboleta^topmathbfh_t + e_t \nendaligned","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"where shift matrix mathbfS is","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"beginaligned\nmathbfS = beginpmatrix\n0  0 \n1  0 \nendpmatrix\nendaligned","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"and unit vector mathbfc: ","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"beginaligned\nmathbfc=1 0\nendaligned","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"when MA order is 2","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"In this way, mathbfh_t containing errors e_t can be viewed as hidden state.","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"In short, the Bayesian ARMA model has two intractabilities: (1) induced by the multiplication of two Gaussian RVs, i.e., boldsymboleta^topmathbfh_t, (2) induced by errors e_t that prevents analytical update of precision parameter gamma (this can be easily seen when constructing the Factor Graph, i.e. there is a loop). Both problems can be easily resolved in RxInfer.jl, by creating a hybrid inference algorithm based on Loopy Variational Message Passing.","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# Load packages\nusing RxInfer, LinearAlgebra, CSV, DataFrames, Plots","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# Define shift function\nfunction shift(dim)\n    S = Matrix{Float64}(I, dim, dim)\n    for i in dim:-1:2\n           S[i,:] = S[i-1, :]\n    end\n    S[1, :] = zeros(dim)\n    return S\nend","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"shift (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"@model function ARMA(x, x_prev, h_prior, γ_prior, τ_prior, η_prior, θ_prior, c, b, S)\n    \n    # priors\n    γ  ~ γ_prior\n    η  ~ η_prior\n    θ  ~ θ_prior\n    τ  ~ τ_prior\n    \n    # initial\n    h_0 ~ h_prior\n    z[1] ~ AR(h_0, η, τ)\n    e[1] ~ Normal(mean = 0.0, precision = γ)\n\n    x[1] ~ dot(b, z[1]) + dot(θ, x_prev[1]) + e[1]\n    \n    h_prev = h_0\n    for t in 1:length(x)-1\n        \n        e[t+1] ~ Normal(mean = 0.0, precision = γ)\n        h[t]   ~ S*h_prev + b*e[t]\n        z[t+1] ~ AR(h[t], η, τ)\n        x[t+1] ~ dot(z[t+1], b) + dot(θ, x_prev[t]) + e[t+1]\n        h_prev = h[t]\n    end\nend","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"To validate our model and inference, we will use American Airlines stock data downloaded from Kaggle","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"x_df = CSV.read(\"../data/arma/aal_stock.csv\", DataFrame)","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"1259×7 DataFrame\n  Row │ date        open     high     low      close    volume    Name\n      │ Date        Float64  Float64  Float64  Float64  Int64     String3\n──────┼───────────────────────────────────────────────────────────────────\n    1 │ 2013-02-08    15.07    15.12   14.63     14.75   8407500  AAL\n    2 │ 2013-02-11    14.89    15.01   14.26     14.46   8882000  AAL\n    3 │ 2013-02-12    14.45    14.51   14.1      14.27   8126000  AAL\n    4 │ 2013-02-13    14.3     14.94   14.25     14.66  10259500  AAL\n    5 │ 2013-02-14    14.94    14.96   13.16     13.99  31879900  AAL\n    6 │ 2013-02-15    13.93    14.61   13.93     14.5   15628000  AAL\n    7 │ 2013-02-19    14.33    14.56   14.08     14.26  11354400  AAL\n    8 │ 2013-02-20    14.17    14.26   13.15     13.33  14725200  AAL\n  ⋮   │     ⋮          ⋮        ⋮        ⋮        ⋮        ⋮         ⋮\n 1253 │ 2018-01-30    52.45    53.05   52.36     52.59   4741808  AAL\n 1254 │ 2018-01-31    53.08    54.71   53.0      54.32   5962937  AAL\n 1255 │ 2018-02-01    54.0     54.64   53.59     53.88   3623078  AAL\n 1256 │ 2018-02-02    53.49    53.99   52.03     52.1    5109361  AAL\n 1257 │ 2018-02-05    51.99    52.39   49.75     49.76   6878284  AAL\n 1258 │ 2018-02-06    49.32    51.5    48.79     51.18   6782480  AAL\n 1259 │ 2018-02-07    50.91    51.98   50.89     51.4    4845831  AAL\n                                                         1244 rows omitted","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# we will use \"close\" column\nx_data = filter(!ismissing, x_df[:, 5]);","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# Plot data\nplot(x_data, xlabel=\"day\", ylabel=\"price\", label=false)","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"p_order = 10 # AR\nq_order = 4 # MA","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"4","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# Training set\ntrain_size = 1000\nx_prev_train = [Float64.(x_data[i+p_order-1:-1:i]) for i in 1:length(x_data)-p_order][1:train_size]\nx_train = Float64.(x_data[p_order+1:end])[1:train_size];","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# Test set\nx_prev_test = [Float64.(x_data[i+p_order-1:-1:i]) for i in 1:length(x_data)-p_order][train_size+1:end]\nx_test = Float64.(x_data[p_order+1:end])[train_size+1:end];","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/#Inference","page":"Autoregressive Models","title":"Inference","text":"","category":"section"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# Constraints are needed for performing VMP\narma_constraints = @constraints begin\n    q(z, h_0, h, η, τ, γ,e) = q(h_0)q(z, h)q(η)q(τ)q(γ)q(e)\nend;","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# This cell defines prior knowledge for model parameters\nh_prior = MvNormalMeanPrecision(zeros(q_order), diageye(q_order))\nγ_prior = GammaShapeRate(1e4, 1.0)\nτ_prior = GammaShapeRate(1e2, 1.0)\nη_prior = MvNormalMeanPrecision(zeros(q_order), diageye(q_order))\nθ_prior = MvNormalMeanPrecision(zeros(p_order), diageye(p_order));","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# Model's graph has structural loops, hence, it requires pre-initialisation\narma_initialization = @initialization begin\n    q(h_0) = h_prior\n    μ(h_0) = h_prior\n    q(h) = h_prior\n    μ(h) = h_prior\n    q(γ) = γ_prior\n    q(τ) = τ_prior\n    q(η) = η_prior\n    q(θ) = θ_prior\n\nend\narma_meta       = ar_meta(Multivariate, q_order, ARsafe());","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"c = zeros(p_order); c[1] = 1.0; # AR\nb = zeros(q_order); b[1] = 1.0; # MA\nS = shift(q_order); # MA\n\n\nresult = infer(\n    model = ARMA(x_prev=x_prev_train, h_prior=h_prior, γ_prior=γ_prior, τ_prior=τ_prior, η_prior=η_prior, θ_prior=θ_prior, c=c, b=b, S=S), \n    data  = (x = x_train, ),\n    initialization = arma_initialization,\n    constraints   = arma_constraints,\n    meta          = arma_meta,\n    iterations    = 10,\n    options       = (limit_stack_depth = 400, ),\n);","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"plot(mean.(result.posteriors[:e][end]), ribbon = var.(result.posteriors[:e][end]), label = \"eₜ\")","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# extract posteriors\nh_posterior = result.posteriors[:h][end][end]\nγ_posterior = result.posteriors[:γ][end]\nτ_posterior = result.posteriors[:τ][end]\nη_posterior = result.posteriors[:η][end]\nθ_posterior = result.posteriors[:θ][end];","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/#Prediction","page":"Autoregressive Models","title":"Prediction","text":"","category":"section"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Here we are going to use our inference results in order to predict the dataset itself","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# The prediction function is aimed at approximating the predictive posterior distribution\n# It triggers the rules in the generative order (in future, RxInfer.jl will provide this function out of the box)\nfunction prediction(x_prev, h_posterior, γ_posterior, τ_posterior, η_posterior, θ_posterior, p, q)\n    h_out = MvNormalMeanPrecision(mean(h_posterior), precision(h_posterior))\n    ar_out = @call_rule AR(:y, Marginalisation) (m_x=h_out, q_θ=η_posterior, q_γ=τ_posterior, meta=ARMeta(Multivariate, p, ARsafe()))\n    c = zeros(p); c[1] = 1.0\n    b = zeros(q); b[1] = 1.0\n    ar_dot_out = @call_rule typeof(dot)(:out, Marginalisation) (m_in1=PointMass(b), m_in2=ar_out)\n    θ_out = MvNormalMeanPrecision(mean(θ_posterior), precision(θ_posterior))\n    ma_dot_out = @call_rule typeof(dot)(:out, Marginalisation) (m_in1=PointMass(x_prev), m_in2=θ_out)\n    e_out = @call_rule NormalMeanPrecision(:out, Marginalisation) (q_μ=PointMass(0.0), q_τ=mean(γ_posterior))\n    ar_ma = @call_rule typeof(+)(:out, Marginalisation) (m_in1=ar_dot_out, m_in2=ma_dot_out)  \n    @call_rule typeof(+)(:out, Marginalisation) (m_in1=ar_ma, m_in2=e_out)  \nend","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"prediction (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"predictions = []\nfor x_prev in x_prev_test\n    push!(predictions, prediction(x_prev, h_posterior, γ_posterior, τ_posterior, η_posterior, θ_posterior, p_order, q_order))\n    # after every new prediction we can actually \"retrain\" the model to use the power of Bayesian approach\n    # we will skip this part at this notebook\nend","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"plot(x_test, label=\"test data\", legend=:topleft)\nplot!(mean.(predictions)[1:end], ribbon=std.(predictions)[1:end], label=\"predicted\", xlabel=\"day\", ylabel=\"price\")","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/#examples-how-to-train-your-hidden-markov-model","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"","category":"section"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate()\n;","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"In this example, we'll be tracking a Roomba as it moves throughout a 3-bedroom apartment consisting of a bathroom, a master bedroom, and a living room. It's important to keep track of your AI's, so we want to make sure we can keep tabs on it whenever we leave the apartment. ","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"First, in order to track the Roomba's movements using RxInfer, we need to come up with a model. Since we have a discrete set of rooms in the apartment, we can use a categorical distribution to represent the Roomba's position. There are three  rooms in the apartment, meaning we need three states in our categorical distribution. At time t, let's call the estimate of the Roomba's position s_t.","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"However, we also know that some rooms are more accessible than others, meaning the Roomba is more likely to move between these rooms - for example, it's rare to have a door directly between the bathroom and the master bedroom. We can encode this information using a transition matrix, which we will call A.","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"Our Roomba is equipped with a small camera that tracks the surface it is moving over. We will use this camera to obtain our observations since we know that there is a carpet in the living room, tiles in the bathroom, and hardwood floors in the master bedroom. However, this method is not foolproof, and sometimes the Roomba will make mistakes and mistake the hardwood floor for tiles or the carpet for hardwood. Don't be too hard on the little guy, it's just a Roomba after all.","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"At time t, we will call our observations x_t and encode the mapping from the Roomba's position to the observations in a matrix we call B. B also encodes the likelihood that the Roomba will make a mistake and get the wrong observation. This leaves us with the following model specification:","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"beginaligned\n    s_t  sim mathcalCat(A s_t-1)\n    x_t  sim mathcalCat(B s_t)\nendaligned","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"This type of discrete state space model is known as a Hidden Markov Model or HMM for short. Our goal is to learn the matrices A and B so we can use them to track the whereabouts of our little cleaning agent.","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"using RxInfer, Random, BenchmarkTools, Distributions, LinearAlgebra, Plots","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"In order to generate data to mimic the observations of the Roomba, we need to specify two things: the actual transition probabilities between the states (i.e., how likely is the Roomba to move from one room to another), and the observation distribution (i.e., what type of texture will the Roomba encounter in each room). We can then use these specifications to generate observations from our hidden Markov model (HMM).","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"To generate our observation data, we'll follow these steps:","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"Assume an initial state for the Roomba. For example, we can start the Roomba in the bedroom.\nDetermine where the Roomba went next by drawing from a Categorical distribution with the transition probabilities between the different rooms.\nDetermine the observation encountered in this room by drawing from a Categorical distribution with the corresponding observation probabilities.\nRepeat steps 2-3 for as many samples as we want.","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"The following code implements this process and generates our observation data:","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"\"\"\"\n    rand_vec(rng, distribution::Categorical)\n\nThis function returns a one-hot encoding of a random sample from a categorical distribution. The sample is drawn with the `rng` random number generator.\n\"\"\"\nfunction rand_vec(rng, distribution::Categorical) \n    k = ncategories(distribution)\n    s = zeros(k)\n    drawn_category = rand(rng, distribution)\n    s[drawn_category] = 1.0\n    return s\nend\n\nfunction generate_data(n_samples; seed = 42)\n    \n    rng = MersenneTwister(seed)\n    \n    # Transition probabilities \n    state_transition_matrix = [0.9 0.0 0.1;\n                                                        0.0 0.9 0.1; \n                                                        0.05 0.05 0.9] \n    # Observation noise\n    observation_distribution_matrix = [0.9 0.05 0.05;\n                                                                         0.05 0.9 0.05;\n                                                                         0.05 0.05 0.9] \n    # Initial state\n    s_initial = [1.0, 0.0, 0.0] \n    \n    states = Vector{Vector{Float64}}(undef, n_samples) # one-hot encoding of the states\n    observations = Vector{Vector{Float64}}(undef, n_samples) # one-hot encoding of the observations\n    \n    s_prev = s_initial\n    \n    for t = 1:n_samples\n        s_probvec = state_transition_matrix * s_prev\n        states[t] = rand_vec(rng, Categorical(s_probvec ./ sum(s_probvec)))\n        obs_probvec = observation_distribution_matrix * states[t]\n        observations[t] = rand_vec(rng, Categorical(obs_probvec ./ sum(obs_probvec)))\n        s_prev = states[t]\n    end\n    \n    return observations, states\nend","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"generate_data (generic function with 1 method)","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"We will generate 100 data points to simulate 100 ticks of the Roomba moving through the apartment. x_data will contain the Roomba's measurements of the floor it's currently on, and s_data will contain information on the room the Roomba was actually in.","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"# Test data\nN = 100\nx_data, s_data = generate_data(N);\n\nscatter(argmax.(s_data), leg=false, xlabel=\"Time\",yticks= ([1,2,3],[\"Bedroom\",\"Living room\",\"Bathroom\"]))","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"Now it is time to build our model. As mentioned earlier, we will use Categorical distributions for the states and observations. To learn the A and B matrices we can use MatrixDirichlet priors. For the A-matrix, since we have no apriori idea how the roomba is actually going to move we will assume that it moves randomly. We can represent this by filling our MatrixDirichlet prior on A with ones. Remember that this will get updated once we start learning, so it's fine if our initial guess is not quite accurate. As for the observations, we have good reason to trust our Roomba's measurements. To represent this, we will add large values to the diagonal of our prior on B. However, we also acknowledge that the Roomba is not infallible, so we will add some noise on the off-diagonal entries.","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"Since we will use Variational Inference, we also have to specify inference constraints. We will use a structured variational approximation to the true posterior distribution, where we decouple the variational posterior over the states (q(s_0, s)) from the posteriors over the transition matrices (q(A) and q(B)). This dependency decoupling in the approximate posterior distribution ensures that inference is tractable. Let's build the model!","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"# Model specification\n@model function hidden_markov_model(x)\n    \n    A ~ MatrixDirichlet(ones(3,3))\n    B ~ MatrixDirichlet([ 10.0 1.0 1.0; \n                                            1.0 10.0 1.0; \n                                            1.0 1.0 10.0 ])\n    \n    s_0 ~ Categorical(fill(1.0 / 3.0, 3))\n    \n    s_prev = s_0\n    \n    for t in eachindex(x)\n        s[t] ~ Transition(s_prev, A) \n        x[t] ~ Transition(s[t], B)\n        s_prev = s[t]\n    end\n    \nend\n\n# Constraints specification\n@constraints function hidden_markov_model_constraints()\n    q(s_0, s, A, B) = q(s_0, s)q(A)q(B)\nend","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"hidden_markov_model_constraints (generic function with 1 method)","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"Now it's time to perform inference and find out where the Roomba went in our absence. Did it remember to clean the bathroom?","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"We'll be using Variational Inference to perform inference, which means we need to set some initial marginals as a starting point. RxInfer makes this easy with the vague function, which provides an uninformative guess. If you have better ideas, you can try a different initial guess and see what happens.","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"Since we're only interested in the final result - the best guess about the Roomba's position - we'll only keep the last results. Let's start the inference process!","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"imarginals = @initialization begin\n    q(A) = vague(MatrixDirichlet, 3, 3)\n    q(B) = vague(MatrixDirichlet, 3, 3) \n    q(s) = vague(Categorical, 3)\nend\n\nireturnvars = (\n    A = KeepLast(),\n    B = KeepLast(),\n    s = KeepLast()\n)\n\nresult = infer(\n    model         = hidden_markov_model(), \n    data          = (x = x_data,),\n    constraints   = hidden_markov_model_constraints(),\n    initialization = imarginals, \n    returnvars    = ireturnvars, \n    iterations    = 20, \n    free_energy   = true\n);","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"That was fast! Let's take a look at our results. If we're successful, we should have a good idea about the actual layout of the apartment (a good posterior marginal over A) and about the uncertainty in the roombas observations (A good posterior over B). Let's see if it worked","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"println(\"Posterior Marginal for A:\")\nmean(result.posteriors[:A])","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"Posterior Marginal for A:\n3×3 Matrix{Float64}:\n 0.871955   0.0290008  0.0572283\n 0.0349989  0.817591   0.135675\n 0.0930457  0.153408   0.807097","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"println(\"Posterior Marginal for B:\")\nmean(result.posteriors[:B])","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"Posterior Marginal for B:\n3×3 Matrix{Float64}:\n 0.949636   0.0974741  0.0518997\n 0.0244903  0.833632   0.0277884\n 0.0258734  0.0688939  0.920312","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"Finally, we can check if we were successful in keeping tabs on our Roomba's whereabouts. We can also check if our model has converged by looking at the Free Energy. ","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"p1 = scatter(argmax.(s_data), \n                        title=\"Inference results\", \n                        label = \"Real\", \n                        ms = 6, \n                        legend=:right,\n                        xlabel=\"Time\" ,\n                        yticks= ([1,2,3],[\"Bedroom\",\"Living room\",\"Bathroom\"]),\n                        size=(900,550)\n                        )\n\np1 = scatter!(p1, argmax.(ReactiveMP.probvec.(result.posteriors[:s])),\n                        label = \"Inferred\",\n                        ms = 3\n                        )\n\np2 = plot(result.free_energy, \n                    label=\"Free energy\",\n                    xlabel=\"Iteration Number\"\n                    )\n\nplot(p1, p2, layout = @layout([ a; b ]))","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"Neat! Now you know how to track a Roomba if you ever need to. You also learned how to fit a Hidden Markov Model using RxInfer in the process.","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/#examples-rts-vs-bifm-smoothing","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"","category":"section"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"___Credits to Martin de Quincey___","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"This notebook performs Kalman smoothing on a factor graph using message passing, based on the BIFM Kalman smoother. This notebook is based on:","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"F. Wadehn, “State Space Methods with Applications in Biomedical Signal Processing,” ETH Zurich, 2019. Accessed: Jun. 16, 2021. [Online]. Available: https://www.research-collection.ethz.ch/handle/20.500.11850/344762\nH. Loeliger, L. Bruderer, H. Malmberg, F. Wadehn, and N. Zalmai, “On sparsity by NUV-EM, Gaussian message passing, and Kalman smoothing,” in 2016 Information Theory and Applications Workshop (ITA), Jan. 2016, pp. 1–10. doi: 10.1109/ITA.2016.7888168.","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"We perform Kalman smoothing in the linear state space model, represented by:","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"beginaligned\n    Z_k+1 = A Z_k + B U_k \n    Y_k = C Z_k + W_k\nendaligned","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"with observations Y_k, latent states Z_k and inputs U_k. W_k is the observation noise. A in mathrmR^n times n, B in mathrmR^n times m and C in mathrmR^d times n are the transition matrices in the model. Here n, m and d denote the dimensionality of the latent, input and output dimension, respectively.","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"The corresponding probabilistic model can be represented as ","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"beginaligned\n        p(y z u)\n        = p(z_0) prod_k=1^N p(y_k mid z_k) p(z_kmid z_k-1 u_k-1) p(u_k-1) \n        = mathcalN(z_0 mid mu_z_0 Sigma_z_0) left( prod_k=1^N mathcalN(y_k mid C z_k Sigma_W) delta(z_k - (Az_k-1 + Bu_k-1)) mathcalN(u_k-1 mid mu_i_k-1 Sigma_u_k-1) right)\nendaligned","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/#Import-packages","page":"RTS vs BIFM Smoothing","title":"Import packages","text":"","category":"section"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"using RxInfer, Random, LinearAlgebra, BenchmarkTools, ProgressMeter, Plots, StableRNGs","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/#Data-generation","page":"RTS vs BIFM Smoothing","title":"Data generation","text":"","category":"section"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"function generate_parameters(dim_out::Int64, dim_in::Int64, dim_lat::Int64; seed::Int64 = 123)\n    \n    # define noise levels\n    input_noise  = 500.0\n    output_noise = 50.0\n\n    # create random generator for reproducibility\n    rng = MersenneTwister(seed)\n\n    # generate matrices, input statistics and noise matrices\n    A      = diagm(0.8 .* ones(dim_lat) .+ 0.2 * rand(rng, dim_lat))                                            # size (dim_lat x dim_lat)\n    B      = rand(dim_lat, dim_in)                                                                              # size (dim_lat x dim_in)\n    C      = rand(dim_out, dim_lat)                                                                             # size (dim_out x dim_lat)\n    μu     = rand(dim_in) .* collect(1:dim_in)                                                                  # size (dim_in x 1)\n    Σu     = input_noise  .* collect(Hermitian(randn(rng, dim_in, dim_in) + diagm(10 .+ 10*rand(dim_in))))      # size (dim_in x dim_in)\n    Σy     = output_noise .* collect(Hermitian(randn(rng, dim_out, dim_out) + diagm(10 .+ 10*rand(dim_out))))   # size (dim_out x dim_out)\n    Wu     = inv(Σu)\n    Wy     = inv(Σy)\n    \n    # return parameters\n    return A, B, C, μu, Σu, Σy, Wu, Wy\n\nend;","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"function generate_data(nr_samples::Int64, A::Array{Float64,2}, B::Array{Float64,2}, C::Array{Float64,2}, μu::Array{Float64,1}, Σu::Array{Float64,2}, Σy::Array{Float64,2}; seed::Int64 = 123)\n        \n    # create random data generator\n    rng = StableRNG(seed)\n    \n    # preallocate space for variables\n    z = Vector{Vector{Float64}}(undef, nr_samples)\n    y = Vector{Vector{Float64}}(undef, nr_samples)\n    u = rand(rng, MvNormal(μu, Σu), nr_samples)'\n    \n    # set initial value of latent states\n    z_prev = zeros(size(A,1))\n    \n    # generate data\n    for i in 1:nr_samples\n\n        # generate new latent state\n        z[i] = A * z_prev + B * u[i,:]\n\n        # generate new observation\n        y[i] = C * z[i] + rand(rng, MvNormal(zeros(dim_out), Σy))\n        \n        # generate new observation\n        z_prev .= z[i]\n        \n    end\n    \n    # return generated data\n    return z, y, u\n    \nend","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"generate_data (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"# specify settings\nnr_samples = 200\ndim_out = 3\ndim_in = 3\ndim_lat = 25\n\n# generate parameters\nA, B, C, μu, Σu, Σy, Wu, Wy = generate_parameters(dim_out, dim_in, dim_lat);\n            \n# generate data\ndata_z, data_y, data_u = generate_data(nr_samples, A, B, C, μu, Σu, Σy);\n\n# visualise data\np = Plots.plot(xlabel = \"sample\", ylabel = \"observations\")\n# plot each dimension independently\nfor i in 1:dim_out\n    Plots.scatter!(p, getindex.(data_y, i), label = \"y_$i\", alpha = 0.5, ms = 2)\nend\np","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/#Model-specification","page":"RTS vs BIFM Smoothing","title":"Model specification","text":"","category":"section"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"@model function RTS_smoother(y, A, B, C, μu, Wu, Wy)\n    \n    # fetch dimensionality\n    dim_lat = size(A, 1)\n    dim_out = size(C, 1)\n    \n    # set initial hidden state\n    z_prev ~ MvNormal(mean = zeros(dim_lat), precision = 1e-5*diagm(ones(dim_lat)))\n\n    # loop through observations\n    for i in eachindex(y)\n\n        # specify input as random variable\n        u[i] ~ MvNormal(mean = μu, precision = Wu)\n        \n        # specify updated hidden state\n        z[i] ~ A * z_prev + B * u[i]\n        \n        # specify observation\n        y[i] ~ MvNormal(mean = C * z[i], precision = Wy)\n        \n        # update last/previous hidden state\n        z_prev = z[i]\n\n    end\nend","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"@model function BIFM_smoother(y, A, B, C, μu, Wu, Wy)\n\n    # fetch dimensionality\n    dim_lat = size(A, 1)\n    \n    # set priors\n    z_prior ~ MvNormal(mean = zeros(dim_lat), precision = 1e-5*diagm(ones(dim_lat)))\n    z[1]  ~ BIFMHelper(z_prior)\n    \n    # loop through observations\n    for i in eachindex(y)\n\n        # specify input as random variable\n        u[i]   ~ MvNormal(mean = μu, precision = Wu)\n\n        # specify observation\n        yt[i]  ~ BIFM(u[i], z[i], new(z[i+1])) where { meta = BIFMMeta(A, B, C) }\n        y[i]   ~ MvNormal(mean = yt[i], precision = Wy)\n    end\n    \n    # set final value\n    z[end] ~ MvNormal(mean = zeros(dim_lat), precision = zeros(dim_lat, dim_lat))\nend\n\n@constraints function bifm_constraint()\n    q(z_prior,z) = q(z_prior)q(z)\nend","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"bifm_constraint (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/#Probabilistic-inference","page":"RTS vs BIFM Smoothing","title":"Probabilistic inference","text":"","category":"section"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"function inference_RTS(data_y, A, B, C, μu, Wu, Wy)\n    \n    # In this task the inference is unstable and can diverge\n    meta = @meta begin \n        *() -> ReactiveMP.MatrixCorrectionTools.ClampSingularValues(tiny, Inf)\n    end\n    \n    result = infer(\n        model      = RTS_smoother(A = A, B = B, C = C, μu = μu, Wu = Wu, Wy = Wy),\n        data       = (y = data_y, ),\n        returnvars = (z = KeepLast(), u = KeepLast()),\n        meta = meta\n    )\n    qs = result.posteriors\n    return (qs[:z], qs[:u])\nend","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"inference_RTS (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"function inference_BIFM(data_y, A, B, C, μu, Wu, Wy)\n    result = infer(\n        model      = BIFM_smoother(A = A, B = B, C = C, μu = μu, Wu = Wu, Wy = Wy),\n        data       = (y = data_y, ),\n        constraints = bifm_constraint(),\n        returnvars = (z = KeepLast(), u = KeepLast())\n    )\n    qs = result.posteriors\n    return (qs[:z], qs[:u])\nend","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"inference_BIFM (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/#Experiments-for-200-observations","page":"RTS vs BIFM Smoothing","title":"Experiments for 200 observations","text":"","category":"section"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"z_BIFM, u_BIFM = inference_BIFM(data_y, A, B, C, μu, Wu, Wy)\nz_RTS, u_RTS = inference_RTS(data_y, A, B, C, μu, Wu, Wy);","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"ax1 = Plots.plot(title = \"RTS smoother\", xlabel = \"sample\", ylabel = \"latent state z\")\nax2 = Plots.plot(title = \"BIFM smoother\", xlabel = \"sample\", ylabel = \"latent state z\")\n\nmz_RTS = mean.(z_RTS)\nmz_BIFM = mean.(z_BIFM)\n\n# Do not plot all latent states, otherwise the output is just too cluttered\n# The main idea here is to check that both algorithms return the (approximately) same output\nfor i in 1:5\n    Plots.scatter!(ax1, getindex.(data_z, i), alpha = 0.1, ms = 2, color = :blue, label = nothing)\n    Plots.plot!(ax1, getindex.(mz_RTS, i), label = nothing)\n    Plots.scatter!(ax2, getindex.(data_z, i), alpha = 0.1, ms = 2, color = :blue, label = nothing)    \n    Plots.plot!(ax2, getindex.(mz_BIFM, i), label = nothing)\nend\n\nPlots.plot(ax1, ax2, layout = @layout([ a; b ]))","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"ax1 = Plots.plot(title = \"RTS smoother\", xlabel = \"sample\", ylabel = \"latent state u\")\nax2 = Plots.plot(title = \"BIFM smoother\", xlabel = \"sample\", ylabel = \"latent state u\")\n\nrdata_u = collect(eachrow(data_u))\nmu_RTS = mean.(u_RTS)\nmu_BIFM = mean.(u_BIFM)\n\n# Do not plot all latent states, otherwise the output is just too cluttered\n# The main idea here is to check that both algorithms return the (approximately) same output\nfor i in 1:1\n    Plots.scatter!(ax1, getindex.(rdata_u, i), alpha = 0.1, ms = 2, color = :blue, label = nothing)\n    Plots.plot!(ax1, getindex.(mu_RTS, i), label = nothing)\n    Plots.scatter!(ax2, getindex.(rdata_u, i), alpha = 0.1, ms = 2, color = :blue, label = nothing)    \n    Plots.plot!(ax2, getindex.(mu_BIFM, i), label = nothing)\nend\n\nPlots.plot(ax1, ax2, layout = @layout([ a; b ]))","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/#Benchmark","page":"RTS vs BIFM Smoothing","title":"Benchmark","text":"","category":"section"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"# This example runs in our documentation pipeline, benchmark executes approximatelly in 20 minutes so we bypass it in the documentation\n# For those who are interested in exact benchmark numbers clone this example and set `run_benchmark = true`\nrun_benchmark = false\n\nif run_benchmark\n    trials_range = 30\n    trials_n = 500\n    trials_RTS  = Array{BenchmarkTools.Trial, 1}(undef, trials_range)\n    trials_BIFM = Array{BenchmarkTools.Trial, 1}(undef, trials_range)\n\n\n    @showprogress for k = 1 : trials_range\n\n        # generate parameters\n        local A, B, C, μu, Σu, Σy, Wu, Wy = generate_parameters(3, 3, k);\n                    \n        # generate data|\n        local data_z, data_y, data_u = generate_data(trials_n, A, B, C, μu, Σu, Σy);\n\n        # run inference\n        trials_RTS[k] = @benchmark inference_RTS($data_y, $A, $B, $C, $μu, $Wu, $Wy)\n        trials_BIFM[k] = @benchmark inference_BIFM($data_y, $A, $B, $C, $μu, $Wu, $Wy)\n\n    end\n\n    m_RTS = [median(trials_RTS[k].times) for k=1:trials_range] ./ 1e9\n    q1_RTS = [quantile(trials_RTS[k].times, 0.25) for k=1:trials_range] ./ 1e9\n    q3_RTS = [quantile(trials_RTS[k].times, 0.75) for k=1:trials_range] ./ 1e9\n    m_BIFM = [median(trials_BIFM[k].times) for k=1:trials_range] ./ 1e9\n    q1_BIFM = [quantile(trials_BIFM[k].times, 0.25) for k=1:trials_range] ./ 1e9\n    q3_BIFM = [quantile(trials_BIFM[k].times, 0.75) for k=1:trials_range] ./ 1e9;\n\n    p = Plots.plot(ylabel = \"duration [sec]\", xlabel = \"latent state dimension\", title = \"Benchmark\", yscale = :log)\n    p = Plots.plot!(p, m_RTS, ribbon = ((q1_RTS .- q3_RTS) ./ 2), color = \"blue\", label = \"mean (RTS)\")\n    p = Plots.plot!(p, 1:trials_range, m_BIFM, ribbon = ((q1_BIFM .- q3_BIFM) ./ 2), color = \"orange\", label = \"mean (BIFM)\")\n    Plots.savefig(p, \"../pics/rts_bifm_benchmark.png\")\n    p\nend","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"(Image: )","category":"page"},{"location":"manuals/meta-specification/#user-guide-meta-specification","page":"Meta specification","title":"Meta Specification","text":"","category":"section"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"RxInfer.jl utilizes the GraphPPL.jl package to construct a factor graph representing a probabilistic model, and then employs the ReactiveMP.jl package to conduct variational inference through message passing on this factor graph. Some factor nodes within the ReactiveMP.jl inference engine require an additional structure, known as meta-information. This meta-information can serve various purposes such as providing extra details to nodes, customizing the inference process, or adjusting how nodes compute outgoing messages. For example, the AR node, which models Auto-Regressive processes, needs to know the order of the AR process. Similarly, the GCV node (Gaussian Controlled Variance) requires an approximation method to handle non-conjugate relationships between its variables. To address these needs, RxInfer.jl utilizes the @meta macro from the GraphPPL.jl package to specify node-specific meta-information and contextual details.","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"Here, we only touch upon the basics of the @meta macro. For further details, please consult the official documentation of the GraphPPL.jl package.","category":"page"},{"location":"manuals/meta-specification/#General-syntax","page":"Meta specification","title":"General syntax","text":"","category":"section"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"The @meta macro accepts either a regular Julia function or a single begin ... end block:","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"using RxInfer\n\nstruct MetaObject\n    arg1\n    arg2\nend\n\n@meta function create_meta(arg1, arg2)\n    Normal(y, x) -> MetaObject(arg1, arg2)\nend\n\nmy_meta = @meta begin \n    Normal(y, x) -> MetaObject(1, 2)\nend\n\nnothing #hide","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"In the first case, it returns a function that produces an object containing metadata when called. For instance, to specify meta for an AR node with an order of 5, you can do the following:","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"@meta function ARmodel_meta(num_order)\n    AR() -> ARMeta(Multivariate, num_order, ARsafe())\nend\n\nmy_meta = ARmodel_meta(5)\nnothing #hide","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"In the second case, it directly provides the meta object. The same meta for the AR node can also be defined as follows:","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"num_order = 5\n\nmy_meta = @meta begin \n    AR() -> ARMeta(Multivariate, num_order, ARsafe())\nend\nnothing #hide","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"Both syntax variations provide the same meta specification and there is no preference given to one over the other. ","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"Another example:","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"my_meta = @meta begin \n    GCV(x, k, w) -> GCVMetadata(GaussHermiteCubature(20))\nend\nnothing #hide","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"This meta specification indicates that for every GCV node in the model with x, k and w as connected variables should use the GCVMetadata(GaussHermiteCubature(20)) meta object.","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"You can have a list of as many meta specification entries as possible for different nodes:","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"my_meta = @meta begin \n    GCV(x1, k1, w1) -> GCVMetadata(GaussHermiteCubature(20))\n    AR() -> ARMeta(Multivariate, 5, ARsafe())\nend\nnothing #hide","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"The meta-information object can be used in the infer function that accepts meta keyword argument:","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"inferred_result = infer(\n    model = my_model(arguments...),\n    data  = ...,\n    meta  = my_meta,\n    ...\n)","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"Users can also specify metadata for nodes directly inside @model, without the need to use @meta. For example:","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"@model function my_model()\n    ...\n\n    y ~ AR(x, θ, γ) where { meta = ARMeta(Multivariate, 5, ARsafe()) }\n\n    ...\nend","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"If you add node-specific meta to your model this way, you do not need to use the meta keyword argument in the infer function.","category":"page"},{"location":"manuals/meta-specification/#Create-your-own-meta","page":"Meta specification","title":"Create your own meta","text":"","category":"section"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"Although some nodes in RxInfer.jl already come with their own meta structure, users have the flexibility to define different meta structures for those nodes and also for custom ones. A meta structure is created by using the struct statement in Julia. For example, the following snippet of code illustrates how you can create your own meta structures for your custom node. This section provides a concrete example of how to create and use meta in RxInfer.jl. Suppose that we have the following Gaussian model:","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"beginaligned\n x  sim mathrmNormal(25 05)\n y  sim mathrmNormal(2*x 20)\nendaligned","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"where y is observable data and x is a latent variable. In RxInfer.jl, the inference procedure for this model is well defined without the need of specifying any meta data for the Normal node.","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"using RxInfer\n\n#create data\ny_data = 4.0 \n\n#make model\n@model function gaussian_model(y)\n    x ~ NormalMeanVariance(2.5, 0.5)\n    y ~ NormalMeanVariance(2*x, 2.)\nend\n\n#do inference\ninference_result = infer(\n    model = gaussian_model(),\n    data = (y = y_data,)\n)","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"However, let's say we would like to experiment with message update rules and define a new inference procedure by introducing a meta structure to the Normal node that always yields a message equal to Normal distribution with mean m clamped between lower_limit and upper_limit for the outbound messages of the node. This is done as follows:","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"#create your new meta structure for Normal node\nstruct MetaConstrainedMeanNormal{T}\n    lower_limit :: T\n    upper_limit :: T\nend\n\n#define rules with meta for the Normal node\n@rule NormalMeanVariance(:out, Marginalisation) (q_μ::Any, q_v::Any, meta::MetaConstrainedMeanNormal) = begin\n    return NormalMeanVariance(clamp(mean(q_μ), meta.lower_limit, meta.upper_limit), mean(q_v))\nend\n\n@rule NormalMeanVariance(:μ, Marginalisation) (q_out::Any, q_v::Any, meta::MetaConstrainedMeanNormal) = begin\n    return NormalMeanVariance(clamp(mean(q_out), meta.lower_limit, meta.upper_limit), mean(q_v))\nend","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"#make model\n@model function gaussian_model_with_meta(y)\n    x ~ NormalMeanVariance(2.5, 0.5)\n    y ~ NormalMeanVariance(2*x, 2.)\nend\n\ncustom_meta = @meta begin\n    NormalMeanVariance(y) -> MetaConstrainedMeanNormal(-2, 2)\nend\n\n#do inference\ninference_result = infer(\n    model = gaussian_model(),\n    data = (y = y_data,),\n    meta = custom_meta\n)\n\nprintln(\"Estimated mean for latent state `x` is \", mean(inference_result.posteriors[:x]), \" with standard deviation \", std(inference_result.posteriors[:x]))","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"warning: Warning\nThe above example is not mathematically correct. It is only used to show how we can work with @meta as well as how to create a meta structure for a node in RxInfer.jl.","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"Read more about the @meta macro in the official documentation of GraphPPL","category":"page"},{"location":"manuals/meta-specification/#Adding-metadata-to-nodes-in-submodels","page":"Meta specification","title":"Adding metadata to nodes in submodels","text":"","category":"section"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"Similarly to the @constraints macro, the @meta macro exposes syntax to push metadata to nodes in submodels. With the for meta in submodel syntax we can apply metadata to nodes in submodels. For example, if we use the gaussian_model_with_meta mnodel in a larger model, we can write:","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"custom_meta = @meta begin\n    for meta in gaussian_model_with_meta\n        NormalMeanVariance(y) -> MetaConstrainedMeanNormal(-2, 2)\n    end\nend","category":"page"},{"location":"manuals/inference/overview/#user-guide-inference-execution","page":"Overview","title":"Inference execution","text":"","category":"section"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"The RxInfer inference API supports different types of message-passing algorithms (including hybrid algorithms combining several different types). While RxInfer implements several algorithms to cater to different computational needs and scenarios, the core message-passing algorithms that form the foundation of our inference capabilities are:","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"Belief Propagation\nVariational Message Passing","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"Whereas belief propagation computes exact inference for the random variables of interest, the variational message passing (VMP) is an approximation method that can be applied to a larger range of models.","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"The inference engine itself isn't aware of different algorithm types and simply does message passing between nodes. However, during the model specification stage user may specify different factorisation constraints around factor nodes with the help of the @constraints macro. Different factorisation constraints lead to different message passing update rules. See more documentation about constraints specification in the corresponding section.","category":"page"},{"location":"manuals/inference/overview/#user-guide-inference-execution-automatic-specification","page":"Overview","title":"Automatic inference specification","text":"","category":"section"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"RxInfer exports the infer function to quickly run and test your model with both static and asynchronous (real-time) datasets. See more information about the infer function on the separate documentation section:","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"Static Inference\nStreamlined Inference","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"infer","category":"page"},{"location":"manuals/inference/overview/#RxInfer.infer","page":"Overview","title":"RxInfer.infer","text":"infer(\n    model; \n    data = nothing,\n    datastream = nothing,\n    autoupdates = nothing,\n    initialization = nothing,\n    constraints = nothing,\n    meta = nothing,\n    options = nothing,\n    returnvars = nothing, \n    predictvars = nothing, \n    historyvars = nothing,\n    keephistory = nothing,\n    iterations = nothing,\n    free_energy = false,\n    free_energy_diagnostics = DefaultObjectiveDiagnosticChecks,\n    showprogress = false,\n    callbacks = nothing,\n    addons = nothing,\n    postprocess = DefaultPostprocess(),\n    warn = true,\n    events = nothing,\n    uselock = false,\n    autostart = true,\n    catch_exception = false\n)\n\nThis function provides a generic way to perform probabilistic inference for batch/static and streamline/online scenarios. Returns either an InferenceResult (batch setting) or RxInferenceEngine (streamline setting) based on the parameters used.\n\nArguments\n\nCheck the official documentation for more information about some of the arguments. \n\nmodel: specifies a model generator, required\ndata: NamedTuple or Dict with data, required (or datastream or predictvars)\ndatastream: A stream of NamedTuple with data, required (or data)\nautoupdates = nothing: auto-updates specification, required for streamline inference, see @autoupdates\ninitialization = nothing: initialization specification object, optional, see @initialization\nconstraints = nothing: constraints specification object, optional, see @constraints\nmeta  = nothing: meta specification object, optional, may be required for some models, see @meta\noptions = nothing: model creation options, optional, see ReactiveMPInferenceOptions\nreturnvars = nothing: return structure info, optional, defaults to return everything at each iteration\npredictvars = nothing: return structure info, optional (exclusive for batch inference)\nhistoryvars = nothing: history structure info, optional, defaults to no history (exclusive for streamline inference)\nkeephistory = nothing: history buffer size, defaults to empty buffer (exclusive for streamline inference)\niterations = nothing: number of iterations, optional, defaults to nothing, the inference engine does not distinguish between variational message passing or Loopy belief propagation or expectation propagation iterations\nfree_energy = false: compute the Bethe free energy, optional, defaults to false. Can be passed a floating point type, e.g. Float64, for better efficiency, but disables automatic differentiation packages, such as ForwardDiff.jl\nfree_energy_diagnostics = DefaultObjectiveDiagnosticChecks: free energy diagnostic checks, optional, by default checks for possible NaNs and Infs. nothing disables all checks.\nshowprogress = false: show progress module, optional, defaults to false (exclusive for batch inference)\ncatch_exception  specifies whether exceptions during the inference procedure should be caught, optional, defaults to false (exclusive for batch inference)\ncallbacks = nothing: inference cycle callbacks, optional\naddons = nothing: inject and send extra computation information along messages\npostprocess = DefaultPostprocess(): inference results postprocessing step, optional\nevents = nothing: inference cycle events, optional (exclusive for streamline inference)\nuselock = false: specifies either to use the lock structure for the inference or not, if set to true uses Base.Threads.SpinLock. Accepts custom AbstractLock. (exclusive for streamline inference)\nautostart = true: specifies whether to call RxInfer.start on the created engine automatically or not (exclusive for streamline inference)\nwarn = true: enables/disables warnings\n\n\n\n\n\n","category":"function"},{"location":"manuals/inference/overview/#Note-on-NamedTuples","page":"Overview","title":"Note on NamedTuples","text":"","category":"section"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"When passing NamedTuple as a value for some argument, make sure you use a trailing comma for NamedTuples with a single entry. The reason is that Julia treats returnvars = (x = KeepLast()) and returnvars = (x = KeepLast(), ) expressions differently. This first expression creates (or overwrites!) new local/global variable named x with contents KeepLast(). The second expression (note trailing comma) creates NamedTuple with x as a key and KeepLast() as a value assigned for this key.","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"using RxInfer #hide\n(x = KeepLast()) # defines a variable `x` with the value `KeepLast()`","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"(x = KeepLast(), ) # defines a NamedTuple with `x` as one of the keys and value `KeepLast()`","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"model","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"Also read the Model Specification section.","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"The model argument accepts a model specification as its input. The easiest way to create the model is to use the @model macro.  For example:","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"using RxInfer #hide\n\n@model function beta_bernoulli(y, a, b)\n    x  ~ Beta(a, b)\n    y .~ Bernoulli(x)\nend\n\nresult = infer(\n    model = beta_bernoulli(a = 1, b = 1),\n    data  = (y = [ true, false, false ], )\n)\n\nresult.posteriors[:x]","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"note: Note\nThe model keyword argument does not accept a ProbabilisticModel instance as a value, as it needs to inject constraints and meta during the inference procedure.","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"data","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"Either data or datastream keyword argument are required.  Specifying both data and datastream is not supported and will result in an error. ","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"note: Note\nThe behavior of the data keyword argument depends on the inference setting (batch or streamline).","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"The data keyword argument must be a NamedTuple (or Dict) where keys (of Symbol type) correspond to some arguments defined in the model specification.  For example, if a model defines y in its argument list ","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"using RxInfer #hide\n@model function beta_bernoulli(y, a, b)\n    x  ~ Beta(a, b)\n    y .~ Bernoulli(x)\nend","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"and you want to condition on this argument, then the data field must have an :y key (of Symbol type) which holds the data.  The values in the data must have the exact same shape as its corresponding variable container. E.g. in the exampl above y is being used in the broadcasting  operation, thus it must be a collection of values. a and b arguments, however, could be just single numbers:","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"result = infer(\n    model = beta_bernoulli(),\n    data  = (y = [ true, false, false ], a = 1, b = 1)\n)\n\nresult.posteriors[:x]","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"datastream","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"Also read the Streamlined Inference section.","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"The datastream keyword argument must be an observable that supports subscribe! and unsubscribe! functions (e.g., streams from the Rocket.jl package). The elements of the observable must be of type NamedTuple where keys (of Symbol type) correspond to input arguments defined in the model specification, except for those which are listed in the @autoupdates specification.  For example, if a model defines y as its argument (which is not part of the @autoupdates specification) the named tuple from the observable must have an :y key (of Symbol type). The values in the named tuple must have the exact same shape as the corresponding variable container.","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"initialization","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"Also read the Initialization section.","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"For specific types of inference algorithms, such as variational message passing, it might be required to initialize (some of) the marginals before running the inference procedure in order to break the dependency loop. If this is not done, the inference algorithm will not be executed due to the lack of information and message and/or marginals will not be updated. In order to specify these initial marginals and messages, you can use the initialization argument in combination with the @initialization macro, such as","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"using RxInfer #hide\ninit = @initialization begin\n    # initialize the marginal distribution of x as a vague Normal distribution\n    # if x is a vector, then it simply uses the same value for all elements\n    # However, it is also possible to provide a vector of distributions to set each element individually \n    q(x) = vague(NormalMeanPrecision)\nend","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"returnvars","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"returnvars specifies latent variables of interest and their posterior updates. Its behavior depends on the inference type: streamline or batch.","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"Batch inference:","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"Accepts a NamedTuple or Dict of return variable specifications.\nTwo specifications available: KeepLast (saves the last update) and KeepEach (saves all updates).\nWhen iterations is set, returns every update for each iteration (equivalent to KeepEach()); if nothing, saves the last update (equivalent to KeepLast()).\nUse iterations = 1 to force KeepEach() for a single iteration or set returnvars = KeepEach() manually.","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"result = infer(\n    ...,\n    returnvars = (\n        x = KeepLast(),\n        τ = KeepEach()\n    )\n)","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"Shortcut for setting the same option for all variables:","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"result = infer(\n    ...,\n    returnvars = KeepLast()  # or KeepEach()\n)","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"Streamline inference:","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"For each symbol in returnvars, infer creates an observable stream of posterior updates.\nAgents can subscribe to these updates using the Rocket.jl package.","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"engine = infer(\n    ...,\n    autoupdates = my_autoupdates,\n    returnvars = (:x, :τ),\n    autostart  = false\n)","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"KeepLast\nKeepEach","category":"page"},{"location":"manuals/inference/overview/#RxInfer.KeepLast","page":"Overview","title":"RxInfer.KeepLast","text":"Instructs the inference engine to keep only the last marginal update and disregard intermediate updates.\n\n\n\n\n\n","category":"type"},{"location":"manuals/inference/overview/#RxInfer.KeepEach","page":"Overview","title":"RxInfer.KeepEach","text":"Instructs the inference engine to keep each marginal update for all intermediate iterations.\n\n\n\n\n\n","category":"type"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"predictvars","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"predictvars specifies the variables which should be predicted. Similar to returnvars, predictvars accepts a NamedTuple or Dict. There are two specifications:","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"KeepLast: saves the last update for a variable, ignoring any intermediate results during iterations\nKeepEach: saves all updates for a variable for all iterations","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"result = infer(\n    ...,\n    predictvars = (\n        o = KeepLast(),\n        τ = KeepEach()\n    )\n)","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"note: Note\nThe predictvars argument is exclusive for batch setting.","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"historyvars","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"Also read the Keeping the history of posteriors.","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"historyvars specifies the variables of interests and the amount of information to keep in history about the posterior updates when performing streamline inference. The specification is similar to the returnvars when applied in batch setting. The historyvars requires keephistory to be greater than zero.","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"historyvars accepts a NamedTuple or Dict or return var specification. There are two specifications:","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"KeepLast: saves the last update for a variable, ignoring any intermediate results during iterations\nKeepEach: saves all updates for a variable for all iterations","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"result = infer(\n    ...,\n    autoupdates = my_autoupdates,\n    historyvars = (\n        x = KeepLast(),\n        τ = KeepEach()\n    ),\n    keephistory = 10\n)","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"It is also possible to set either historyvars = KeepLast() or historyvars = KeepEach() that acts as an alias and sets the given option for all random variables in the model.","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"result = infer(\n    ...,\n    autoupdates = my_autoupdates,\n    historyvars = KeepLast(),\n    keephistory = 10\n)","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"keep_history","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"Specifies the buffer size for the updates history both for the historyvars and the free_energy buffers in streamline inference.","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"note: Note\nThe historyvars and keep_history arguments are exclusive for streamlined setting.","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"iterations","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"Specifies the number of variational (or loopy belief propagation) iterations. By default set to nothing, which is equivalent of doing 1 iteration. However, if set explicitly to 1 the default setting for returnvars changes from KeepLast to KeepEach.","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"free_energy","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"Batch inference:","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"Specifies if the infer function should return Bethe Free Energy (BFE) values.","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"Optionally accepts a floating-point type (e.g., Float64) for improved BFE computation performance, but restricts the use of automatic differentiation packages.","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"Streamline inference:","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"Specifies if the infer function should create an observable stream of Bethe Free Energy (BFE) values, computed at each VMP iteration.","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"When free_energy = true and keephistory > 0, additional fields are exposed in the engine for accessing the history of BFE updates.\nengine.free_energy_history: Averaged BFE history over VMP iterations.\nengine.free_energy_final_only_history: BFE history of values computed in the last VMP iterations for each observation.\nengine.free_energy_raw_history: Raw BFE history.","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"free_energy_diagnostics","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"This settings specifies either a single or a tuple of diagnostic checks for Bethe Free Energy values stream. By default checks for NaNs and Infs.  See also RxInfer.ObjectiveDiagnosticCheckNaNs and RxInfer.ObjectiveDiagnosticCheckInfs. Pass nothing to disable any checks.","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"options","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"RxInfer.ReactiveMPInferenceOptions","category":"page"},{"location":"manuals/inference/overview/#RxInfer.ReactiveMPInferenceOptions","page":"Overview","title":"RxInfer.ReactiveMPInferenceOptions","text":"ReactiveMPInferenceOptions(; kwargs...)\n\nCreates model inference options object. The list of available options is present below.\n\nOptions\n\nlimit_stack_depth: limits the stack depth for computing messages, helps with StackOverflowError for some huge models, but reduces the performance of inference backend. Accepts integer as an argument that specifies the maximum number of recursive depth. Lower is better for stack overflow error, but worse for performance.\nwarn: (optional) flag to suppress warnings. Warnings are not displayed if set to false. Defaults to true.\n\nAdvanced options\n\nscheduler: changes the scheduler of reactive streams, see Rocket.jl for more info, defaults to AsapScheduler.\n\nSee also: infer\n\n\n\n\n\n","category":"type"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"catch_exception","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"The catch_exception keyword argument specifies whether exceptions during the batch inference procedure should be caught in the error field of the  result. By default, if exception occurs during the inference procedure the result will be lost. Set catch_exception = true to obtain partial result  for the inference in case if an exception occurs. Use RxInfer.issuccess and RxInfer.iserror function to check if the inference completed successfully or failed. If an error occurs, the error field will store a tuple, where first element is the exception itself and the second element is the caught backtrace. Use the stacktrace function  with the backtrace as an argument to recover the stacktrace of the error. Use Base.showerror function to display the error.","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"RxInfer.issuccess\nRxInfer.iserror","category":"page"},{"location":"manuals/inference/overview/#RxInfer.issuccess","page":"Overview","title":"RxInfer.issuccess","text":"Checks if the InferenceResult object does not contain an error. \n\n\n\n\n\n","category":"function"},{"location":"manuals/inference/overview/#RxInfer.iserror","page":"Overview","title":"RxInfer.iserror","text":"Checks if the InferenceResult object contains an error. \n\n\n\n\n\n","category":"function"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"callbacks","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"The inference function has its own lifecycle. The user is free to provide some (or none) of the callbacks to inject some extra logging or other procedures in the inference function, e.g.","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"result = infer(\n    ...,\n    callbacks = (\n        on_marginal_update = (model, name, update) -> println(\"\\$(name) has been updated: \\$(update)\"),\n        after_inference    = (args...) -> println(\"Inference has been completed\")\n    )\n)","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"The callbacks keyword argument accepts a named-tuple of 'name = callback' pairs.  The list of all possible callbacks for different inference setting (batch or streamline) and their arguments is present below:","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"before_model_creation()\nafter_model_creation(model::ProbabilisticModel)","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"Exlusive for batch inference","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"on_marginal_update(model::ProbabilisticModel, name::Symbol, update)\nbefore_inference(model::ProbabilisticModel)\nbefore_iteration(model::ProbabilisticModel, iteration::Int)::Bool\nbefore_data_update(model::ProbabilisticModel, data)\nafter_data_update(model::ProbabilisticModel, data)\nafter_iteration(model::ProbabilisticModel, iteration::Int)::Bool\nafter_inference(model::ProbabilisticModel)","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"note: Note\nbefore_iteration and after_iteration callbacks are allowed to return true/false value. true indicates that iterations must be halted and no further inference should be made.","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"Exlusive for streamline inference","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"before_autostart(engine::RxInferenceEngine)\nafter_autostart(engine::RxInferenceEngine)","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"addons","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"The addons field extends the default message computation rules with some extra information, e.g. computing log-scaling factors of messages or saving debug-information. Accepts a single addon or a tuple of addons.  Automatically changes the default value of the postprocess argument to NoopPostprocess.","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"postprocess","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"Also read the Inference results postprocessing section.","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"The postprocess keyword argument controls whether the inference results must be modified in some way before exiting the inference function. By default, the inference function uses the DefaultPostprocess strategy, which by default removes the Marginal wrapper type from the results. Change this setting to NoopPostprocess if you would like to keep the Marginal wrapper type, which might be useful in the combination with the addons argument. If the addons argument has been used, automatically changes the default strategy value to NoopPostprocess.","category":"page"},{"location":"manuals/inference/overview/#Where-to-go-next?","page":"Overview","title":"Where to go next?","text":"","category":"section"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"Read more explanation about the other keyword arguments in the Streamlined (online) inferencesection or check out the Static Inference section or check some more advanced examples.","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/#examples-chance-constrained-active-inference","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"","category":"section"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"This notebook applies reactive message passing for active inference in the context of chance-constraints. The implementation is based on (van de Laar et al., 2021, \"Chance-constrained active inference\") and discussion with John Boik.","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"We consider a 1-D agent that tries to elevate itself above ground level. Instead of a goal prior, we impose a chance constraint on future states, such that the agent prefers to avoid the ground with a preset probability (chance) level. ","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"using Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"using Plots, Distributions, StatsFuns, RxInfer\nPkg.status()","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"Status `~/work/RxInfer.jl/RxInfer.jl/docs/src/examples/Project.toml`\n  [b4ee3484] BayesBase v1.2.1\n  [6e4b80f9] BenchmarkTools v1.5.0\n  [336ed68f] CSV v0.10.14\n  [a93c6f00] DataFrames v1.6.1\n  [31c24e10] Distributions v0.25.107\n  [62312e5e] ExponentialFamily v1.4.1\n⌃ [587475ba] Flux v0.14.11\n  [38e38edf] GLM v1.9.0\n  [b3f8163a] GraphPPL v4.0.0\n  [34004b35] HypergeometricFunctions v0.3.23\n  [7073ff75] IJulia v1.24.2\n  [4138dd39] JLD v0.13.5\n  [b964fa9f] LaTeXStrings v1.3.1\n  [429524aa] Optim v1.9.4\n⌃ [3bd65402] Optimisers v0.2.20\n  [d96e819e] Parameters v0.12.3\n  [91a5bcdd] Plots v1.40.4\n  [92933f4c] ProgressMeter v1.10.0\n  [a194aa59] ReactiveMP v4.0.0\n  [37e2e3b7] ReverseDiff v1.15.1\n  [df971d30] Rocket v1.8.0\n  [86711068] RxInfer v3.0.0 `~/work/RxInfer.jl/RxInfer.jl`\n  [276daf66] SpecialFunctions v2.3.1\n  [860ef19b] StableRNGs v1.0.1\n  [4c63d2b9] StatsFuns v1.3.1\n  [f3b207a7] StatsPlots v0.15.7\n  [44d3d7a6] Weave v0.10.12\nInfo Packages marked with ⌃ have new versions available and may be upgradab\nle.","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/#Chance-Constraint-Node-Definition","page":"Chance-Constrained Active Inference","title":"Chance-Constraint Node Definition","text":"","category":"section"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"A chance-constraint is meant to constraint a marginal distribution to abide by certain properties. In this case, a (posterior) probability distribution should not \"overflow\" a given region by more than a certain probability mass. This constraint then affects adjacent beliefs and ultimately the controls to (hopefully) account for the imposed constraint.","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"In order to enforce this constraint on a marginal distribution, an auxiliary chance-constraint node is included in the graphical model. This node then sends messages that enforce the marginal to abide by the preset conditions. In other words, the (chance) constraint on the (posterior) marginal, is converted to a prior constraint on the generative model that sends an adaptive message. We start by defining this chance-constraint node and its message.","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"struct ChanceConstraint end  \n\n# Node definition with safe region limits (lo, hi), overflow chance epsilon and tolerance atol\n@node ChanceConstraint Stochastic [out, lo, hi, epsilon, atol]","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"# Function to compute normalizing constant and central moments of a truncated Gaussian distribution\nfunction truncatedGaussianMoments(m::Float64, V::Float64, a::Float64, b::Float64)\n    V = clamp(V, tiny, huge)\n    StdG = Distributions.Normal(m, sqrt(V))\n    TrG = Distributions.Truncated(StdG, a, b)\n    \n    Z = Distributions.cdf(StdG, b) - Distributions.cdf(StdG, a)  # safe mass for standard Gaussian\n    \n    if Z < tiny\n        # Invalid region; return undefined mean and variance of truncated distribution\n        Z    = 0.0\n        m_tr = 0.0\n        V_tr = 0.0\n    else\n        m_tr = Distributions.mean(TrG)\n        V_tr = Distributions.var(TrG)\n    end\n    \n    return (Z, m_tr, V_tr)\nend;","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"@rule ChanceConstraint(:out, Marginalisation) (\n    m_out::UnivariateNormalDistributionsFamily, # Require inbound message\n    q_lo::PointMass, \n    q_hi::PointMass, \n    q_epsilon::PointMass, \n    q_atol::PointMass) = begin \n\n    # Extract parameters\n    lo = mean(q_lo)\n    hi = mean(q_hi)\n    epsilon = mean(q_epsilon)\n    atol = mean(q_atol)\n    \n    (m_bw, V_bw) = mean_var(m_out)\n    (xi_bw, W_bw) = (m_bw, 1. /V_bw)  # check division by  zero\n    (m_tilde, V_tilde) = (m_bw, V_bw)\n    \n    # Compute statistics (and normalizing constant) of q in safe region G\n    # Phi_G is called the \"safe mass\" \n    (Phi_G, m_G, V_G) = truncatedGaussianMoments(m_bw, V_bw, lo, hi)\n\n    xi_fw = xi_bw\n    W_fw  = W_bw\n    if epsilon <= 1.0 - Phi_G # If constraint is active\n        # Initialize statistics of uncorrected belief\n        m_tilde = m_bw\n        V_tilde = V_bw\n        for i = 1:100 # Iterate at most this many times\n            (Phi_lG, m_lG, V_lG) = truncatedGaussianMoments(m_tilde, V_tilde, -Inf, lo) # Statistics for q in region left of G\n            (Phi_rG, m_rG, V_rG) = truncatedGaussianMoments(m_tilde, V_tilde, hi, Inf) # Statistics for q in region right of G\n\n            # Compute moments of non-G region as a mixture of left and right truncations\n            Phi_nG = Phi_lG + Phi_rG\n            m_nG = Phi_lG / Phi_nG * m_lG + Phi_rG / Phi_nG * m_rG\n            V_nG = Phi_lG / Phi_nG * (V_lG + m_lG^2) + Phi_rG/Phi_nG * (V_rG + m_rG^2) - m_nG^2\n\n            # Compute moments of corrected belief as a mixture of G and non-G regions\n            m_tilde = (1.0 - epsilon) * m_G + epsilon * m_nG\n            V_tilde = (1.0 - epsilon) * (V_G + m_G^2) + epsilon * (V_nG + m_nG^2) - m_tilde^2\n            # Re-compute statistics (and normalizing constant) of corrected belief\n            (Phi_G, m_G, V_G) = truncatedGaussianMoments(m_tilde, V_tilde, lo, hi)\n            if (1.0 - Phi_G) < (1.0 + atol)*epsilon\n                break # Break the loop if the belief is sufficiently corrected\n            end\n        end\n        \n        # Convert moments of corrected belief to canonical form\n        W_tilde = inv(V_tilde)\n        xi_tilde = W_tilde * m_tilde\n\n        # Compute canonical parameters of forward message\n        xi_fw = xi_tilde - xi_bw\n        W_fw  = W_tilde - W_bw\n    end\n\n    return NormalWeightedMeanPrecision(xi_fw, W_fw)\nend","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/#Definition-of-the-Environment","page":"Chance-Constrained Active Inference","title":"Definition of the Environment","text":"","category":"section"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"We consider an environment where the agent has an elevation level, and where the agent directly controls its vertical velocity. After some time, an unexpected and sudden gust of wind tries to push the agent to the ground.","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"wind(t::Int64) = -0.1*(60 <= t < 100) # Time-dependent wind profile\n\nfunction initializeWorld()\n    x_0 = 0.0 # Initial elevation\n    \n    x_t_last = x_0\n    function execute(t::Int64, a_t::Float64)\n        x_t = x_t_last + a_t + wind(t) # Update elevation\n    \n        x_t_last = x_t # Reset state\n                \n        return x_t\n    end\n\n    x_t = x_0 # Predefine outcome variable\n    observe() = x_t # State is fully observed\n\n    return (execute, observe)\nend;","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/#Generative-Model-for-Regulator","page":"Chance-Constrained Active Inference","title":"Generative Model for Regulator","text":"","category":"section"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"We consider a fully observed Markov decision process, where the agent directly observes the true state (elevation) of the world. In this case we only need to define a chance-constrained generative model of future states. Inference for controls on this model then derives our controller.","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"# m_u ::Vector{Float64}, ,   Control prior means\n# v_u = datavar(Float64, T)  Control prior variances\n# x_t ::Float64              Fully observed state\n\n@model function regulator_model(T, m_u, v_u, x_t, lo, hi, epsilon, atol)\n    \n    # Loop over horizon\n    x_k_last = x_t\n    for k = 1:T\n        u[k] ~ NormalMeanVariance(m_u[k], v_u[k]) # Control prior\n        x[k] ~ x_k_last + u[k] # Transition model\n        x[k] ~ ChanceConstraint(lo, hi, epsilon, atol) where { # Simultaneous constraint on state\n            dependencies = RequireMessageFunctionalDependencies(out = NormalWeightedMeanPrecision(0, 0.01))} # Predefine inbound message to break circular dependency\n        x_k_last = x[k]\n    end\n \nend","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/#Reactive-Agent-Definition","page":"Chance-Constrained Active Inference","title":"Reactive Agent Definition","text":"","category":"section"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"function initializeAgent()\n    # Set control prior statistics\n    m_u = zeros(T)\n    v_u = lambda^(-1)*ones(T)\n    \n    function compute(x_t::Float64)\n        model_t = regulator_model(;T=T, lo=lo, hi=hi, epsilon=epsilon, atol=atol)\n        data_t = (m_u = m_u, v_u = v_u, x_t = x_t)\n\n        result = infer(\n            model = model_t,\n            data = data_t,\n            iterations = n_its)\n\n        # Extract policy from inference results\n        pol = mode.(result.posteriors[:u][end])\n\n        return pol\n    end\n\n    pol = zeros(T) # Predefine policy variable\n    act() = pol[1]\n\n    return (compute, act)\nend;","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/#Action-Perception-Cycle","page":"Chance-Constrained Active Inference","title":"Action-Perception Cycle","text":"","category":"section"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"Next we define and execute the action-perception cycle. Because the state is fully observed, these is no slide (estimator) step in the cycle. ","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"# Simulation parameters\nN = 160 # Total simulation time\nT = 1 # Lookahead time horizon\nlambda = 1.0 # Control prior precision\nlo = 1.0 # Chance region lower bound\nhi = Inf # Chance region upper bound\nepsilon = 0.01 # Allowed chance violation\natol = 0.01 # Convergence tolerance for chance constraints\nn_its = 10;  # Number of inference iterations","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"(execute, observe) = initializeWorld() # Let there be a world\n(compute, act) = initializeAgent() # Let there be an agent\n\na = Vector{Float64}(undef, N) # Actions\nx = Vector{Float64}(undef, N) # States\nfor t = 1:N\n    a[t] = act()\n           execute(t, a[t])\n    x[t] = observe()\n           compute(x[t])\nend","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/#Results","page":"Chance-Constrained Active Inference","title":"Results","text":"","category":"section"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"Results show that the agent does not allow the wind to push it all the way to the ground.","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"p1 = plot(1:N, wind.(1:N), color=\"blue\", label=\"Wind\", ylabel=\"Velocity\", lw=2)\nplot!(p1, 1:N, a, color=\"red\", label=\"Control\", lw=2)\np2 = plot(1:N, x, color=\"black\", lw=2, label=\"Agent\", ylabel=\"Elevation\")\nplot(p1, p2, layout=(2,1))","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/#examples-gamma-mixture-model","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"","category":"section"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"This notebook implements one of the experiments outlined in https://biaslab.github.io/publication/mp-based-inference-in-gmm/.","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/#Load-packages","page":"Gamma Mixture Model","title":"Load packages","text":"","category":"section"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"using RxInfer, Random, StatsPlots","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"# create custom structure for model parameters for simplicity\nstruct GammaMixtureModelParameters\n    nmixtures   # number of mixtures\n    priors_as   # tuple of priors for variable a\n    priors_bs   # tuple of priors for variable b\n    prior_s     # prior of variable s\nend","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/#Model-specification","page":"Gamma Mixture Model","title":"Model specification","text":"","category":"section"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"@model function gamma_mixture_model(y, parameters)\n\n    # fetch information from struct\n    nmixtures = parameters.nmixtures\n    priors_as = parameters.priors_as\n    priors_bs = parameters.priors_bs\n    prior_s   = parameters.prior_s\n\n    # set prior on global selection variable\n    s ~ Dirichlet(probvec(prior_s))\n\n    # allocate variables for mixtures\n    local as\n    local bs\n\n    # set priors on variables of mixtures\n    for i in 1:nmixtures\n        as[i] ~ Gamma(shape = shape(priors_as[i]), rate = rate(priors_as[i]))\n        bs[i] ~ Gamma(shape = shape(priors_bs[i]), rate = rate(priors_bs[i]))\n    end\n\n    # allocate variables for local selection variable\n    local z\n    # specify local selection variable and data generating process\n    for i in 1:length(y)\n        z[i] ~ Categorical(s)\n        y[i] ~ GammaMixture(switch = z[i], a = as, b = bs)\n    end\n    \nend","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"constraints = @constraints begin \n\n    q(z, as, bs, s) = q(z)q(as)q(bs)q(s)\n\n    q(as) = q(as[begin])..q(as[end])\n    q(bs) = q(bs[begin])..q(bs[end])\n    \n    q(as)::PointMassFormConstraint(starting_point = (args...) -> [1.0])\nend","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"Constraints: \n  q(z, as, bs, s) = q(z)q(as)q(bs)q(s)\n  q(as) = q(as[(begin)..(end)])\n  q(bs) = q(bs[(begin)..(end)])\n  q(as) :: PointMassFormConstraint()","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"# specify seed and number of data points\nrng = MersenneTwister(43)\nn_samples = 2500\n\n# specify parameters of mixture model that generates the data\n# Note that mixture components have exactly the same means\nmixtures  = [ Gamma(9.0, inv(27.0)), Gamma(90.0, inv(270.0)) ]\nnmixtures = length(mixtures)\nmixing    = rand(rng, nmixtures)\nmixing    = mixing ./ sum(mixing)\nmixture   = MixtureModel(mixtures, mixing)\n\n# generate data set\ndataset = rand(rng, mixture, n_samples);","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"# specify priors of probabilistic model\n# NOTE: As the means of the mixtures \"collide\", we specify informative prior for selector variable\nnmixtures = 2\ngpriors = GammaMixtureModelParameters(\n    nmixtures,                                                    # number of mixtures\n    [ Gamma(1.0, 0.1), Gamma(1.0, 1.0) ],                         # priors on variables a\n    [ GammaShapeRate(10.0, 2.0), GammaShapeRate(1.0, 3.0) ],      # priors on variables b\n    Dirichlet(1e3*mixing)                                         # prior on variable s\n)\n\ngmodel         = gamma_mixture_model(parameters = gpriors)\ngdata          = (y = dataset, )\ninit           = @initialization begin \n    q(s) = gpriors.prior_s\n    q(z) = vague(Categorical, gpriors.nmixtures)\n    q(bs) = GammaShapeRate(1.0, 1.0)\nend\ngreturnvars    = (s = KeepLast(), z = KeepLast(), as = KeepEach(), bs = KeepEach())\n\ngoptions = (\n     \n    default_factorisation = MeanField() # Mixture models require Mean-Field assumption currently\n)\n\ngresult = infer(\n    model          = gmodel, \n    data           = gdata,\n    constraints    = constraints,\n    options        = (limit_stack_depth = 100,),\n    initialization = init,\n    returnvars     = greturnvars,\n    free_energy    = true,\n    iterations     = 250, \n    showprogress   = true\n);","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"# extract inferred parameters\n_as, _bs = mean.(gresult.posteriors[:as][end]), mean.(gresult.posteriors[:bs][end])\n_dists   = map(g -> Gamma(g[1], inv(g[2])), zip(_as, _bs))\n_mixing = mean(gresult.posteriors[:s])\n\n# create model from inferred parameters\n_mixture   = MixtureModel(_dists, _mixing);","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"# report on outcome of inference\nprintln(\"Generated means: $(mean(mixtures[1])) and $(mean(mixtures[2]))\")\nprintln(\"Inferred means: $(mean(_dists[1])) and $(mean(_dists[2]))\")\nprintln(\"========\")\nprintln(\"Generated mixing: $(mixing)\")\nprintln(\"Inferred mixing: $(_mixing)\")","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"Generated means: 0.3333333333333333 and 0.33333333333333337\nInferred means: 0.33821336265609075 and 0.3330561752512995\n========\nGenerated mixing: [0.18923488676601088, 0.8107651132339891]\nInferred mixing: [0.11395578236399123, 0.8860442176360088]","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"# plot results\np1 = histogram(dataset, ylim = (0, 13), xlim = (0, 1), normalize=:pdf, label=\"data\", opacity=0.3)\np1 = plot!(mixture, label=false, title=\"Generated mixtures\", linewidth=3.0)\n\np2 = histogram(dataset, ylim = (0, 13), xlim = (0, 1), normalize=:pdf, label=\"data\", opacity=0.3)\np2 = plot!(_mixture, label=false, title=\"Inferred mixtures\", linewidth=3.0)\n\n# evaluate the convergence of the algorithm by monitoring the BFE\np3 = plot(gresult.free_energy, label=false, xlabel=\"iterations\", title=\"Bethe FE\")\n\nplot(plot(p1, p2, layout = @layout([ a; b ])), plot(p3), layout = @layout([ a b ]), size = (800, 400))","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"","category":"page"},{"location":"contributing/guidelines/#contributing-guidelines","page":"Contribution guidelines","title":"Contribution Guidelines","text":"","category":"section"},{"location":"contributing/guidelines/","page":"Contribution guidelines","title":"Contribution guidelines","text":"Welcome to the contribution guidelines for RxInfer.jl. Here you'll find detailed instructions on how to contribute effectively to the project.","category":"page"},{"location":"contributing/guidelines/#Reporting-bugs","page":"Contribution guidelines","title":"Reporting bugs","text":"","category":"section"},{"location":"contributing/guidelines/","page":"Contribution guidelines","title":"Contribution guidelines","text":"If you encounter any bugs while using the software, please report them via GitHub issues. To ensure efficient bug resolution, please provide comprehensive and reproducible bug reports. Include details such as the versions of Julia and RxInfer you're using, along with a concise description of the issue. Additionally, attach relevant code snippets, test cases, screenshots, or any other pertinent information that could aid in replicating and addressing the problem.","category":"page"},{"location":"contributing/guidelines/#Nightly-Julia-status","page":"Contribution guidelines","title":"Nightly Julia status","text":"","category":"section"},{"location":"contributing/guidelines/","page":"Contribution guidelines","title":"Contribution guidelines","text":"Check the badge below to see if RxInfer can be installed on a Julia nightly version. A failing badge may indicate issues with RxInfer or its dependencies. Click on the badge to access the latest evaluation report.","category":"page"},{"location":"contributing/guidelines/","page":"Contribution guidelines","title":"Contribution guidelines","text":"(Image: PkgEval)","category":"page"},{"location":"contributing/guidelines/#Suggesting-features","page":"Contribution guidelines","title":"Suggesting features","text":"","category":"section"},{"location":"contributing/guidelines/","page":"Contribution guidelines","title":"Contribution guidelines","text":"We encourage proposals for new features. Before submitting a feature request, consider the following:","category":"page"},{"location":"contributing/guidelines/","page":"Contribution guidelines","title":"Contribution guidelines","text":"Determine if the feature necessitates changes to the core RxInfer code. If not, such as adding a factor node for a specific application, consider local extensions in your script/notebook or creating a separate repository for your extensions.\nFor feature implementations requiring significant changes to the core RxInfer code, open a GitHub issue to discuss your proposal before proceeding with implementation. This allows for thorough deliberation and avoids investing time in features that may be challenging to integrate later on.","category":"page"},{"location":"contributing/guidelines/#Contributing-code","page":"Contribution guidelines","title":"Contributing code","text":"","category":"section"},{"location":"contributing/guidelines/#Installing-RxInfer","page":"Contribution guidelines","title":"Installing RxInfer","text":"","category":"section"},{"location":"contributing/guidelines/","page":"Contribution guidelines","title":"Contribution guidelines","text":"For development purposes, it's recommended to use the dev command from the Julia package manager to install RxInfer. Use your fork's URL in the dev command to work on your forked version. For example:","category":"page"},{"location":"contributing/guidelines/","page":"Contribution guidelines","title":"Contribution guidelines","text":"] dev git@github.com:your_username/RxInfer.jl.git","category":"page"},{"location":"contributing/guidelines/","page":"Contribution guidelines","title":"Contribution guidelines","text":"The dev command clones RxInfer to ~/.julia/dev/RxInfer. All local changes to RxInfer code will be reflected in imported code.","category":"page"},{"location":"contributing/guidelines/","page":"Contribution guidelines","title":"Contribution guidelines","text":"note: Note\nIt is also might be useful to install Revise.jl package as it allows you to modify code and use the changes without restarting Julia.","category":"page"},{"location":"contributing/guidelines/#Core-dependencies","page":"Contribution guidelines","title":"Core dependencies","text":"","category":"section"},{"location":"contributing/guidelines/","page":"Contribution guidelines","title":"Contribution guidelines","text":"RxInfer.jl depends heavily on the core packages ReactiveMP.jl, GraphPPL.jl, and Rocket.jl. Ensure RxInfer.jl is updated whenever any of these packages undergo major updates or API changes. While making changes to RxInfer.jl, developers are advised to use the dev command for these packages as well. Note that standard Julia testing utilities ignore the local development environment and test the package with the latest released versions of core dependencies. Refer to the Makefile section below to learn how to test RxInfer.jl with locally installed core dependencies.","category":"page"},{"location":"contributing/guidelines/#Committing-code","page":"Contribution guidelines","title":"Committing code","text":"","category":"section"},{"location":"contributing/guidelines/","page":"Contribution guidelines","title":"Contribution guidelines","text":"We use the standard GitHub Flow workflow where all contributions are added through pull requests. To contribute:","category":"page"},{"location":"contributing/guidelines/","page":"Contribution guidelines","title":"Contribution guidelines","text":"Fork the repository\nCommit your contributions to your fork\nCreate a pull request on the main branch of the RxInfer repository.","category":"page"},{"location":"contributing/guidelines/","page":"Contribution guidelines","title":"Contribution guidelines","text":"Before opening a pull request, ensure all tests pass without errors. Additionally, ensure all examples (found in the /examples/ directory) run succesfully. ","category":"page"},{"location":"contributing/guidelines/","page":"Contribution guidelines","title":"Contribution guidelines","text":"note: Note\nUse make test, make examples and make docs commands to verify that all tests, examples, and documentation build correctly. See the Makefile section below for detailed command descriptions.","category":"page"},{"location":"contributing/guidelines/#Style-conventions","page":"Contribution guidelines","title":"Style conventions","text":"","category":"section"},{"location":"contributing/guidelines/","page":"Contribution guidelines","title":"Contribution guidelines","text":"We use the default Julia style guide. There are a couple of important points modifications to the Julia style guide to take into account:","category":"page"},{"location":"contributing/guidelines/","page":"Contribution guidelines","title":"Contribution guidelines","text":"Use 4 spaces for indentation\nType names use UpperCamelCase. For example: AbstractFactorNode, RandomVariable, etc..\nFunction names are lowercase with underscores, when necessary. For example: activate!, randomvar, as_variable, etc..\nVariable names and function arguments use snake_case\nThe name of a method that modifies its argument(s) must end in !","category":"page"},{"location":"contributing/guidelines/","page":"Contribution guidelines","title":"Contribution guidelines","text":"note: Note\nThe RxInfer repository contains scripts to automatically format code according to our guidelines. Use make format command to fix code style. This command overwrites files. Use make lint to run a linting procedure without overwriting the actual source files.","category":"page"},{"location":"contributing/guidelines/#Unit-tests","page":"Contribution guidelines","title":"Unit tests","text":"","category":"section"},{"location":"contributing/guidelines/","page":"Contribution guidelines","title":"Contribution guidelines","text":"We use the test-driven development (TDD) methodology for RxInfer development. Aim for comprehensive test coverage, ensuring tests cover each piece of added code.","category":"page"},{"location":"contributing/guidelines/","page":"Contribution guidelines","title":"Contribution guidelines","text":"All unit tests are located in the /test/ directory. The /test/ directory follows the structure of the /src/ directory. Each test file should have the following filename format: *_tests.jl. Some tests are also present in jldoctest docs annotations directly in the source code. See Julia's documentation about doctests.","category":"page"},{"location":"contributing/guidelines/","page":"Contribution guidelines","title":"Contribution guidelines","text":"The tests can be evaluated by running following command in the Julia REPL:","category":"page"},{"location":"contributing/guidelines/","page":"Contribution guidelines","title":"Contribution guidelines","text":"] test RxInfer","category":"page"},{"location":"contributing/guidelines/","page":"Contribution guidelines","title":"Contribution guidelines","text":"In addition tests can be evaluated by running following command in the RxInfer root directory:","category":"page"},{"location":"contributing/guidelines/","page":"Contribution guidelines","title":"Contribution guidelines","text":"make test","category":"page"},{"location":"contributing/guidelines/","page":"Contribution guidelines","title":"Contribution guidelines","text":"note: Note\nUse make devtest to use local dev-ed versions of the core packages.","category":"page"},{"location":"contributing/guidelines/#Makefile","page":"Contribution guidelines","title":"Makefile","text":"","category":"section"},{"location":"contributing/guidelines/","page":"Contribution guidelines","title":"Contribution guidelines","text":"RxInfer.jl uses Makefile for most common operations:","category":"page"},{"location":"contributing/guidelines/","page":"Contribution guidelines","title":"Contribution guidelines","text":"make help: Shows help snippet\nmake test: Run tests, supports extra arguments\nmake test test_args=\"distributions:normal_mean_variance\" would run tests only from distributions/test_normal_mean_variance.jl\nmake test test_args=\"distributions:normal_mean_variance models:lgssm\" would run tests both from distributions/test_normal_mean_variance.jl and models/test_lgssm.jl\nmake test dev=true would run tests while using dev-ed versions of core packages\nmake devtest: Alias for the make test dev=true ...\nmake docs: Compile documentation\nmake devdocs: Same as make docs, but uses dev-ed versions of core packages\nmake examples: Run all examples and put them in the docs/ folder if successfull \nmake devexamples: Same as make examples, but uses dev-ed versions of core packages\nmake lint: Check codestyle\nmake format: Check and fix codestyle ","category":"page"},{"location":"contributing/guidelines/","page":"Contribution guidelines","title":"Contribution guidelines","text":"note: Note\nCore packages include ReactiveMP.jl, GraphPPL.jl and Rocket.jl. When using any of the dev commands from the Makefile those packages must be present in the Pkg.devdir() directory.","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/#examples-gaussian-mixture","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"","category":"section"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"This notebook illustrates how to use the NormalMixture node in RxInfer.jl for both univariate and multivariate observations.","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/#Load-packages","page":"Gaussian Mixture","title":"Load packages","text":"","category":"section"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"using RxInfer, Plots, Random, LinearAlgebra, StableRNGs, LaTeXStrings","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/#Univariate-Gaussian-Mixture-Model","page":"Gaussian Mixture","title":"Univariate Gaussian Mixture Model","text":"","category":"section"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"Consider the data set of length N observed below.","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"function generate_univariate_data(nr_samples; rng = MersenneTwister(123))\n\n    # data generating parameters\n    class        = [1/3, 2/3]\n    mean1, mean2 = -10, 10\n    precision    = 1.777\n\n    # generate data\n    z = rand(rng, Categorical(class), nr_samples)\n    y = zeros(nr_samples)\n    for k in 1:nr_samples\n        y[k] = rand(rng, Normal(z[k] == 1 ? mean1 : mean2, 1/sqrt(precision)))\n    end\n\n    return y\n\nend;","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"data_univariate = generate_univariate_data(100)\nhistogram(data_univariate, bins=50, label=\"data\", normed=true)\nxlims!(minimum(data_univariate), maximum(data_univariate))\nylims!(0, Inf)\nylabel!(\"relative occurrence [%]\")\nxlabel!(\"y\")","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/#Model-specification","page":"Gaussian Mixture","title":"Model specification","text":"","category":"section"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"The goal here is to create a model for the data set above. In this case a Gaussian mixture model with K components seems to suite the situation well. We specify the factorized model as  p(y z s m w) = prod_n=1^N bigg(p(y_n mid m w z_n) p(z_n mid s) bigg)prod_k=1^K bigg(p(m_k) p(w_k) bigg) p(s) where the individual terms are specified as beginaligned     p(s)                    = mathrmBeta(s mid alpha_s beta_s) \n    p(m_k)                = mathcalN(m_k mid mu_k sigma_k^2)          p(w_k)                = Gamma(w_k mid alpha_k beta_k) \n    p(z_n mid s)           = mathrmBer(z_n mid s) \n    p(y_n mid m w z_n)   = prod_k=1^K mathcalNleft(y_n mid m_k w_kright)^z_nk endaligned","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"The set of observations y = y_1 y_2 ldots y_N is modeled by a mixture of Gaussian distributions, parameterized by means m = m_1 m_2 ldots m_K and precisions w =  w_1 w_2 ldots w_K, where k denotes the component index. This component is selected per observation by the indicator variable z_n, which is a one-of-K encoded vector satisfying sum_k=1^K z_nk = 1 and z_nk in 0 1 forall k. We put a hyperprior on these variables, termed s, which represents the relative occurrence of the different realizations of z_n.","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"Here we implement the following model with uninformative values for the hyperparameters as","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"@model function univariate_gaussian_mixture_model(y)\n    \n    s ~ Beta(1.0, 1.0)\n\n    m[1] ~ Normal(mean = -2.0, variance = 1e3)\n    w[1] ~ Gamma(shape = 0.01, rate = 0.01)\n\n    m[2] ~ Normal(mean = 2.0, variance = 1e3)\n    w[2] ~ Gamma(shape = 0.01, rate = 0.01)\n\n    for i in eachindex(y)\n        z[i] ~ Bernoulli(s)\n        y[i] ~ NormalMixture(switch = z[i], m = m, p = w)\n    end\n    \nend","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/#Probabilistic-inference","page":"Gaussian Mixture","title":"Probabilistic inference","text":"","category":"section"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"In order to fit the model to the data, we are interested in computing the posterior distribution p(z s m w mid y) However, computation of this term is intractable. Therefore, it is approximated by a naive mean-field approximation, specified as  p(z s m w mid y) approx prod_n=1^N q(z_n) prod_k=1^K bigg(q(m_k) q(w_k)bigg) q(s) with the functional forms beginaligned     q(s)   = mathrmBeta(s mid hatalpha_s hatbeta_s) \n    q(m_k) = mathcalN(m_k mid hatmu_k hatsigma^2_k) \n    q(w_k) = Gamma (w_k mid hatalpha_k hatbeta_k) \n    q(z_n) = mathrmBer(z_n mid hatp_n) endaligned In order to get the inference procedure started, these marginal distribution need to be initialized.","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"n_iterations = 10\n\ninit = @initialization begin\n    q(s) = vague(Beta)\n    q(m) = [NormalMeanVariance(-2.0, 1e3), NormalMeanVariance(2.0, 1e3)]\n    q(w) = [vague(GammaShapeRate), vague(GammaShapeRate)]\nend\n\nresults_univariate = infer(\n    model = univariate_gaussian_mixture_model(), \n    constraints = MeanField(),\n    data  = (y = data_univariate,), \n    initialization = init, \n    iterations  = n_iterations, \n    free_energy = true\n)","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"Inference results:\n  Posteriors       | available for (m, w, s, z)\n  Free Energy:     | Real[251.052, 196.44, 165.236, 135.738, 135.278, 135.2\n77, 135.277, 135.277, 135.277, 135.277]","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/#Results","page":"Gaussian Mixture","title":"Results","text":"","category":"section"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"Below the inference results can be seen as a function of the iterations","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"m1 = [results_univariate.posteriors[:m][i][1] for i in 1:n_iterations]\nm2 = [results_univariate.posteriors[:m][i][2] for i in 1:n_iterations]\nw1 = [results_univariate.posteriors[:w][i][1] for i in 1:n_iterations]\nw2 = [results_univariate.posteriors[:w][i][2] for i in 1:n_iterations];","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"mp = plot(mean.(m1), ribbon = std.(m1) .|> sqrt, label = L\"posterior $m_1$\")\nmp = plot!(mean.(m2), ribbon = std.(m2) .|> sqrt, label = L\"posterior $m_2$\")\nmp = plot!(mp, [ -10 ], seriestype = :hline, label = L\"true $m_1$\")\nmp = plot!(mp, [ 10 ], seriestype = :hline, label = L\"true $m_2$\")\n\nwp = plot(mean.(w1), ribbon = std.(w1) .|> sqrt, label = L\"posterior $w_1$\", legend = :bottomright, ylim = (-1, 3))\nwp = plot!(wp, mean.(w2), ribbon = std.(w2) .|> sqrt, label = L\"posterior $w_2$\")\nwp = plot!(wp, [ 1.777 ], seriestype = :hline, label = L\"true $w_1$\")\nwp = plot!(wp, [ 1.777 ], seriestype = :hline, label = L\"true $w_2$\")\n\nswp = plot(mean.(results_univariate.posteriors[:s]), ribbon = std.(results_univariate.posteriors[:s]) .|> sqrt, label = L\"posterior $s$\")\nswp = plot!(swp, [ 2/3 ], seriestype = :hline, label = L\"true $s$\")\n\nfep = plot(results_univariate.free_energy, label = \"Free Energy\", legend = :topright)\n\nplot(mp, wp, swp, fep, layout = @layout([ a b; c d ]), size = (800, 400))\nxlabel!(\"iteration\")","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/#Multivariate-Gaussian-Mixture-Model","page":"Gaussian Mixture","title":"Multivariate Gaussian Mixture Model","text":"","category":"section"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"The above example can also be extended to the multivariate case. Consider the data set below","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"function generate_multivariate_data(nr_samples; rng = MersenneTwister(123))\n\n    L         = 50.0\n    nr_mixtures = 6\n\n    probvec = normalize!(ones(nr_mixtures), 1)\n\n    switch = Categorical(probvec)\n\n    gaussians = map(1:nr_mixtures) do index\n        angle      = 2π / nr_mixtures * (index - 1)\n        basis_v    = L * [ 1.0, 0.0 ]\n        R          = [ cos(angle) -sin(angle); sin(angle) cos(angle) ]\n        mean       = R * basis_v \n        covariance = Matrix(Hermitian(R * [ 10.0 0.0; 0.0 20.0 ] * transpose(R)))\n        return MvNormal(mean, covariance)\n    end\n\n    z = rand(rng, switch, nr_samples)\n    y = Vector{Vector{Float64}}(undef, nr_samples)\n\n    for n in 1:nr_samples\n        y[n] = rand(rng, gaussians[z[n]])\n    end\n\n    return y\n\nend;","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"data_multivariate = generate_multivariate_data(500)\n\nsdim(n) = (a) -> map(d -> d[n], a) # helper function\nscatter(data_multivariate |> sdim(1), data_multivariate |> sdim(2), ms = 2, alpha = 0.4, size = (600, 400), legend=false)\nxlabel!(L\"y_1\")\nylabel!(L\"y_2\")","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/#Model-specification-2","page":"Gaussian Mixture","title":"Model specification","text":"","category":"section"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"The goal here is to create a model for the data set above. In this case a Gaussian mixture model with K components seems to suite the situation well. We specify the factorized model as  p(y z s m w) = prod_n=1^N bigg(p(y_n mid m W z_n) p(z_n mid s) bigg)prod_k=1^K bigg(p(m_k) p(W_k) bigg) p(s) where the individual terms are specified as beginaligned     p(s)                    = mathrmDir(s mid alpha_s) \n    p(m_k)                = mathcalN(m_k mid mu_k Sigma_k)          p(W_k)                = mathcalW(W_k mid V_k nu_k) \n    p(z_n mid s)           = mathrmCat(z_n mid s) \n    p(y_n mid m W z_n)   = prod_k=1^K mathcalNleft(y_n mid m_k W_kright)^z_nk endaligned","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"The set of observations y = y_1 y_2 ldots y_N is modeled by a mixture of Gaussian distributions, parameterized by means m = m_1 m_2 ldots m_K and precisions W =  W_1 W_2 ldots W_K, where k denotes the component index. This component is selected per observation by the indicator variable z_n, which is a one-of-K encoded vector satisfying sum_k=1^K z_nk = 1 and z_nk in 0 1 forall k. We put a hyperprior on these variables, termed s, which represents the relative occurrence of the different realizations of z_n.","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"@model function multivariate_gaussian_mixture_model(nr_mixtures, priors, y)\n    local m\n    local w\n\n    for k in 1:nr_mixtures        \n        m[k] ~ priors[k]\n        w[k] ~ Wishart(3, 1e2*diagm(ones(2)))\n    end\n    \n    s ~ Dirichlet(ones(nr_mixtures))\n    \n    for n in eachindex(y)\n        z[n] ~ Categorical(s) \n        y[n] ~ NormalMixture(switch = z[n], m = m, p = w)\n    end\n    \nend","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/#Probabilistic-inference-2","page":"Gaussian Mixture","title":"Probabilistic inference","text":"","category":"section"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"In order to fit the model to the data, we are interested in computing the posterior distribution p(z s m W mid y) However, computation of this term is intractable. Therefore, it is approximated by a naive mean-field approximation, specified as  p(z s m W mid y) approx prod_n=1^N q(z_n) prod_k=1^K bigg(q(m_k) q(W_k)bigg) q(s) with the functional forms beginaligned     q(s)   = mathrmDir(s mid hatalpha_s) \n    q(m_k) = mathcalN(m_k mid hatmu_k hatSigma_k) \n    q(w_k) = mathcalW(W_k mid hatV_k hatnu_k) \n    q(z_n) = mathrmCat(z_n mid hatp_n) endaligned In order to get the inference procedure started, these marginal distribution need to be initialized.","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"rng = MersenneTwister(121)\npriors = [MvNormal([cos(k*2π/6), sin(k*2π/6)], diagm(1e2 * ones(2))) for k in 1:6]\ninit = @initialization begin\n    q(s) = vague(Dirichlet, 6)\n    q(m) = priors\n    q(w) = Wishart(3, diagm(1e2 * ones(2)))\nend\n\nresults_multivariate = infer(\n    model = multivariate_gaussian_mixture_model(\n        nr_mixtures = 6, \n        priors = priors,\n    ), \n    data  = (y = data_multivariate,), \n    constraints   = MeanField(),\n    initialization = init, \n    iterations  = 50, \n    free_energy = true\n)","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"Inference results:\n  Posteriors       | available for (w, m, s, z)\n  Free Energy:     | Real[3938.2, 3894.49, 3894.49, 3894.49, 3894.49, 3894.\n49, 3894.49, 3894.49, 3894.49, 3894.49  …  3894.49, 3894.49, 3894.49, 3894.\n49, 3894.49, 3894.49, 3894.49, 3894.49, 3894.49, 3894.49]","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/#Results-2","page":"Gaussian Mixture","title":"Results","text":"","category":"section"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"Below the inference results can be seen","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"p_data = scatter(data_multivariate |> sdim(1), data_multivariate |> sdim(2), ms = 2, alpha = 0.4, legend=false, title=\"Data\", xlims=(-75, 75), ylims=(-75, 75))\np_result = plot(xlims = (-75, 75), ylims = (-75, 75), title=\"Inference result\", legend=false, colorbar = false)\nfor (e_m, e_w) in zip(results_multivariate.posteriors[:m][end], results_multivariate.posteriors[:w][end])\n    gaussian = MvNormal(mean(e_m), Matrix(Hermitian(mean(inv, e_w))))\n    global p_result = contour!(p_result, range(-75, 75, step = 0.25), range(-75, 75, step = 0.25), (x, y) -> pdf(gaussian, [ x, y ]), title=\"Inference result\", legend=false, levels = 7, colorbar = false)\nend\np_fe = plot(results_multivariate.free_energy, label = \"Free Energy\")\n\nplot(p_data, p_result, p_fe, layout = @layout([ a b; c ]))","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/#examples-kalman-filtering-and-smoothing","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"","category":"section"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"In the following set of examples the goal is to estimate hidden states of a Dynamical process where all hidden states are Gaussians.","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"We start our journey with a simple multivariate Linear Gaussian State Space Model (LGSSM), which can be solved analytically.","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"We then solve an identification problem which does not have an analytical solution.","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"Utimately, we show how RxInfer.jl can deal with missing observations.","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/#Gaussian-Linear-Dynamical-System","page":"Kalman filtering and smoothing","title":"Gaussian Linear Dynamical System","text":"","category":"section"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"LGSSM can be described with the following equations:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"beginaligned\n p(x_ix_i - 1)  = mathcalN(x_iA * x_i - 1 mathcalP)\n p(y_ix_i)  = mathcalN(y_iB * x_i mathcalQ)\nendaligned","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"where x_i are hidden states, y_i are noisy observations, A, B are state transition and observational matrices, mathcalP and mathcalQ are state transition noise and observation noise covariance matrices. For a more rigorous introduction to Linear Gaussian Dynamical systems we refer to Simo Sarkka, Bayesian Filtering and Smoothing book.","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"To model this process in RxInfer, first, we start with importing all needed packages:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"using RxInfer, BenchmarkTools, Random, LinearAlgebra, Plots","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"Next step, is to generate some synthetic data:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"function generate_data(rng, A, B, Q, P)\n    x_prev = [ 10.0, -10.0 ]\n\n    x = Vector{Vector{Float64}}(undef, n)\n    y = Vector{Vector{Float64}}(undef, n)\n\n    for i in 1:n\n        x[i] = rand(rng, MvNormal(A * x_prev, Q))\n        y[i] = rand(rng, MvNormal(B * x[i], P))\n        x_prev = x[i]\n    end\n    \n    return x, y\nend","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"generate_data (generic function with 1 method)","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"# Seed for reproducibility\nseed = 1234\n\nrng = MersenneTwister(1234)\n\n# We will model 2-dimensional observations with rotation matrix `A`\n# To avoid clutter we also assume that matrices `A`, `B`, `P` and `Q`\n# are known and fixed for all time-steps\nθ = π / 35\nA = [ cos(θ) -sin(θ); sin(θ) cos(θ) ]\nB = diageye(2)\nQ = diageye(2)\nP = 25.0 .* diageye(2)\n\n# Number of observations\nn = 300;","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"x, y = generate_data(rng, A, B, Q, P);","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"Let's plot our synthetic dataset. Lines represent our hidden states we want to estimate using noisy observations, which are represented as dots.","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"px = plot()\n\npx = plot!(px, getindex.(x, 1), label = \"Hidden Signal (dim-1)\", color = :orange)\npx = scatter!(px, getindex.(y, 1), label = false, markersize = 2, color = :orange)\npx = plot!(px, getindex.(x, 2), label = \"Hidden Signal (dim-2)\", color = :green)\npx = scatter!(px, getindex.(y, 2), label = false, markersize = 2, color = :green)\n\nplot(px)","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"To create a model we use GraphPPL package and @model macro:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"@model function rotate_ssm(y, x0, A, B, Q, P)\n    x_prior ~ MvNormalMeanCovariance(mean(x0), cov(x0))\n    x_prev = x_prior\n    \n    for i in 1:length(y)\n        x[i] ~ MvNormalMeanCovariance(A * x_prev, Q)\n        y[i] ~ MvNormalMeanCovariance(B * x[i], P)\n        x_prev = x[i]\n    end\n\nend","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"To run inference we also specify prior for out first hidden state:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"x0 = MvNormalMeanCovariance(zeros(2), 100.0 * diageye(2));","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"# For large number of observations you need to use limit_stack_depth = 100 option during model creation, e.g. \n# infer(..., options = (limit_stack_depth = 500, ))`\nresult = infer(\n    model = rotate_ssm(x0=x0, A=A, B=B, Q=Q, P=P), \n    data = (y = y,),\n    free_energy = true\n);\n\nxmarginals  = result.posteriors[:x]\nlogevidence = -result.free_energy; # given the analytical solution, free energy will be equal to the negative log evidence","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"px = plot()\n\npx = plot!(px, getindex.(x, 1), label = \"Hidden Signal (dim-1)\", color = :orange)\npx = plot!(px, getindex.(x, 2), label = \"Hidden Signal (dim-2)\", color = :green)\n\npx = plot!(px, getindex.(mean.(xmarginals), 1), ribbon = getindex.(var.(xmarginals), 1) .|> sqrt, fillalpha = 0.5, label = \"Estimated Signal (dim-1)\", color = :teal)\npx = plot!(px, getindex.(mean.(xmarginals), 2), ribbon = getindex.(var.(xmarginals), 2) .|> sqrt, fillalpha = 0.5, label = \"Estimated Signal (dim-1)\", color = :violet)\n\nplot(px)","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"As we can see from our plot, estimated signal resembles closely to the real hidden states with small variance. We maybe also interested in the value for minus log evidence:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"logevidence","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"1-element Vector{Float64}:\n -1882.2434870099432","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/#System-Identification-Problem","page":"Kalman filtering and smoothing","title":"System Identification Problem","text":"","category":"section"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"In this example we are going to attempt to run Bayesian inference and decouple two random-walk signals, which were combined into a single single through some deterministic function f. We do not have access to the real values of these signals, but only to their combination. First, we create the generate_data function that accepts f as an argument:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"using RxInfer, Distributions, StableRNGs, Plots","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"function generate_data(f, n; seed = 123, x_i_min = -20.0, w_i_min = 20.0, noise = 20.0, real_x_τ = 0.1, real_w_τ = 1.0)\n\n    rng = StableRNG(seed)\n\n    real_x = Vector{Float64}(undef, n)\n    real_w = Vector{Float64}(undef, n)\n    real_y = Vector{Float64}(undef, n)\n\n    for i in 1:n\n        real_x[i] = rand(rng, Normal(x_i_min, sqrt(1.0 / real_x_τ)))\n        real_w[i] = rand(rng, Normal(w_i_min, sqrt(1.0 / real_w_τ)))\n        real_y[i] = rand(rng, Normal(f(real_x[i], real_w[i]), sqrt(noise)))\n\n        x_i_min = real_x[i]\n        w_i_min = real_w[i]\n    end\n    \n    return real_x, real_w, real_y\nend","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"generate_data (generic function with 2 methods)","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"The function returns the real signals real_x and  real_w for later comparison (we are not going to use them during inference) and their combined version real_y (we are going to use it as our observations during the inference). We also assume that real_y is corrupted with some measurement noise.","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/#Combination-1:-y-x-w","page":"Kalman filtering and smoothing","title":"Combination 1: y = x + w","text":"","category":"section"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"In our first example, we are going to use a simple addition (+) as the function f. In general, it is impossible to decouple the signals x and w without strong priors, but we can try and see how good an inference can be. The + operation on two random variables also has a special meaning in the probabilistic inference, namely the convolution of pdf's of the two random variables, and RxInfer treats it specially with many precomputed analytical rules, which may make the inference task easier. First, let us create a test dataset:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"n = 250\nreal_x, real_w, real_y = generate_data(+, n);\n\npl = plot(title = \"Underlying signals\")\npl = plot!(pl, real_x, label = \"x\")\npl = plot!(pl, real_w, label = \"w\")\n\npr = plot(title = \"Combined y = x + w\")\npr = scatter!(pr, real_y, ms = 3, color = :red, label = \"y\")\n\nplot(pl, pr, size = (800, 300))","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"To run inference, we need to create a probabilistic model: our beliefs about how our data could have been generated. For this we can use the @model macro from RxInfer.jl:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"@model function identification_problem(f, y, m_x_0, τ_x_0, a_x, b_x, m_w_0, τ_w_0, a_w, b_w, a_y, b_y)\n    \n    x0 ~ Normal(mean = m_x_0, precision = τ_x_0)\n    τ_x ~ Gamma(shape = a_x, rate = b_x)\n    w0 ~ Normal(mean = m_w_0, precision = τ_w_0)\n    τ_w ~ Gamma(shape = a_w, rate = b_w)\n    τ_y ~ Gamma(shape = a_y, rate = b_y)\n    \n    x_i_min = x0\n    w_i_min = w0\n\n    local x\n    local w\n    local s\n    \n    for i in 1:length(y)\n        x[i] ~ Normal(mean = x_i_min, precision = τ_x)\n        w[i] ~ Normal(mean = w_i_min, precision = τ_w)\n        s[i] := f(x[i], w[i])\n        y[i] ~ Normal(mean = s[i], precision = τ_y)\n        \n        x_i_min = x[i]\n        w_i_min = w[i]\n    end\n    \nend","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"RxInfer runs Bayesian inference as a variational optimisation procedure between the real solution and its variational proxy q. In our model specification we assumed noise components to be unknown, thus, we need to enforce a structured mean-field assumption for the variational family of distributions q. This inevitably reduces the accuracy of the result, but makes the task easier and allows for fast and analytical message passing-based variational inference:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"constraints = @constraints begin \n    q(x0, w0, x, w, τ_x, τ_w, τ_y, s) = q(x, x0, w, w0, s)q(τ_w)q(τ_x)q(τ_y)\nend","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"Constraints: \n  q(x0, w0, x, w, τ_x, τ_w, τ_y, s) = q(x, x0, w, w0, s)q(τ_w)q(τ_x)q(τ_y)","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"The next step is to assign priors, initialise needed messages and marginals and call the inference function:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"m_x_0, τ_x_0 = -20.0, 1.0\nm_w_0, τ_w_0 = 20.0, 1.0\n\n# We set relatively strong priors for random walk noise components\n# and sort of vague prior for the noise of the observations\na_x, b_x = 0.01, 0.01var(real_x)\na_w, b_w = 0.01, 0.01var(real_w)\na_y, b_y = 1.0, 1.0\n\n# We set relatively strong priors for messages\nxinit = map(r -> NormalMeanPrecision(r, τ_x_0), reverse(range(-60, -20, length = n)))\nwinit = map(r -> NormalMeanPrecision(r, τ_w_0), range(20, 60, length = n))\n\n\ninit = @initialization begin\n    μ(x) = xinit\n    μ(w) = winit\n    q(τ_x) = GammaShapeRate(a_x, b_x)\n    q(τ_w) = GammaShapeRate(a_w, b_w)\n    q(τ_y) = GammaShapeRate(a_y, b_y)\nend\n\nresult = infer(\n    model = identification_problem(f=+, m_x_0=m_x_0, τ_x_0=τ_x_0, a_x=a_x, b_x=b_x, m_w_0=m_w_0, τ_w_0=τ_w_0, a_w=a_w, b_w=b_w, a_y=a_y, b_y=b_y),\n    data  = (y = real_y,), \n    options = (limit_stack_depth = 500, ), \n    constraints = constraints, \n    initialization = init,\n    iterations = 50\n)","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"Inference results:\n  Posteriors       | available for (x, w, x0, s, τ_x, τ_w, τ_y, w0)","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"Let's examine our inference results:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"τ_x_marginals = result.posteriors[:τ_x]\nτ_w_marginals = result.posteriors[:τ_w]\nτ_y_marginals = result.posteriors[:τ_y]\n\nsmarginals = result.posteriors[:s]\nxmarginals = result.posteriors[:x]\nwmarginals = result.posteriors[:w];","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"px1 = plot(legend = :bottomleft, title = \"Estimated hidden signals\")\npx2 = plot(legend = :bottomright, title = \"Estimated combined signals\")\n\npx1 = plot!(px1, real_x, label = \"Real hidden X\")\npx1 = plot!(px1, mean.(xmarginals[end]), ribbon = var.(xmarginals[end]), label = \"Estimated X\")\n\npx1 = plot!(px1, real_w, label = \"Real hidden W\")\npx1 = plot!(px1, mean.(wmarginals[end]), ribbon = var.(wmarginals[end]), label = \"Estimated W\")\n\npx2 = scatter!(px2, real_y, label = \"Observations\", ms = 2, alpha = 0.5, color = :red)\npx2 = plot!(px2, mean.(smarginals[end]), ribbon = std.(smarginals[end]), label = \"Combined estimated signal\", color = :green)\n\nplot(px1, px2, size = (800, 300))","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"The inference results are not so bad, even though RxInfer missed the correct values of the signals between 100 and 150.","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/#Combination-2:-y-min(x,-w)","page":"Kalman filtering and smoothing","title":"Combination 2: y = min(x, w)","text":"","category":"section"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"In this example we use a slightly more complex function, for which RxInfer does not have precomputed analytical message update rules. We are going to attempt to run Bayesian inference with min as a combination function. Note, however, that directly using min may cause problems for the built-in approximation methods as it has zero partial derviates with respect to all but one of the variables. We generate data with the min function directly however we model it with a somewhat smoothed version:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"# Smoothed version of `min` without zero-ed derivatives\nfunction smooth_min(x, y)    \n    if x < y\n        return x + 1e-4 * y\n    else\n        return y + 1e-4 * x\n    end\nend","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"smooth_min (generic function with 1 method)","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"RxInfer supports arbitrary nonlinear functions, but it requires an explicit approximation method specification. That can be achieved with the built-in @meta macro:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"min_meta = @meta begin \n    # In this example we are going to use a simple `Linearization` method\n    smooth_min() -> Linearization()\nend","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"Meta: \n  smooth_min() -> Linearization()","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"n = 200\nmin_real_x, min_real_w, min_real_y = generate_data(min, n, seed = 1, x_i_min = 0.0, w_i_min = 0.0, noise = 1.0, real_x_τ = 1.0, real_w_τ = 1.0);\n\npl = plot(title = \"Underlying signals\")\npl = plot!(pl, min_real_x, label = \"x\")\npl = plot!(pl, min_real_w, label = \"w\")\n\npr = plot(title = \"Combined y = min(x, w)\")\npr = scatter!(pr, min_real_y, ms = 3, color = :red, label = \"y\")\n\nplot(pl, pr, size = (800, 300))","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"min_m_x_0, min_τ_x_0 = -1.0, 1.0\nmin_m_w_0, min_τ_w_0 = 1.0, 1.0\n\nmin_a_x, min_b_x = 1.0, 1.0\nmin_a_w, min_b_w = 1.0, 1.0\nmin_a_y, min_b_y = 1.0, 1.0\n\ninit = @initialization begin\n   μ(x) = NormalMeanPrecision(min_m_x_0, min_τ_x_0) \n   μ(w) = NormalMeanPrecision(min_m_w_0, min_τ_w_0)\n   q(τ_x) = GammaShapeRate(min_a_x, min_b_x) \n   q(τ_w) = GammaShapeRate(min_a_w, min_b_w)\n   q(τ_y) = GammaShapeRate(min_a_y, min_b_y)\nend\n\n\nmin_result = infer(\n    model = identification_problem(f=smooth_min, m_x_0=min_m_x_0, τ_x_0=min_τ_x_0, a_x=min_a_x, b_x=min_b_x, m_w_0=min_m_w_0, τ_w_0=min_τ_w_0, a_w=min_a_w, b_w=min_b_w, a_y=min_a_y, b_y=min_b_y),\n    data  = (y = min_real_y,), \n    options = (limit_stack_depth = 500, ), \n    constraints = constraints, \n    initialization = init,\n    meta = min_meta,\n    iterations = 50\n)","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"Inference results:\n  Posteriors       | available for (x, w, x0, s, τ_x, τ_w, τ_y, w0)","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"min_τ_x_marginals = min_result.posteriors[:τ_x]\nmin_τ_w_marginals = min_result.posteriors[:τ_w]\nmin_τ_y_marginals = min_result.posteriors[:τ_y]\n\nmin_smarginals = min_result.posteriors[:s]\nmin_xmarginals = min_result.posteriors[:x]\nmin_wmarginals = min_result.posteriors[:w]\n\npx1 = plot(legend = :bottomleft, title = \"Estimated hidden signals\")\npx2 = plot(legend = :bottomright, title = \"Estimated combined signals\")\n\npx1 = plot!(px1, min_real_x, label = \"Real hidden X\")\npx1 = plot!(px1, mean.(min_xmarginals[end]), ribbon = var.(min_xmarginals[end]), label = \"Estimated X\")\n\npx1 = plot!(px1, min_real_w, label = \"Real hidden W\")\npx1 = plot!(px1, mean.(min_wmarginals[end]), ribbon = var.(min_wmarginals[end]), label = \"Estimated W\")\n\npx2 = scatter!(px2, min_real_y, label = \"Observations\", ms = 2, alpha = 0.5, color = :red)\npx2 = plot!(px2, mean.(min_smarginals[end]), ribbon = std.(min_smarginals[end]), label = \"Combined estimated signal\", color = :green)\n\nplot(px1, px2, size = (800, 300))","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"As we can see inference with the min function is significantly harder. Even though the combined signal has been inferred with high precision the underlying x and w signals are barely inferred. This may be expected, since the min function essentially destroy the information about one of the signals, thus, making it impossible to decouple two seemingly identical random walk signals. The only one inferred signal is the one which is lower and we have no inference information about the signal which is above. It might be possible to infer the states, however, with more informative priors and structural information about two different signals (e.g. if these are not random walks). ","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/#Online-(filtering)-identification:-y-min(x,-w)","page":"Kalman filtering and smoothing","title":"Online (filtering) identification: y = min(x, w)","text":"","category":"section"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"Another way to approach to this problem is to use online (filtering) inference procedure from RxInfer, but for that we also need to modify our model specification a bit:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"@model function rx_identification(f, m_x_0, τ_x_0, m_w_0, τ_w_0, a_x, b_x, a_y, b_y, a_w, b_w, y)\n    x0 ~ Normal(mean = m_x_0, precision = τ_x_0)\n    τ_x ~ Gamma(shape = a_x, rate = b_x)\n    w0 ~ Normal(mean = m_w_0, precision = τ_w_0)\n    τ_w ~ Gamma(shape = a_w, rate = b_w)\n    τ_y ~ Gamma(shape = a_y, rate = b_y)\n    \n    x ~ Normal(mean = x0, precision = τ_x)\n    w ~ Normal(mean = w0, precision = τ_w)\n\n    s := f(x, w)\n    y ~ Normal(mean = s, precision = τ_y)\n    \nend","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"We impose structured mean-field assumption for this model as well:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"rx_constraints = @constraints begin \n    q(x0, x, w0, w, τ_x, τ_w, τ_y, s) = q(x0, x)q(w, w0)q(τ_w)q(τ_x)q(s)q(τ_y)\nend","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"Constraints: \n  q(x0, x, w0, w, τ_x, τ_w, τ_y, s) = q(x0, x)q(w, w0)q(τ_w)q(τ_x)q(s)q(τ_y\n)","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"Online inference in the RxInfer supports the @autoupdates specification, which tells inference procedure how to update priors based on new computed posteriors:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"autoupdates = @autoupdates begin \n    m_x_0, τ_x_0 = mean_precision(q(x))\n    m_w_0, τ_w_0 = mean_precision(q(w))\n    a_x = shape(q(τ_x)) \n    b_x = rate(q(τ_x))\n    a_y = shape(q(τ_y))\n    b_y = rate(q(τ_y))\n    a_w = shape(q(τ_w)) \n    b_w = rate(q(τ_w))\nend","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"(m_x_0,τ_x_0 = mean_precision(q(x)), m_w_0,τ_w_0 = mean_precision(q(w)), a_\nx = shape(q(τ_x)), b_x = rate(q(τ_x)), a_y = shape(q(τ_y)), b_y = rate(q(τ_\ny)), a_w = shape(q(τ_w)), b_w = rate(q(τ_w)))","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"As previously we need to define the @meta structure that specifies the approximation method for the nonlinear function smooth_min (f in the model specification):","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"rx_meta = @meta begin \n    smooth_min() -> Linearization()\nend","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"Meta: \n  smooth_min() -> Linearization()","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"Next step is to generate our dataset and to run the actual inference procedure! For that we use the infer function with autoupdates keyword:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"n = 300\nrx_real_x, rx_real_w, rx_real_y = generate_data(min, n, seed = 1, x_i_min = 1.0, w_i_min = -1.0, noise = 1.0, real_x_τ = 1.0, real_w_τ = 1.0);\n\npl = plot(title = \"Underlying signals\")\npl = plot!(pl, rx_real_x, label = \"x\")\npl = plot!(pl, rx_real_w, label = \"w\")\n\npr = plot(title = \"Combined y = min(x, w)\")\npr = scatter!(pr, rx_real_y, ms = 3, color = :red, label = \"y\")\n\nplot(pl, pr, size = (800, 300))","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"init = @initialization begin\n    q(w)= NormalMeanVariance(-2.0, 1.0) \n    q(x) = NormalMeanVariance(2.0, 1.0) \n    q(τ_x) = GammaShapeRate(1.0, 1.0) \n    q(τ_w) = GammaShapeRate(1.0, 1.0) \n    q(τ_y) = GammaShapeRate(1.0, 20.0)\nend\n\nengine = infer(\n    model         = rx_identification(f=smooth_min),\n    constraints   = rx_constraints,\n    data          = (y = rx_real_y,),\n    autoupdates   = autoupdates,\n    meta          = rx_meta,\n    returnvars    = (:x, :w, :τ_x, :τ_w, :τ_y, :s),\n    keephistory   = 1000,\n    historyvars   =  KeepLast(),\n    initialization = init,\n    iterations    = 10,\n    free_energy = true, \n    free_energy_diagnostics = nothing,\n    autostart     = true,\n)","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"RxInferenceEngine:\n  Posteriors stream    | enabled for (w, s, τ_x, τ_w, τ_y, x)\n  Free Energy stream   | enabled\n  Posteriors history   | available for (x, w, x0, s, τ_x, τ_w, τ_y, w0)\n  Free Energy history  | available\n  Enabled events       | [  ]","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"rx_smarginals = engine.history[:s]\nrx_xmarginals = engine.history[:x]\nrx_wmarginals = engine.history[:w];","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"px1 = plot(legend = :bottomleft, title = \"Estimated hidden signals\")\npx2 = plot(legend = :bottomright, title = \"Estimated combined signals\")\n\npx1 = plot!(px1, rx_real_x, label = \"Real hidden X\")\npx1 = plot!(px1, mean.(rx_xmarginals), ribbon = var.(rx_xmarginals), label = \"Estimated X\")\n\npx1 = plot!(px1, rx_real_w, label = \"Real hidden W\")\npx1 = plot!(px1, mean.(rx_wmarginals), ribbon = var.(rx_wmarginals), label = \"Estimated W\")\n\npx2 = scatter!(px2, rx_real_y, label = \"Observations\", ms = 2, alpha = 0.5, color = :red)\npx2 = plot!(px2, mean.(rx_smarginals), ribbon = std.(rx_smarginals), label = \"Combined estimated signal\", color = :green)\n\nplot(px1, px2, size = (800, 300))","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"The results are quite similar to the smoothing case and, as we can see, one of the random walk is again in the \"disabled\" state, does not infer anything and simply increases its variance (which is expected for the random walk).","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/#Handling-Missing-Data","page":"Kalman filtering and smoothing","title":"Handling Missing Data","text":"","category":"section"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"An interesting case in filtering and smoothing problems is the processing of missing data. It can happen that sometimes your reading devices failt to acquire the data leading to missing observation.","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"Let us assume that the following model generates the data","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"beginaligned\n    x_t sim mathcalNleft(x_t-1 10right) \n    y_t sim mathcalNleft(x_t P right) \nendaligned","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"with prior x_0 sim mathcalN(m_x_0 v_x_0). Suppose that our measurement device fails to acquire data from time to time.  In this case, instead of scalar observation haty_t in mathrmR we sometimes will catch missing observations.","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"using RxInfer, Plots","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"@model function smoothing(x0, y)\n    \n    P ~ Gamma(shape = 0.001, scale = 0.001)\n    x_prior ~ Normal(mean = mean(x0), var = var(x0)) \n\n    local x\n    x_prev = x_prior\n\n    for i in 1:length(y)\n        x[i] ~ Normal(mean = x_prev, precision = 1.0)\n        y[i] ~ Normal(mean = x[i], precision = P)\n        \n        x_prev = x[i]\n    end\n\nend","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"P = 1.0\nn = 250\n\nreal_signal     = map(e -> sin(0.05 * e), collect(1:n))\nnoisy_data      = real_signal + rand(Normal(0.0, sqrt(P)), n);\nmissing_indices = 100:125\nmissing_data    = similar(noisy_data, Union{Float64, Missing}, )\n\ncopyto!(missing_data, noisy_data)\n\nfor index in missing_indices\n    missing_data[index] = missing\nend","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"constraints = @constraints begin\n    q(x_prior, x, y, P) = q(x_prior, x)q(P)q(y)\nend","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"Constraints: \n  q(x_prior, x, y, P) = q(x_prior, x)q(P)q(y)","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"x0_prior = NormalMeanVariance(0.0, 1000.0)\ninitm = @initialization begin\n    q(P) = Gamma(0.001, 0.001)\nend\n\nresult = infer(\n    model = smoothing(x0=x0_prior), \n    data  = (y = missing_data,), \n    constraints = constraints,\n    initialization = initm, \n    returnvars = (x = KeepLast(),),\n    iterations = 20\n);","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"plot(real_signal, label = \"Noisy signal\", legend = :bottomright)\nscatter!(missing_indices, real_signal[missing_indices], ms = 2, opacity = 0.75, label = \"Missing region\")\nplot!(mean.(result.posteriors[:x]), ribbon = var.(result.posteriors[:x]), label = \"Estimated hidden state\")","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"(Image: )","category":"page"},{"location":"manuals/customization/postprocess/#user-guide-inference-postprocess","page":"Inference results postprocessing","title":"Inference results postprocessing","text":"","category":"section"},{"location":"manuals/customization/postprocess/","page":"Inference results postprocessing","title":"Inference results postprocessing","text":"infer allow users to postprocess the inference result with the postprocess = ... keyword argument. The inference engine  operates on wrapper types to distinguish between marginals and messages. By default these wrapper types are removed from the inference results if no addons option is present. Together with the enabled addons, however, the wrapper types are preserved in the inference result output value. Use the options below to change this behaviour:","category":"page"},{"location":"manuals/customization/postprocess/","page":"Inference results postprocessing","title":"Inference results postprocessing","text":"inference_postprocess\nDefaultPostprocess\nUnpackMarginalPostprocess\nNoopPostprocess","category":"page"},{"location":"manuals/customization/postprocess/#RxInfer.inference_postprocess","page":"Inference results postprocessing","title":"RxInfer.inference_postprocess","text":"inference_postprocess(strategy, result)\n\nThis function modifies the result of the inference procedure according to the strategy.  The result can be a Marginal or a collection of Marginals. The default strategy is DefaultPostprocess.\n\n\n\n\n\n","category":"function"},{"location":"manuals/customization/postprocess/#RxInfer.DefaultPostprocess","page":"Inference results postprocessing","title":"RxInfer.DefaultPostprocess","text":"DefaultPostprocess picks the most suitable postprocessing step automatically.\n\n\n\n\n\n","category":"type"},{"location":"manuals/customization/postprocess/#RxInfer.UnpackMarginalPostprocess","page":"Inference results postprocessing","title":"RxInfer.UnpackMarginalPostprocess","text":"This postprocessing step removes the Marginal wrapper type from the result.\n\n\n\n\n\n","category":"type"},{"location":"manuals/customization/postprocess/#RxInfer.NoopPostprocess","page":"Inference results postprocessing","title":"RxInfer.NoopPostprocess","text":"This postprocessing step does nothing.\n\n\n\n\n\n","category":"type"},{"location":"manuals/customization/postprocess/#user-guide-inference-postprocess-custom","page":"Inference results postprocessing","title":"Custom postprocessing step","text":"","category":"section"},{"location":"manuals/customization/postprocess/","page":"Inference results postprocessing","title":"Inference results postprocessing","text":"In order to implement a custom postprocessing strategy simply implement the inference_postprocess method:","category":"page"},{"location":"manuals/customization/postprocess/","page":"Inference results postprocessing","title":"Inference results postprocessing","text":"using RxInfer\n\nstruct CustomPostprocess end\n\n# For demonstration purposes out postprocessing step simply stringifyes the result\nRxInfer.inference_postprocess(::CustomPostprocess, result::Marginal) = string(ReactiveMP.getdata(result))","category":"page"},{"location":"manuals/customization/postprocess/","page":"Inference results postprocessing","title":"Inference results postprocessing","text":"Now, we can use the postprocessing step in the infer function:","category":"page"},{"location":"manuals/customization/postprocess/","page":"Inference results postprocessing","title":"Inference results postprocessing","text":"using Test #hide\n@model function beta_bernoulli(y)\n    θ ~ Beta(1, 1)\n    y ~ Bernoulli(θ)\nend\n\nresult = infer(\n    model = beta_bernoulli(),\n    data  = (y = 1.,),\n    postprocess = CustomPostprocess()\n)\n\n@test occursin(\"Beta{Float64}(α=2.0, β=1.0)\", result.posteriors[:θ]) #hide\nresult.posteriors[:θ] # should be a `String`","category":"page"},{"location":"manuals/customization/postprocess/","page":"Inference results postprocessing","title":"Inference results postprocessing","text":"@test result.posteriors[:θ] isa String #hide\nresult.posteriors[:θ] isa String","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/#examples-conjugate-computational-variational-message-passing-(cvi)","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"","category":"section"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"using RxInfer, Random, LinearAlgebra, Plots, Optimisers, Plots, StableRNGs, SpecialFunctions","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"In this notebook, the usage of Conjugate-NonConjugate Variational Inference (CVI) will be described. The implementation of CVI follows the paper Probabilistic programming with stochastic variational message passing.","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"This notebook will first describe an example in which CVI is used, then it discusses several limitations, followed by an explanation on how to extend upon CVI.","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/#An-example:-nonlinear-dynamical-system","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"An example: nonlinear dynamical system","text":"","category":"section"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"A group of researchers is performing a tracking experiment on some moving object along a 1-dimensional trajectory. The object is moving at a constant velocity, meaning that its position increases constantly over time. However, the researchers do not have access to the absolute position z_t at time t. Instead they have access to the observed squared distance y_t between the object and some reference point s. Because of budget cuts, the servo moving the object and the measurement devices are quite outdated and therefore lead to noisy measurements:","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"# data generating process\nnr_observations = 50\nreference_point = 53\nhidden_location = collect(1:nr_observations) + rand(StableRNG(124), NormalMeanVariance(0.0, sqrt(5)), nr_observations)\nmeasurements = (hidden_location .- reference_point).^2 + rand(MersenneTwister(124), NormalMeanVariance(0.0, 5), nr_observations);","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"# plot hidden location and reference frame\np1 = plot(1:nr_observations, hidden_location, linewidth=3, legend=:topleft, label=\"hidden location\")\nhline!([reference_point], linewidth=3, label=\"reference point\")\nxlabel!(\"time [sec]\"), ylabel!(\"location [cm]\")\n\n# plot measurements\np2 = scatter(1:nr_observations, measurements, linewidth=3, label=\"measurements\")\nxlabel!(\"time [sec]\"), ylabel!(\"squared distance [cm2]\")\n\nplot(p1, p2, size=(1200, 500))","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"The researchers are interested in quantifying this noise and in tracking the unobserved location of the object. As a result of this uncertainty, the researchers employ a probabilistic modeling approach. They formulate the probabilistic model","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"beginaligned\n p(tau)  = Gamma(tau mid alpha_tau beta_tau)\n p(gamma)  = Gamma(gamma mid alpha_gamma beta_gamma)\n p(z_t mid z_t - 1 tau)  = mathcalN(z_t mid z_t - 1 + 1 tau^-1)\n p(y_t mid z_t gamma)  = mathcalN(y_t mid (z_t - s)^2 gamma^-1)\nendaligned","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"where the researchers put priors on the process and measurement noise parameters, tau and gamma, respectively. They do this, because they do not know the accuracy of their devices.","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"The researchers have recently heard of this cool probabilistic programming package RxInfer.jl. They decided to give it a try and create the above model as follows:","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"function compute_squared_distance(z)\n    (z - reference_point)^2\nend;","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"@model function measurement_model(y)\n\n    # set priors on precision parameters\n    τ ~ Gamma(shape = 0.01, rate = 0.01)\n    γ ~ Gamma(shape = 0.01, rate = 0.01)\n    \n    # specify estimate of initial location\n    z[1] ~ Normal(mean = 0, precision = τ)\n    y[1] ~ Normal(mean = compute_squared_distance(z[1]), precision = γ)\n\n    # loop over observations\n    for t in 2:length(y)\n\n        # specify state transition model\n        z[t] ~ Normal(mean = z[t-1] + 1, precision = τ)\n\n        # specify non-linear observation model\n        y[t] ~ Normal(mean = compute_squared_distance(z[t]), precision = γ)\n        \n    end\n\nend","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"But here is the problem, our compute_squared_distance function is already compelx enough such that the exact Bayesian inference is intractable in this model. But the researchers knew that the RxInfer.jl supports a various collection of approximation methods for exactly such cases. One of these approximations is called CVI. CVI allows us to perform probabilistic inference around the non-linear measurement function. In general, for any (non-)linear relationship y = f(x1, x2, ..., xN) CVI can be employed, by specifying the function f and by adding this relationship inside the @model macro as y ~ f(x1, x2, ...,xN). The @model macro will generate a factor node with node function p(y | x1, x2, ..., xN) = δ(y - f(x1, x2, ...,xN)).","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"The use of this non-linearity requires us to specify that we would like to use CVI. This can be done by specifying the metadata using the @meta macro as:","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"@meta function measurement_meta(rng, nr_samples, nr_iterations, optimizer)\n    compute_squared_distance() -> CVI(rng, nr_samples, nr_iterations, optimizer)\nend;","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"In general, for any (non-)linear function f(), CVI can be enabled with the @meta macro as:","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"@meta function model_meta(...)\n    f() -> CVI(args...)\nend","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"See ?CVI for more information about the args....","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"In our model, the z variables are connected to the non-linear node function. So in order to run probabilstic inference with CVI we need to enforce a constraint on the joint posterior distribution. Specifically, we need to create a factorization in which the variables that are directly connected to non-linearities are assumed to be independent from the rest of the variables.","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"In the above example, we will assume the following posterior factorization:","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"@constraints function measurement_constraints()\n    q(z, τ, γ) = q(z)q(τ)q(γ)\nend;","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"This constraint can be explained by the set of two constraints, one for getting CVI to run, and one for assuming a mean-field factorization around the normal node as ","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"@constraints function posterior_constraints() begin\n    q(z, γ) = q(z)q(γ) # CVI\n    q(z, τ) = q(z)q(τ) # the mean-field assumption around normal node\nend","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"Because the engineers are using RxInfer.jl, they can automate the inference procedure. They track the inference performance using the Bethe free energy.","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"initialization = @initialization begin\n    μ(z) = NormalMeanVariance(0, 5)\n    q(z) = NormalMeanVariance(0, 5)\n    q(τ) = GammaShapeRate(1e-12, 1e-3)\n    q(γ) = GammaShapeRate(1e-12, 1e-3)\nend\n\nresults = infer(\n    model = measurement_model(),\n    data = (y = measurements,),\n    iterations = 50,\n    free_energy = true,\n    returnvars = (z = KeepLast(),),\n    constraints = measurement_constraints(),\n    meta = measurement_meta(StableRNG(42), 1000, 1000, Optimisers.Descent(0.001)),\n    initialization = initialization\n)","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"Inference results:\n  Posteriors       | available for (z)\n  Free Energy:     | Real[506.207, 327.398, 325.066, 320.864, 315.809, 312.\n717, 310.775, 309.344, 308.486, 308.456  …  306.705, 306.266, 306.172, 306.\n587, 306.621, 306.501, 306.382, 306.512, 306.659, 306.485]","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"# plot estimates for location\np1 = plot(collect(1:nr_observations), hidden_location, label = \"hidden location\", legend=:topleft, linewidth=3, color = :red)\nplot!(map(mean, results.posteriors[:z]), label = \"estimated location (±2σ)\", ribbon = map(x -> 2*std(x), results.posteriors[:z]), fillalpha=0.5, linewidth=3, color = :orange)\nxlabel!(\"time [sec]\"), ylabel!(\"location [cm]\")\n\n# plot Bethe free energy\np2 = plot(results.free_energy, linewidth=3, label = \"\")\nxlabel!(\"iteration\"), ylabel!(\"Bethe free energy [nats]\")\n\nplot(p1, p2, size = (1200, 500))","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/#Requirements","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Requirements","text":"","category":"section"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"There are several main requirements for the CVI procedure to satisfy:","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"The out interface of the non-linearity must be independently factorized with respect to other variables in the model.\nThe messages on input interfaces (x1, x2, ..., xN) are required to be from the exponential family of distributions.","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"In RxInfer, you can satisfy the first requirement by using appropriate factor nodes (Normal, Gamma, Bernoulli, etc) and second requirement by specifying the @constraints macro. In general you can specify this procedure as","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"@model function model(...)\n    ...\n    y ~ f(x1, x2, ..., xN)\n    ... ~ Node2(z1,..., y, zM) # some node that is using the out interface of the non-linearity\n    ... \nend\n\n@constraints function constraints_meta() begin\n    q(y, z1, ..., zn) = q(y)q(z1,...,zM)\n    ...\nend;\n\n@meta function model_meta(...)\n    f() -> CVI(rng, nr_samples, nr_iterations, optimizer))\nend","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"Note that not all exponential family distributions are implemented.","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/#Extensions","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Extensions","text":"","category":"section"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/#Using-a-custom-optimizer","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Using a custom optimizer","text":"","category":"section"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"CVI only supports Optimisers optimizers out of the box.","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"Below an explanation on how to extend to it to a custom optimizer.","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"Suppose we have CustomDescent structure which we want to use inside CVI for optimization.","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"To do so, we need to implement ReactiveMP.cvi_update!(opt::CustomDescent, λ, ∇).","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"ReactiveMP.cvi_update! incapsulates the gradient step:","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"opt is used to select your optimizer structure\nλ is the current value\n∇ is a gradient value computed inside CVI.","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"struct CustomDescent \n    learning_rate::Float64\nend\n\n# Must return an optimizer and its initial state\nfunction ReactiveMP.cvi_setup(opt::CustomDescent, q)\n     return (opt, nothing)\nend\n\n# Must return an updated (opt, state) and an updated λ (can use new_λ for inplace operation)\nfunction ReactiveMP.cvi_update!(opt_and_state::Tuple{CustomDescent, Nothing}, new_λ, λ, ∇)\n    opt, _ = opt_and_state\n    λ̂ = vec(λ) - (opt.learning_rate .* vec(∇))\n    copyto!(new_λ, λ̂)\n    return opt_and_state, new_λ\nend","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"Let's try to apply it to a model: beginaligned  p(x)  = mathcalN(0 1)\n p(y_imid x)  = mathcalN(y_i mid x^2 1)\nendaligned","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"Let's generate some synthetic data for the model","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"# generate data\ny = rand(StableRNG(123), NormalMeanVariance(19^2, 10), 1000)\nhistogram(y)","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"Again we can create the corresponding model as:","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"# specify non-linearity\nf(x) = x ^ 2\n\n# specify model\n@model function normal_square_model(y)\n    # describe prior on latent state, we set an arbitrary prior \n    # in a positive domain\n    x ~ Normal(mean = 5, precision = 1e-3)\n    # transform latent state\n    mean := f(x)\n    # observation model\n    y .~ Normal(mean = mean, precision = 0.1)\nend\n\n# specify meta\n@meta function normal_square_meta(rng, nr_samples, nr_iterations, optimizer)\n    f() ->  CVI(rng, nr_samples, nr_iterations, optimizer)\nend","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"normal_square_meta (generic function with 1 method)","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"We will use the inference function from ReactiveMP to run inference, where we provide an instance of the CustomDescent structure in our meta macro function:","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"res = infer(\n    model = normal_square_model(),\n    data = (y = y,),\n    iterations = 5,\n    free_energy = true,\n    meta = normal_square_meta(StableRNG(123), 1000, 1000, CustomDescent(0.001)),\n    free_energy_diagnostics = nothing\n)\n\nmean(res.posteriors[:x][end])","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"18.992828536095285","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"The mean inferred value of x is indeed close to 19, which was used to generate the data. Inference is working! ","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"p1 = plot(mean.(res.posteriors[:x]), ribbon = 3std.(res.posteriors[:x]), label = \"Posterior estimation\", ylim = (0, 40))\np2 = plot(res.free_energy, label = \"Bethe Free Energy\")\n\nplot(p1, p2, layout = @layout([ a b ]))","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"Note: x^2 can not be inverted; the sign information can be lost: -19 and 19 are both equally good solutions.","category":"page"},{"location":"library/functional-forms/#lib-forms","page":"Functional form constraints","title":"Built-in Functional Forms","text":"","category":"section"},{"location":"library/functional-forms/","page":"Functional form constraints","title":"Functional form constraints","text":"This section describes built-in functional forms that can be used for posterior marginal and/or messages form constraints specification. Read more information about constraints specification syntax in the Constraints Specification section.","category":"page"},{"location":"library/functional-forms/#lib-forms-background","page":"Functional form constraints","title":"Background","text":"","category":"section"},{"location":"library/functional-forms/","page":"Functional form constraints","title":"Functional form constraints","text":"In message passing framework, in order to compute a posterior over some latent state q(x), it is necessary to compute a normalized product of two messages q(x) = fracmu_1(x) mu_2(x)int mu_1(x) mu_2(x) mathrmdx In some situations, when functional forms of mu_1(x) and mu_2(x) are know in advance, it is possible to compute the normalized product efficiently and analytically. It is, however, not always the case, since the messages can have arbitrary functional form and it is not always easy to compute the normalization factor.","category":"page"},{"location":"library/functional-forms/","page":"Functional form constraints","title":"Functional form constraints","text":"Functional forms help to circumvent this. They implement a custom callback on the product of two messages, which cannot  be computed analytically. Essentially, a functional form constraint defines a functional F, such that  q(x) = Fmu_1 mu_2 approx fracmu_1(x) mu_2(x)int mu_1(x) mu_2(x) mathrmdx","category":"page"},{"location":"library/functional-forms/","page":"Functional form constraints","title":"Functional form constraints","text":"See also Bethe Free Energy section for more information on variational inference and posterior computation.","category":"page"},{"location":"library/functional-forms/#lib-forms-unspecified-constraint","page":"Functional form constraints","title":"UnspecifiedFormConstraint","text":"","category":"section"},{"location":"library/functional-forms/","page":"Functional form constraints","title":"Functional form constraints","text":"Unspecified functional form constraint is used by default and uses only analytical update rules for computing posterior marginals. Throws an error if a product of two colliding messages cannot be computed analytically.","category":"page"},{"location":"library/functional-forms/","page":"Functional form constraints","title":"Functional form constraints","text":"using RxInfer, Distributions #hide\n@constraints begin \n    # This is the default setting for all latent variables\n    q(x) :: UnspecifiedFormConstraint() \nend\nnothing #hide","category":"page"},{"location":"library/functional-forms/#lib-forms-point-mass-constraint","page":"Functional form constraints","title":"PointMassFormConstraint","text":"","category":"section"},{"location":"library/functional-forms/","page":"Functional form constraints","title":"Functional form constraints","text":"The most basic form of posterior marginal approximation is the PointMass function. In a few words PointMass represents the delta function. In the context of functional form constraints PointMass approximation corresponds to the MAP estimate. For a given distribution d - PointMass functional form simply finds the argmax of the logpdf of q(x), thus q(x) = Fmu_1 mu_2 = delta(x - argmin_x mu_1(x) mu_2(x)). This is especially useful when exact functional form of q(x) is not available or cannot be parametrized efficiently. ","category":"page"},{"location":"library/functional-forms/","page":"Functional form constraints","title":"Functional form constraints","text":"@constraints begin \n    q(x) :: PointMassFormConstraint()\nend\n\n@constraints begin \n    q(x) :: PointMassFormConstraint(starting_point = (args...) -> 1.0)\nend\nnothing #hide","category":"page"},{"location":"library/functional-forms/","page":"Functional form constraints","title":"Functional form constraints","text":"RxInfer.PointMassFormConstraint\nRxInfer.default_point_mass_form_constraint_optimizer\nRxInfer.default_point_mass_form_constraint_starting_point\nRxInfer.default_point_mass_form_constraint_boundaries","category":"page"},{"location":"library/functional-forms/#RxInfer.PointMassFormConstraint","page":"Functional form constraints","title":"RxInfer.PointMassFormConstraint","text":"PointMassFormConstraint\n\nOne of the form constraint objects. Constraint a message to be in a form of dirac's delta point mass.  By default uses Optim.jl package to find argmin of -logpdf(x).  Accepts custom optimizer callback which might be used to customise optimisation procedure with different packages  or different arguments for Optim.jl package.\n\nKeyword arguments\n\noptimizer: specifies a callback function for logpdf optimisation. See also: RxInfer.default_point_mass_form_constraint_optimizer\nstarting_point: specifies a callback function for initial optimisation point: See also: RxInfer.default_point_mass_form_constraint_starting_point\nboundaries: specifies a callback function for determining optimisation boundaries: See also: RxInfer.default_point_mass_form_constraint_boundaries\n\nCustom optimizer callback interface\n\n# This is an example of the `custom_optimizer` interface\nfunction custom_optimizer(::Type{ Univariate }, ::Type{ Continuous }, constraint::PointMassFormConstraint, distribution)\n    # should return argmin of the -logpdf(distribution)\nend\n\nCustom starting point callback interface\n\n# This is an example of the `custom_starting_point` interface\nfunction custom_starting_point(::Type{ Univariate }, ::Type{ Continuous }, constraint::PointMassFormConstraint, distribution)\n    # built-in optimizer expects an array, even for a univariate distribution\n    return [ 0.0 ] \nend\n\nCustom boundaries callback interface\n\n# This is an example of the `custom_boundaries` interface\nfunction custom_boundaries(::Type{ Univariate }, ::Type{ Continuous }, constraint::PointMassFormConstraint, distribution)\n    # returns a tuple of `lower` and `upper` boundaries\n    return (-Inf, Inf)\nend\n\n\n\n\n\n","category":"type"},{"location":"library/functional-forms/#RxInfer.default_point_mass_form_constraint_optimizer","page":"Functional form constraints","title":"RxInfer.default_point_mass_form_constraint_optimizer","text":"default_point_mass_form_constraint_optimizer(::Type{<:VariateType}, ::Type{<:ValueSupport}, constraint::PointMassFormConstraint, distribution)\n\nDefines a default optimisation procedure for the PointMassFormConstraint. By default uses Optim.jl package to find argmin of -logpdf(x). Uses the starting_point and boundaries callbacks to determine the starting point and boundaries for the optimisation procedure.\n\n\n\n\n\n","category":"function"},{"location":"library/functional-forms/#RxInfer.default_point_mass_form_constraint_starting_point","page":"Functional form constraints","title":"RxInfer.default_point_mass_form_constraint_starting_point","text":"default_point_mass_form_constraint_starting_point(::Type{<:VariateType}, ::Type{<:ValueSupport}, constraint::PointMassFormConstraint, distribution)\n\nDefines a default starting point for the PointMassFormConstraint. By default uses the support of the distribution. If support is unbounded returns a zero point. Otherwise throws an error.\n\n\n\n\n\n","category":"function"},{"location":"library/functional-forms/#RxInfer.default_point_mass_form_constraint_boundaries","page":"Functional form constraints","title":"RxInfer.default_point_mass_form_constraint_boundaries","text":"default_point_mass_form_constraint_boundaries(::Type{<:VariateType}, ::Type{<:ValueSupport}, constraint::PointMassFormConstraint, distribution)\n\nDefines a default boundaries for the PointMassFormConstraint. By default simply uses the support of the distribution.\n\n\n\n\n\n","category":"function"},{"location":"library/functional-forms/#lib-forms-sample-list-constraint","page":"Functional form constraints","title":"SampleListFormConstraint","text":"","category":"section"},{"location":"library/functional-forms/","page":"Functional form constraints","title":"Functional form constraints","text":"SampleListFormConstraints approximates the resulting posterior marginal (product of two colliding messages) as a list of weighted samples. Hence, it requires one of the arguments to be a proper distribution (or at least the inference backend should be able to sample from it). This setting is controlled with LeftProposal(), RightProposal() or AutoProposal() objects. It also accepts an optional method object, but the only one available sampling method currently is the BayesBase.BootstrapImportanceSampling.","category":"page"},{"location":"library/functional-forms/","page":"Functional form constraints","title":"Functional form constraints","text":"@constraints begin \n    q(x) :: SampleListFormConstraint(1000)\n    # or \n    q(y) :: SampleListFormConstraint(1000, LeftProposal())\nend\nnothing #hide","category":"page"},{"location":"library/functional-forms/","page":"Functional form constraints","title":"Functional form constraints","text":"RxInfer.SampleListFormConstraint\nRxInfer.AutoProposal\nRxInfer.LeftProposal\nRxInfer.RightProposal","category":"page"},{"location":"library/functional-forms/#RxInfer.SampleListFormConstraint","page":"Functional form constraints","title":"RxInfer.SampleListFormConstraint","text":"SampleListFormConstraint(rng, strategy, method)\n\nOne of the form constraint objects. Approximates DistProduct with a SampleList object. \n\n\n\n\n\n","category":"type"},{"location":"library/functional-forms/#RxInfer.AutoProposal","page":"Functional form constraints","title":"RxInfer.AutoProposal","text":"Tries to determine the proposal distribution in the SampleList approximation automatically.\n\n\n\n\n\n","category":"type"},{"location":"library/functional-forms/#RxInfer.LeftProposal","page":"Functional form constraints","title":"RxInfer.LeftProposal","text":"Uses the left argument in the prod call as the proposal distribution in the SampleList approximation.\n\n\n\n\n\n","category":"type"},{"location":"library/functional-forms/#RxInfer.RightProposal","page":"Functional form constraints","title":"RxInfer.RightProposal","text":"Uses the right argument in the prod call as the proposal distribution in the SampleList approximation.\n\n\n\n\n\n","category":"type"},{"location":"library/functional-forms/#lib-forms-fixed-marginal-constraint","page":"Functional form constraints","title":"FixedMarginalFormConstraint","text":"","category":"section"},{"location":"library/functional-forms/","page":"Functional form constraints","title":"Functional form constraints","text":"Fixed marginal form constraint replaces the resulting posterior marginal obtained during the inference procedure with the prespecified one. Worth to note that the inference backend still tries to compute real posterior marginal and may fail during this process. Might be useful for debugging purposes. If nothing is passed then the computed posterior marginal is returned (see also UnspecifiedFormConstraint).","category":"page"},{"location":"library/functional-forms/","page":"Functional form constraints","title":"Functional form constraints","text":"@constraints function block_updates(x_posterior = nothing) \n    # `nothing` returns the computed posterior marginal\n    q(x) :: FixedMarginalFormConstraint(x_posterior)\nend\nnothing #hide","category":"page"},{"location":"library/functional-forms/","page":"Functional form constraints","title":"Functional form constraints","text":"RxInfer.FixedMarginalFormConstraint","category":"page"},{"location":"library/functional-forms/#RxInfer.FixedMarginalFormConstraint","page":"Functional form constraints","title":"RxInfer.FixedMarginalFormConstraint","text":"FixedMarginalFormConstraint\n\nOne of the form constraint objects. Provides a constraint on the marginal distribution such that it remains fixed during inference.  Can be viewed as blocking of updates of a specific edge associated with the marginal. If nothing is passed then the computed posterior marginal is returned. Use .fixed_value field to update the value of the constraint.\n\n\n\n\n\n","category":"type"},{"location":"library/functional-forms/","page":"Functional form constraints","title":"Functional form constraints","text":"It is also possible to control the constraint manually, e.g:","category":"page"},{"location":"library/functional-forms/","page":"Functional form constraints","title":"Functional form constraints","text":"form_constraint = FixedMarginalFormConstraint(nothing)\n\nconstraints_specification = @constraints function manual_block_updates(form_constraint) \n    q(x) :: form_constraint\nend\n\n# later on ...\nform_constraint.fixed_value = Gamma(1.0, 1.0)","category":"page"},{"location":"library/functional-forms/#lib-forms-composite-constraint","page":"Functional form constraints","title":"CompositeFormConstraint","text":"","category":"section"},{"location":"library/functional-forms/","page":"Functional form constraints","title":"Functional form constraints","text":"It is possible to create a composite functional form by stacking operators, e.g:","category":"page"},{"location":"library/functional-forms/","page":"Functional form constraints","title":"Functional form constraints","text":"@constraints begin \n    q(x) :: SampleListFormConstraint(1000) :: PointMassFormConstraint()\nend","category":"page"},{"location":"library/functional-forms/#lib-forms-custom-constraints","page":"Functional form constraints","title":"Custom functional forms","text":"","category":"section"},{"location":"library/functional-forms/","page":"Functional form constraints","title":"Functional form constraints","text":"See the ReactiveMP.jl library documentation for more information about defining novel custom functional forms that are compatible with ReactiveMP inference backend.","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/#examples-hierarchical-gaussian-filter","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"","category":"section"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"In this demo the goal is to perform approximate variational Bayesian Inference for Univariate Hierarchical Gaussian Filter (HGF).","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Simple HGF model can be defined as:","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"beginaligned\n  x^(j)_k  sim  mathcalN(x^(j)_k - 1 f_k(x^(j - 1)_k)) \n  y_k  sim  mathcalN(x^(j)_k tau_k)\nendaligned","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"where j is an index of layer in hierarchy, k is a time step and f_k is a variance activation function. RxInfer.jl export Gaussian Controlled Variance (GCV) node with f_k = exp(kappa x + omega) variance activation function. By default the node uses Gauss-Hermite cubature with a prespecified number of approximation points in the cubature. In this demo we also show how we can change the hyperparameters in different approximation methods (iin this case Gauss-Hermite cubature) with the help of metadata structures. Here how our model will look like with the GCV node:","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"beginaligned\n  z_k  sim  mathcalN(z_k - 1 mathcaltau_z) \n  x_k  sim  mathcalN(x_k - 1 exp(kappa z_k + omega)) \n  y_k  sim  mathcalN(x_k mathcaltau_y)\nendaligned","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"In this experiment we will create a single time step of the graph and perform variational message passing filtering alrogithm to estimate hidden states of the system. For a more rigorous introduction to Hierarchical Gaussian Filter we refer to Ismail Senoz, Online Message Passing-based Inference in the Hierarchical Gaussian Filter paper.","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"For simplicity we will consider tau_z, tau_y, kappa and omega known and fixed, but there are no principled limitations to make them random variables too.","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"To model this process in RxInfer, first, we start with importing all needed packages:","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"using RxInfer, BenchmarkTools, Random, Plots, StableRNGs","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Next step, is to generate some synthetic data:","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"function generate_data(rng, n, k, w, zv, yv)\n    z_prev = 0.0\n    x_prev = 0.0\n\n    z = Vector{Float64}(undef, n)\n    v = Vector{Float64}(undef, n)\n    x = Vector{Float64}(undef, n)\n    y = Vector{Float64}(undef, n)\n\n    for i in 1:n\n        z[i] = rand(rng, Normal(z_prev, sqrt(zv)))\n        v[i] = exp(k * z[i] + w)\n        x[i] = rand(rng, Normal(x_prev, sqrt(v[i])))\n        y[i] = rand(rng, Normal(x[i], sqrt(yv)))\n\n        z_prev = z[i]\n        x_prev = x[i]\n    end \n    \n    return z, x, y\nend","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"generate_data (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"# Seed for reproducibility\nseed = 42\n\nrng = StableRNG(seed)\n\n# Parameters of HGF process\nreal_k = 1.0\nreal_w = 0.0\nz_variance = abs2(0.2)\ny_variance = abs2(0.1)\n\n# Number of observations\nn = 300\n\nz, x, y = generate_data(rng, n, real_k, real_w, z_variance, y_variance);","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Let's plot our synthetic dataset. Lines represent our hidden states we want to estimate using noisy observations.","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"let \n    pz = plot(title = \"Hidden States Z\")\n    px = plot(title = \"Hidden States X\")\n    \n    plot!(pz, 1:n, z, label = \"z_i\", color = :orange)\n    plot!(px, 1:n, x, label = \"x_i\", color = :green)\n    scatter!(px, 1:n, y, label = \"y_i\", color = :red, ms = 2, alpha = 0.2)\n    \n    plot(pz, px, layout = @layout([ a; b ]))\nend","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"To create a model we use the @model macro:","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"# We create a single-time step of corresponding state-space process to\n# perform online learning (filtering)\n@model function hgf(y, κ, ω, z_variance, y_variance, z_prev_mean, z_prev_var, x_prev_mean, x_prev_var)\n\n    z_prev ~ Normal(mean = z_prev_mean, variance = z_prev_var)\n    x_prev ~ Normal(mean = x_prev_mean, variance = x_prev_var)\n\n    # Higher layer is modelled as a random walk \n    z_next ~ Normal(mean = z_prev, variance = z_variance)\n    \n    # Lower layer is modelled with `GCV` node\n    x_next ~ GCV(x_prev, z_next, κ, ω)\n    \n    # Noisy observations \n    y ~ Normal(mean = x_next, variance = y_variance)\nend\n\n@constraints function hgfconstraints() \n    # Structuted factorization constraints\n    q(x_next, x_prev, z_next) = q(x_next)q(x_prev)q(z_next)\nend\n\n@meta function hgfmeta()\n    # Lets use 31 approximation points in the Gauss Hermite cubature approximation method\n    GCV() -> GCVMetadata(GaussHermiteCubature(31)) \nend","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"hgfmeta (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"The code below uses the infer function from RxInfer to generate the message passing algorithm given the model and constraints specification.  We also specify the @autoupdates in order to set new priors for the next observation based on posteriors.","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"function run_inference(data, real_k, real_w, z_variance, y_variance)\n\n    autoupdates   = @autoupdates begin\n        # The posterior becomes the prior for the next time step\n        z_prev_mean, z_prev_var = mean_var(q(z_next))\n        x_prev_mean, x_prev_var = mean_var(q(x_next))\n    end\n\n    init = @initialization begin\n        q(x_next) = NormalMeanVariance(0.0, 5.0)\n        q(z_next) = NormalMeanVariance(0.0, 5.0)\n    end\n\n    return infer(\n        model          = hgf(κ = real_k, ω = real_w, z_variance = z_variance, y_variance = y_variance),\n        constraints    = hgfconstraints(),\n        meta           = hgfmeta(),\n        data           = (y = data, ),\n        autoupdates    = autoupdates,\n        keephistory    = length(data),\n        historyvars    = (\n            x_next = KeepLast(),\n            z_next = KeepLast()\n        ),\n        initialization = init,\n        iterations     = 5,\n        free_energy    = true,\n    )\nend","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"run_inference (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Everything is ready to run the algorithm. We used the online version of the algorithm, thus we need to fetch the history of the posterior estimation instead of the actual posteriors.","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"result = run_inference(y, real_k, real_w, z_variance, y_variance);\n\nmz = result.history[:z_next];\nmx = result.history[:x_next];","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"let \n    pz = plot(title = \"Hidden States Z\")\n    px = plot(title = \"Hidden States X\")\n    \n    plot!(pz, 1:n, z, label = \"z_i\", color = :orange)\n    plot!(pz, 1:n, mean.(mz), ribbon = std.(mz), label = \"estimated z_i\", color = :teal)\n    \n    plot!(px, 1:n, x, label = \"x_i\", color = :green)\n    plot!(px, 1:n, mean.(mx), ribbon = std.(mx), label = \"estimated x_i\", color = :violet)\n    \n    plot(pz, px, layout = @layout([ a; b ]))\nend","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"As we can see from our plot, estimated signal resembles closely to the real hidden states with small variance. We maybe also interested in the values for Bethe Free Energy functional:","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"plot(result.free_energy_history, label = \"Bethe Free Energy\")","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"As we can see BetheFreeEnergy converges nicely to a stable point.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = RxInfer","category":"page"},{"location":"#RxInfer","page":"Home","title":"RxInfer","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"<div class=\"light-biglogo\">","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: RxInfer Logo)","category":"page"},{"location":"","page":"Home","title":"Home","text":"</div>","category":"page"},{"location":"","page":"Home","title":"Home","text":"<div class=\"dark-biglogo\">","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: RxInfer Logo)","category":"page"},{"location":"","page":"Home","title":"Home","text":"</div>","category":"page"},{"location":"","page":"Home","title":"Home","text":"Julia package for automatic Bayesian inference on a factor graph with reactive message passing.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Given a probabilistic model, RxInfer allows for an efficient message-passing based Bayesian inference. It uses the model structure to generate an algorithm that consists of a sequence of local computations on a Forney-style factor graph (FFG) representation of the model. RxInfer.jl has been designed with a focus on efficiency, scalability and maximum performance for running inference with reactive message passing.","category":"page"},{"location":"#Why-RxInfer","page":"Home","title":"Why RxInfer","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Many important AI applications, including audio processing, self-driving vehicles, weather forecasting, and extended-reality video processing require continually solving an inference task in sophisticated probabilistic models with a large number of latent variables. Often, the inference task in these applications must be performed continually and in real-time in response to new observations.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Popular MC-based inference methods, such as the No U-Turn Sampler (NUTS) or Hamiltonian Monte Carlo (HMC) sampling, rely on computationally heavy sampling procedures that do not scale well to probabilistic models with thousands of latent states. Therefore, while MC-based inference is an very versatile tool, it is practically not suitable for real-time applications. While the alternative variational inference method (VI) promises to scale better to large models than sampling-based inference, VI requires the derivation of gradients of a \"Variational Free Energy\" cost function. For large models, manual derivation of these gradients might not be feasible, while automated \"black-box\" gradient methods do not scale either because they are not capable of taking advantage of sparsity or conjugate pairs in the model. Therefore, while Bayesian inference is known as the optimal data processing framework, in practice, real-time AI applications rely on much simpler, often ad hoc, data processing algorithms.","category":"page"},{"location":"","page":"Home","title":"Home","text":"RxInfer aims to remedy these issues by running efficient Bayesian inference in sophisticated probabilistic models, taking advantage of local conjugate relationships in probabilistic models, and focusing on real-time Bayesian inference in large state-space models with thousands of latent variables. In addition, RxInfer provides a straightforward way to extend its functionality with custom factor nodes and message passing update rules. The engine is capable of running various Bayesian inference algorithms in different parts of the factor graph of a single probabilistic model. This makes it easier to explore different \"what-if\" scenarios and enables very efficient inference in specific cases.","category":"page"},{"location":"#Package-Features","page":"Home","title":"Package Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"User friendly syntax for specification of probabilistic models, achieved with GraphPPL.\nSupport for hybrid models combining discrete and continuous latent variables.\nFactorization and functional form constraints specification.\nGraph visualisation and extensions with different custom plugins.\nSaving graph on a disk and re-loading it later on.\nAutomatic generation of message passing algorithms, achieved with ReactiveMP.\nSupport for hybrid distinct message passing inference algorithm under a unified paradigm.\nEvaluation of Bethe Free Energy as a model performance measure.\nSchedule-free reactive message passing API.\nScalability for large models with millions of parameters and observations.\nHigh performance.\nInference procedure is differentiable.\nEasy to extend with custom nodes and message update rules.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Curious about how RxInfer compares to other tools you might be considering? We invite you to view a detailed comparison, where we put RxInfer head-to-head with other popular packages in the field.","category":"page"},{"location":"#How-to-get-started?","page":"Home","title":"How to get started?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Head to the Getting started section to get up and running with RxInfer. Alternatively, explore various examples in the documentation.","category":"page"},{"location":"#Table-of-Contents","page":"Home","title":"Table of Contents","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n  \"manuals/comparison.md\",\n  \"manuals/getting-started.md\",\n  \"manuals/model-specification.md\",\n  \"manuals/constraints-specification.md\",\n  \"manuals/meta-specification.md\",\n  \"manuals/inference-execution.md\",\n  \"manuals/custom-node.md\",\n  \"manuals/debugging.md\",\n  \"manuals/delta-node.md\",\n  \"examples/overview.md\",\n  \"library/functional-forms.md\",\n  \"library/bethe-free-energy.md\",\n  \"library/model-construction.md\",\n  \"library/exported-methods.md\",\n  \"contributing/overview.md\",\n  \"contributing/new-example.md\"\n]\nDepth = 2","category":"page"},{"location":"#References","page":"Home","title":"References","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"RxInfer: A Julia package for reactive real-time Bayesian inference - a reference paper for the RxInfer.jl framwork.\nReactive Probabilistic Programming for Scalable Bayesian Inference - a PhD dissertation outlining core ideas and principles behind RxInfer (link2, link3).\nVariational Message Passing and Local Constraint Manipulation in Factor Graphs - describes theoretical aspects of the underlying Bayesian inference method.\nReactive Message Passing for Scalable Bayesian Inference - describes implementation aspects of the Bayesian inference engine and performs benchmarks and accuracy comparison on various models.\nA Julia package for reactive variational Bayesian inference - a reference paper for the ReactiveMP.jl package, the underlying inference engine.\nThe Factor Graph Approach to Model-Based Signal Processing - an introduction to message passing and FFGs.","category":"page"},{"location":"#Ecosystem","page":"Home","title":"Ecosystem","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The RxInfer is a part of the ReactiveBayes ecosystem unites 3 core packages into one powerful reactive message passing-based Bayesian inference framework:","category":"page"},{"location":"","page":"Home","title":"Home","text":"ReactiveMP.jl - core package for efficient and scalable for reactive message passing \nGraphPPL.jl - package for model and constraints specification\nRocket.jl - reactive programming tools","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nReactiveMP.jl engine is a successor of the ForneyLab package. It follows the same ideas and concepts for message-passing based inference, but uses new reactive and efficient message passing implementation under the hood. The API between two packages is different due to a better flexibility, performance and new reactive approach for solving inference problems.","category":"page"},{"location":"","page":"Home","title":"Home","text":"While these packages form the core, RxInfer relies on numerous other excellent open-source packages.  The developers of RxInfer express their deep appreciation to the entire open-source community for their tremendous efforts.","category":"page"},{"location":"#Index","page":"Home","title":"Index","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"examples/advanced_examples/overview/#examples-advanced_examples-overview","page":"Overview","title":"Advanced examples","text":"","category":"section"},{"location":"examples/advanced_examples/overview/","page":"Overview","title":"Overview","text":"This section contains a set of examples for Bayesian Inference with RxInfer package in various probabilistic models.","category":"page"},{"location":"examples/advanced_examples/overview/","page":"Overview","title":"Overview","text":"note: Note\nAll examples have been pre-generated automatically from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/advanced_examples/overview/","page":"Overview","title":"Overview","text":"Advanced examples contain more complex inference problems.","category":"page"},{"location":"examples/advanced_examples/overview/","page":"Overview","title":"Overview","text":"Active Inference Mountain car: This notebooks covers RxInfer usage in the Active Inference setting for the simple mountain car problem.\nAdvanced Tutorial: This notebook covers the fundamentals and advanced usage of the RxInfer.jl package.\nAssessing People’s Skills: The demo is inspired by the example from Chapter 2 of Bishop's Model-Based Machine Learning book. We are going to perform an exact inference to assess the skills of a student given the results of the test.\nChance-Constrained Active Inference: This notebook applies reactive message passing for active inference in the context of chance-constraints.\nConjugate-Computational Variational Message Passing (CVI): This example provides an extensive tutorial for the non-conjugate message-passing based inference by exploiting the local CVI approximation.\nGlobal Parameter Optimisation: This example shows how to use RxInfer.jl automated inference within other optimisation packages such as Optim.jl.\nSolve GP regression by SDE: In this notebook, we solve a GP regression problem by using 'Stochastic Differential Equation' (SDE). This method is well described in the dissertation 'Stochastic differential equation methods for spatio-temporal Gaussian process regression.' by Arno Solin and 'Sequential Inference for Latent Temporal Gaussian Process Models' by Jouni Hartikainen.\nInfinite Data Stream: This example shows RxInfer capabilities of running inference for infinite time-series data.\nNonlinear Sensor Fusion: Nonlinear object position identification using a sparse set of sensors","category":"page"},{"location":"manuals/constraints-specification/#user-guide-constraints-specification","page":"Constraints specification","title":"Constraints Specification","text":"","category":"section"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"RxInfer.jl uses a macro called @constraints from GraphPPL to add extra constraints during the inference process. For details on using the @constraints macro, you can check out the official documentation of GraphPPL.","category":"page"},{"location":"manuals/constraints-specification/#user-guide-constraints-specification-background","page":"Constraints specification","title":"Background and example","text":"","category":"section"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"Here we briefly cover the mathematical aspects of constraints specification. For additional information and relevant links, please refer to the Bethe Free Energy section. In essence, RxInfer performs Variational Inference (via message passing) given specific constraints mathcalQ:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"q^* = argmin_q(s) in mathcalQFq(haty) = mathbbE_q(s)leftlog fracq(s)p(s y=haty) right","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"The @model macro specifies generative model p(s, y) where s is a set of random variables and y is a set of observations. In a nutshell the goal of probabilistic programming is to find p(s|y). RxInfer approximates p(s|y) with a proxy distribution q(x) using KL divergence and Bethe Free Energy optimisation procedure. By default there are no extra factorization constraints on q(s) and the optimal solution is q(s) = p(s|y).","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"For certain problems, it may be necessary to adjust the set of constraints mathcalQ (also known as the variational family of distributions) to either improve accuracy at the expense of computational resources or reduce accuracy to conserve computational resources. Sometimes, we are compelled to impose certain constraints because otherwise, the problem becomes too challenging to solve within a reasonable timeframe.","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"For instance, consider the following model:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"using RxInfer\n\n@model function iid_normal(y)\n    μ  ~ Normal(mean = 0.0, variance = 1.0)\n    τ  ~ Gamma(shape = 1.0, rate = 1.0)\n    y .~ Normal(mean = μ, precision = τ)\nend","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"In this model, we characterize all observations in a dataset y as a Normal distribution with mean μ and precision τ. It's reasonable to assume that the latent variables μ and τ are jointly independent, thereby rendering their joint posterior distribution as:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"q(μ τ) = q(μ)q(τ)","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"If we would write the variational family of distribution for such an assumption, it would be expressed as:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"mathcalQ = left q  q(μ τ) = q(μ)q(τ) right","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"We can express this constraint with the @constraints macro:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"constraints = @constraints begin \n    q(μ, τ) = q(μ)q(τ)\nend","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"and use the created constraints object to the infer function:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"# We need to specify initial marginals, since with the constraints \n# the problem becomes inherently iterative (we could also specify initial for the `μ` instead)\ninit = @initialization begin \n    q(τ) = vague(Gamma)\nend\n\nresult = infer(\n    model       = iid_normal(),\n    # Sample data from mean `3.1415` and precision `2.7182`\n    data        = (y = rand(NormalMeanPrecision(3.1415, 2.7182), 1000), ),\n    constraints = constraints,\n    initialization = init,\n    iterations     = 25\n)","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"println(\"Estimated mean of `μ` is \", mean(result.posteriors[:μ][end]), \" with standard deviation \", std(result.posteriors[:μ][end]))\nprintln(\"Estimated mean of `τ` is \", mean(result.posteriors[:τ][end]), \" with standard deviation \", std(result.posteriors[:τ][end]))","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"We observe that the estimates tend to slightly deviate from what the real values are.  This behavior is a known characteristic of inference with the aforementioned constraints, often referred to as Mean Field constraints.","category":"page"},{"location":"manuals/constraints-specification/#General-syntax","page":"Constraints specification","title":"General syntax","text":"","category":"section"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"You can use the @constraints macro with either a regular Julia function or a single begin ... end block. Both ways are valid, as shown below:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"using RxInfer #hide\n\n# `functional` style\n@constraints function create_my_constraints()\n    q(μ, τ) = q(μ)q(τ)\nend\n\n# `block` style\nmyconstraints = @constraints begin \n    q(μ, τ) = q(μ)q(τ)\nend\n\nnothing #hide","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"The function-based syntax can also take arguments, like this:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints function make_constraints(mean_field)\n    # Specify mean-field only if the flag is `true`\n    if mean_field\n        q(μ, τ) = q(μ)q(τ)\n    end\nend\n\nmyconstraints = make_constraints(true)","category":"page"},{"location":"manuals/constraints-specification/#Marginal-and-messages-form-constraints","page":"Constraints specification","title":"Marginal and messages form constraints","text":"","category":"section"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"To specify marginal or messages form constraints @constraints macro uses :: operator (in somewhat similar way as Julia uses it for multiple dispatch type specification). Read more about available functional form constraints in the Built-In Functional Forms section.","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"As an example, the following constraint:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints begin \n    q(x) :: PointMassFormConstraint()\nend","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"indicates that the resulting marginal of the variable (or array of variables) named x must be approximated with a PointMass object. Message passing based algorithms compute posterior marginals as a normalized product of two colliding messages on corresponding edges of a factor graph. In a few words q(x)::PointMassFormConstraint reads as:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"mathrmapproximate q(x) = fracoverrightarrowmu(x)overleftarrowmu(x)int overrightarrowmu(x)overleftarrowmu(x) mathrmdxmathrmasPointMass","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"Sometimes it might be useful to set a functional form constraint on messages too. For example if it is essential to keep a specific Gaussian parametrisation or if some messages are intractable and need approximation. To set messages form constraint @constraints macro uses μ(...) instead of q(...):","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints begin \n    q(x) :: PointMassFormConstraint()\n    μ(x) :: SampleListFormConstraint(1000)\n    # it is possible to assign different form constraints on the same variable \n    # both for the marginal and for the messages \nend","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints macro understands \"stacked\" form constraints. For example the following form constraint","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints begin \n    q(x) :: SampleListFormConstraint(1000) :: PointMassFormConstraint()\nend","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"indicates that the q(x) first must be approximated with a SampleList and in addition the result of this approximation should be approximated as a PointMass. ","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"note: Note\nNot all combinations of \"stacked\" form constraints are compatible between each other.","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"You can find more information about built-in functional form constraint in the Built-in Functional Forms section. In addition, the ReactiveMP library documentation explains the functional form interfaces and shows how to build a custom functional form constraint that is compatible with RxInfer.jl and ReactiveMP.jl inference engine.","category":"page"},{"location":"manuals/constraints-specification/#Factorization-constraints-on-posterior-distribution-q","page":"Constraints specification","title":"Factorization constraints on posterior distribution q","text":"","category":"section"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"As has been mentioned above, inference may be not tractable for every model without extra factorization constraints. To circumvent this, RxInfer.jl allows for extra factorization constraints, for example:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints begin \n    q(x, y) = q(x)q(y)\nend","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"specifies a so-called mean-field assumption on variables x and y in the model. Furthermore, if x is an array of variables in our model we may induce extra mean-field assumption on x in the following way.","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints begin \n    q(x) = q(x[begin])..q(x[end])\n    q(x, y) = q(x)q(y)\nend","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"These constraints specify a mean-field assumption between variables x and y (either single variable or collection of variables) and additionally specify mean-field assumption on variables x_i.","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"note: Note\n@constraints macro does not support matrix-based collections of variables. E.g. it is not possible to write q(x[begin, begin])..q(x[end, end]). Use q(x[begin])..q(x[end]) instead.","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"Read more about the @constraints macro in the official documentation of GraphPPL","category":"page"},{"location":"manuals/constraints-specification/#Constraints-in-submodels","page":"Constraints specification","title":"Constraints in submodels","text":"","category":"section"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"RxInfer allows you to define your generative model hierarchically, using previously defined @model modules as submodels in larger models. Because of this, users need to specify their constraints hierarchically as well to avoid ambiguities. Consider the following example:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@model function inner_inner(τ, y)\n    y ~ Normal(τ[1], τ[2])\nend\n\n@model function inner(θ, α)\n    β ~ Normal(0, 1)\n    α ~ Gamma(β, 1)\n    α ~ inner_inner(τ = θ)\nend\n\n@model function outer()\n    local w\n    for i = 1:5\n        w[i] ~ inner(θ = Gamma(1, 1))\n    end\n    y ~ inner(θ = w[2:3])\nend","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"To access the variables in the submodels, we use the for q in __submodel__ syntax, which will allow us to specify constraints over variables in the context of an inner submodel:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints begin\n    for q in inner\n        q(α) :: PointMassFormConstraint()\n        q(α, β) = q(α)q(β)\n    end\nend","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"Similarly, we can specify constraints over variables in the context of the innermost submodel by using the for q in __submodel__ syntax twice:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints begin\n    for q in inner\n        for q in inner_inner\n            q(y, τ) = q(y)q(τ[1])q(τ[2])\n        end\n        q(α) :: PointMassFormConstraint()\n        q(α, β) = q(α)q(β)\n    end\nend","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"The for q in __submodel__ applies the constraints specified in this code block to all instances of __submodel__ in the current context. If we want to apply constraints to a specific instance of a submodel, we can use the for q in (__submodel__, __identifier__) syntax, where __identifier__ is a counter integer. For example, if we want to specify constraints on the first instance of inner in our outer model, we can do so with the following syntax:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints begin\n    for q in (inner, 1)\n        q(α) :: PointMassFormConstraint()\n        q(α, β) = q(α)q(β)\n    end\nend","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"Factorization constraints specified in a context propagate to their child submodels. This means that we can specify factorization constraints over variables where the factor node that connects the two are in a submodel, without having to specify the factorization constraint in the submodel itself. For example, if we want to specify a factorization constraint between w[2] and w[3] in our outer model, we can specify it in the context of outer, and RxInfer will recognize that these variables are connected through the Normal node in the inner_inner submodel:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints begin\n    q(w) = q(w[begin])..q(w[end])\nend","category":"page"},{"location":"manuals/constraints-specification/#Default-constraints","page":"Constraints specification","title":"Default constraints","text":"","category":"section"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"Sometimes, a submodel is used in multiple contexts, on multiple levels of hierarchy and in different submodels. In such cases, it becomes cumbersome to specify constraints for each instance of the submodel and track its usage throughout the model. To alleviate this, RxInfer allows users to specify default constraints for a submodel. These constraints will be applied to all instances of the submodel unless overridden by specific constraints. To specify default constraints for a submodel, override the GraphPPL.default_constraints function for the submodel:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"RxInfer.GraphPPL.default_constraints(::typeof(inner)) = @constraints begin\n    q(α) :: PointMassFormConstraint()\n    q(α, β) = q(α)q(β)\nend","category":"page"},{"location":"contributing/new-release/#contributing-new-release","page":"Publishing a new release","title":"Publishing a new release","text":"","category":"section"},{"location":"contributing/new-release/","page":"Publishing a new release","title":"Publishing a new release","text":"Please read first the general Contributing section. Also, please read the FAQ section in the Julia General registry.","category":"page"},{"location":"contributing/new-release/#Start-the-release-process","page":"Publishing a new release","title":"Start the release process","text":"","category":"section"},{"location":"contributing/new-release/","page":"Publishing a new release","title":"Publishing a new release","text":"In order to start the release process a person with the associated permissions should: ","category":"page"},{"location":"contributing/new-release/","page":"Publishing a new release","title":"Publishing a new release","text":"Open a commit page on GitHub\nWrite the @JuliaRegistrator register comment for the commit:","category":"page"},{"location":"contributing/new-release/","page":"Publishing a new release","title":"Publishing a new release","text":"(Image: Release comment)","category":"page"},{"location":"contributing/new-release/","page":"Publishing a new release","title":"Publishing a new release","text":"The Julia Registrator bot should automatically register a request for the new release. Once all checks have passed on the Julia Registrator's side, the new release will be published and tagged automatically.","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/#examples-solve-gp-regression-by-sde","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"","category":"section"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"In this notebook, we solve a GP regression problem by using \"Stochastic Differential Equation\" (SDE). This method is well described in the dissertation \"Stochastic differential equation methods for spatio-temporal Gaussian process regression.\" by Arno Solin and \"Sequential Inference for Latent Temporal Gaussian Process Models\" by Jouni Hartikainen. The idea of the method is as follows.","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"Suppose a function f(x) follows a zero-mean Gaussian Process beginaligned f(x) sim mathcalGP(0 k(xx)) endaligned","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"When the dimensionality of x is 1, we can consider f(x) as a stochastic process over time, i.e. f(t). For a certain classses of covariance functions, f(t) is a solution to an m-th order linear stochastic differential equation (SDE) beginaligned a_0 f(t) + a_1 fracd f(t)dt + dots + a_m fracd^m f(t)dt^m = w(t)  endaligned","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"where w(t) is a zero-mean white noise process with spectral density Q_c. If we define a vector-valued function mathbff(t) = (f(t) ddt f(t)dots d^m-1dt^m-1f(t)), then we can rewrite the above SDE under the companion form","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"beginaligned\nfracd mathbff(t)dt = mathbfF mathbff(t) + mathbfL w(t) quad (1)\nendaligned","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"where mathbfF and mathbfL are defined based on the choice of covariance functions.  From (1), we have the following state-space model: beginaligned mathbff_k = mathbfA_k-1  mathbff_k-1 + mathbfq_k-1 quad mathbfq_k-1 sim mathcalN(mathbf0 mathbfQ_k-1) quad(2a) \ny_k = mathbfH  mathbff(t_k) + epsilon_k  quad epsilon_k sim mathcalN(0 sigma^2_noise) quad(2b) \nendaligned","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"where mathbfA_k = exp(mathbfFDelta t_k), with Delta t_k = t_k+1 - t_k, is called the discrete-time state transition matrix, and mathbfQ_k the process noise covariance matrix. For the computation of mathbfQ_k, we will come back later. According to Arno Solin and Jouni Hartikainen's dissertation, the GP regression problem amounts to the inference problem of the above state-space model, and this can be solved by RTS-smoothing. The state-space model starts from  the initial state f_0 sim mathcalN(mathbf0 mathbfP_0). For stationary covariance function, the SDE has a stationary state f_infty sim mathcalN(mathbf0 mathbfP_infty), where mathbfP_infty is the solution to beginaligned fracdmathbfP_inftydt = mathbfF mathbfP_infty + mathbfP_infty mathbfF^T + mathbfL mathbfQ_c mathbfL^T = 0 quad (mathrmLyapunov  equation) endaligned","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"With this stationary condition, the process noise covariance mathbfQ_k is computed as follows beginaligned mathbfQ_k = mathbfP_infty - mathbfA_k mathbfP_infty mathbfA_k^T  endaligned","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"For one-dimensional problem the SDE representation of the GP is defined by the matrices mathbfF  mathbfL  mathbfQ_c  mathbfP_0 and mathbfH. Once we obtain all the matrices, we can do GP regression by implementing RTS-smoothing on the state-space model (2). In this notebook we will particularly use the Matern class of covariance functions for Gaussian Process.","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"using RxInfer, Random, Distributions, LinearAlgebra, Plots","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/#Create-state-space-model-for-GP-regression","page":"Solve GP regression by SDE","title":"Create state space model for GP regression","text":"","category":"section"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"Here we create a state-space model beginaligned mathbff_k = mathbfA_k-1  mathbff_k-1 + mathbfq_k-1 quad mathbfq_k-1 sim mathcalN(mathbf0 mathbfQ_k-1) \ny_k = mathbfH  mathbff(t_k) + epsilon_k  quad epsilon_k sim mathcalN(0 sigma^2_noise) \nendaligned where y_k is the noisy observation of the function f at time t_k, and sigma^2_noise is the noise variance and assumed to be known.","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"@model function gp_regression(y, P, A, Q, H, var_noise)\n    f_prev ~ MvNormal(μ = zeros(length(H)), Σ = P) #initial state\n    for i in eachindex(y)\n        f[i] ~ MvNormal(μ = A[i] * f_prev,Σ = Q[i])\n        y[i] ~ Normal(μ = dot(H , f[i]), var = var_noise)\n        f_prev = f[i]\n    end\nend","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/#Generate-data","page":"Solve GP regression by SDE","title":"Generate data","text":"","category":"section"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"Random.seed!(10)\nn = 100\nσ²_noise = 0.04;\nt = collect(range(-2, 2, length=n)); #timeline\nf_true = sinc.(t); # true process\nf_noisy = f_true + sqrt(σ²_noise)*randn(n); #noisy process\n\npos = sort(randperm(75)[1:2:75]); \nt_obser = t[pos]; # time where we observe data\n\ny_data = Array{Union{Float64,Missing}}(missing, n)\nfor i in pos \n    y_data[i] = f_noisy[i]\nend\n\nθ = [1., 1.]; # store [l, σ²]\nΔt = [t[1]]; # time difference\nappend!(Δt, t[2:end] - t[1:end-1]);","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/#Let's-visualize-our-data","page":"Solve GP regression by SDE","title":"Let's visualize our data","text":"","category":"section"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"plot(t, f_true, label=\"True process f(t)\")\nscatter!(t_obser, y_data[pos], label = \"Noisy observations\")\nxlabel!(\"t\")\nylabel!(\"f(t)\")","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/#Covariance-function:-Matern-3/2","page":"Solve GP regression by SDE","title":"Covariance function: Matern-3/2","text":"","category":"section"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"The Matern is a stationary covariance function and defined as follows beginaligned k(tau) = sigma^2 frac2^1-nuGamma(nu) left(fracsqrt2nutaul right)^nu K_nuleft(fracsqrt2nutaul right) endaligned where  beginaligned sigma^2 textthe magnitude scale hyperparameter\nl textthe characteristic length-scale\nnu textthe smoothness hyperparameter\nK_nu() textthe modified Bessel function of the second kind endaligned When we say the Matern-3/2, we mean nu=32. The matrices for the state space model are computed as follows beginaligned mathbfF = beginpmatrix 0  1\n-lambda^2  -2lambda endpmatrix quad quad mathbfL = beginpmatrix 0  1 endpmatrix quad quad mathbfP_infty = beginpmatrix sigma^2  0  0  lambda^2sigma^2 endpmatrix quad quad mathbfH = beginpmatrix 1  0 endpmatrix quad quad Q_c = 4lambda^3sigma^2 endaligned  where lambda = fracsqrt3l  From these matrices we can define mathbfA_k and mathbfQ_k.","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"λ = sqrt(3)/θ[1];\n#### compute matrices for the state-space model ######\nL = [0., 1.];\nH = [1., 0.];\nF = [0. 1.; -λ^2 -2λ]\nP∞ = [θ[2] 0.; 0. (λ^2*θ[2]) ]\nA = [exp(F * i) for i in Δt]; \nQ = [P∞ - i*P∞*i' for i in A];","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"result_32 = infer(\n    model = gp_regression(P = P∞, A = A, Q = Q, H = H, var_noise = σ²_noise),\n    data = (y = y_data,)\n)","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"Inference results:\n  Posteriors       | available for (f, f_prev)\n  Predictions      | available for (y)","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/#Covariance-function:-Matern-5/2","page":"Solve GP regression by SDE","title":"Covariance function: Matern-5/2","text":"","category":"section"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"Now let's try the Matern-5/2 kernel. The matrices for the SDE representation of the Matern-5/2 are:","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"beginaligned\nmathbfF = beginpmatrix\n0  1  0\n0  0  1 \n-lambda^3  -3lambda^2  -3lambda\nendpmatrix quad quad mathbfL = beginpmatrix\n0  0  1\nendpmatrix quad quad mathbfH = beginpmatrix\n1  0  0\nendpmatrix quad quad Q_c = frac163 sigma^2 lambda^5 \nendaligned","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"where lambda = sqrt5  l. To find mathbfP_infty, we solve the Lyapunov equation","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"beginaligned\nfracdmathbfP_inftydt = mathbfF mathbfP_infty + mathbfP_infty mathbfF^T + mathbfL mathbfQ_c mathbfL^T = 0\nendaligned","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"of which the solution is","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"beginaligned\nvec(mathbfP_infty) = (mathbfI otimes mathbfF + mathbfFotimesmathbfI)^-1 vec(-mathbfLQ_cmathbfL^T)\nendaligned","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"where vec() is the vectorization operator and otimes denotes the Kronecker product. Now we can find mathbfA_k and mathbfQ_k ","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"beginaligned\nmathbfA_k = exp(mathbfFDelta t_k) \nendaligned","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"beginaligned\nmathbfQ_k = mathbfP_infty - mathbfA_k mathbfP_infty mathbfA_k^T  \nendaligned","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"λ = sqrt(5)/θ[1];\n#### compute matrices for the state-space model ######\nL = [0., 0., 1.];\nH = [1., 0., 0.];\nF = [0. 1. 0.; 0. 0. 1.;-λ^3 -3λ^2 -3λ]\nQc = 16/3 * θ[2] * λ^5;\n\nI = diageye(3) ; \nvec_P = inv(kron(I,F) + kron(F,I)) * vec(-L * Qc * L'); \nP∞ = reshape(vec_P,3,3);\nA = [exp(F * i) for i in Δt]; \nQ = [P∞ - i*P∞*i' for i in A];","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"result_52 = infer(\n    model = gp_regression(P = P∞, A = A, Q = Q, H = H, var_noise = σ²_noise),\n    data = (y = y_data,)\n)","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"Inference results:\n  Posteriors       | available for (f, f_prev)\n  Predictions      | available for (y)","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/#Result","page":"Solve GP regression by SDE","title":"Result","text":"","category":"section"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"slicedim(dim) = (a) -> map(e -> e[dim], a)\n\nplot(t, mean.(result_32.posteriors[:f]) |> slicedim(1), ribbon = var.(result_32.posteriors[:f]) |> slicedim(1) .|> sqrt, label =\"Approx. process_M32\", title = \"Matern-3/2\", legend =false, lw = 2)\nplot!(t, mean.(result_52.posteriors[:f]) |> slicedim(1), ribbon = var.(result_52.posteriors[:f]) |> slicedim(1) .|> sqrt, label =\"Approx. process_M52\",legend = :bottomleft, title = \"GPRegression by SSM\", lw = 2)\nplot!(t, f_true,label=\"true process\", lw = 2)\nscatter!(t_obser, f_noisy[pos], label=\"Observations\")\nxlabel!(\"t\")\nylabel!(\"f(t)\")","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"As we can see from the plot, both cases of Matern kernel provide good approximations (small variance) to the true process at the area with dense observations (namely from t = 0 to around 3.5), and when we move far away from this region the approximated processes become less accurate (larger variance). This result makes sense because GP regression exploits the correlation between observations to predict unobserved points, and the choice of covariance functions as well as their hyperparameters might not be optimal. We can increase the accuracy of the approximated processes by simply adding more observations. This way of improvement does not trouble the state-space method much but it might cause computational problem for naive GP regression, because with N observations the complexity of naive GP regression scales with N^3 while the state-space method scales linearly with N.     ","category":"page"},{"location":"contributing/new-documentation/#guide-docs-contributing","page":"Contributing to the documentation","title":"Contributing to the documentation","text":"","category":"section"},{"location":"contributing/new-documentation/","page":"Contributing to the documentation","title":"Contributing to the documentation","text":"Contributing to our documentation is a valuable way to enhance the RxInfer ecosystem. To get started, you can follow these steps:","category":"page"},{"location":"contributing/new-documentation/","page":"Contributing to the documentation","title":"Contributing to the documentation","text":"Familiarize Yourself: First, take some time to explore our existing documentation. Understand the structure, style, and content to align your contributions with our standards.\nIdentify Needs: Identify areas that require improvement, clarification, or expansion. These could be missing explanations, code examples, or outdated information.\nFork the Repository: Fork our documentation repository on GitHub to create your own copy. This allows you to work on your changes independently.\nMake Your Edits: Create or modify content in your forked repository. Ensure your contributions are clear, concise, and well-structured.\nSubmit a Pull Request: When you're satisfied with your changes, submit a pull request (PR) to our main repository. Describe your changes in detail in the PR description.\nReview and Feedback: Our documentation maintainers will review your PR. They may provide feedback or request adjustments. Be responsive to this feedback to facilitate the merging process.\nMerging: Once your changes align with our documentation standards, they will be merged into the main documentation. Congratulations, you've successfully contributed to the RxInfer ecosystem!","category":"page"},{"location":"contributing/new-documentation/","page":"Contributing to the documentation","title":"Contributing to the documentation","text":"By following these steps, you can play an essential role in improving and expanding our documentation, making it more accessible and valuable to the RxInfer community.","category":"page"},{"location":"contributing/new-documentation/#Use-[LiveServer.jl](https://github.com/tlienart/LiveServer.jl)","page":"Contributing to the documentation","title":"Use LiveServer.jl","text":"","category":"section"},{"location":"contributing/new-documentation/","page":"Contributing to the documentation","title":"Contributing to the documentation","text":"LiveServer.jl is a simple and lightweight web server developed in Julia. It features live-reload capabilities, making it a valuable tool for automatically refreshing the documentation of a package while you work on its content.","category":"page"},{"location":"contributing/new-documentation/","page":"Contributing to the documentation","title":"Contributing to the documentation","text":"To use LiveServer.jl, simply follow these steps[1]","category":"page"},{"location":"contributing/new-documentation/","page":"Contributing to the documentation","title":"Contributing to the documentation","text":"[1]: Make sure to install the LiveServer and Documenter in your current working environment.","category":"page"},{"location":"contributing/new-documentation/","page":"Contributing to the documentation","title":"Contributing to the documentation","text":"Make sure to import the required packages ","category":"page"},{"location":"contributing/new-documentation/","page":"Contributing to the documentation","title":"Contributing to the documentation","text":"julia> using LiveServer, Documenter","category":"page"},{"location":"contributing/new-documentation/","page":"Contributing to the documentation","title":"Contributing to the documentation","text":"After importing the required packages, you can start the live server with the following command:","category":"page"},{"location":"contributing/new-documentation/","page":"Contributing to the documentation","title":"Contributing to the documentation","text":"julia> servedocs()","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#examples-bayesian-linear-regression-tutorial","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"This notebook is an extensive tutorial on Bayesian linear regression with RxInfer and consists of two major parts:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"The first part uses a regular Bayesian Linear Regression on a simple application of fuel consumption for a car with synthetic data.\nThe second part is an adaptation of a tutorial from NumPyro and uses Hierarchical Bayesian linear regression on the OSIC pulmonary fibrosis progression dataset from Kaggle.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"using RxInfer, Random, Plots, StableRNGs, LinearAlgebra, StatsPlots, LaTeXStrings, DataFrames, CSV, GLM","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Part-1.-Bayesian-Linear-Regression","page":"Bayesian Linear Regression Tutorial","title":"Part 1. Bayesian Linear Regression","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"John recently purchased a new car and is interested in its fuel consumption rate. He believes that this rate has a linear relationship with speed, and as such, he wants to conduct tests by driving his car on different types of roads, recording both the fuel usage and speed. In order to determine the fuel consumption rate, John employs Bayesian linear regression.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Univariate-regression-with-known-noise","page":"Bayesian Linear Regression Tutorial","title":"Univariate regression with known noise","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"First, he drives the car on a urban road. John enjoys driving on the well-built, wide, and flat urban roads. Urban roads also offer the advantage of precise fuel consumption measurement with minimal noise. Therefore John models the fuel consumption y_ninmathbbR as a normal distribution and treats x_n as a fixed hyperparameter:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"beginaligned\np(y_n mid a b) = mathcalN(y_n mid a x_n + b  1)\nendaligned","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"The recorded speed is denoted as x_n in mathbbR and the recorded fuel consumption as y_n in mathbbR. Prior beliefs on a and b are informed by the vehicle manual.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"beginaligned\n    p(a) = mathcalN(a mid m_a v_a) \n    p(b) = mathcalN(b mid m_b v_b) \nendaligned","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Together they form the probabilistic model p(y a b) = p(a)p(b) prod_N=1^N p(y_n mid a b) where the goal is to infer the posterior distributions p(a mid y) and p(bmid y).","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"He records the speed and fuel consumption for the urban road which is the xdata and ydata.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"function generate_data(a, b, v, nr_samples; rng=StableRNG(1234))\n    x = float.(collect(1:nr_samples))\n    y = a .* x .+ b .+ randn(rng, nr_samples) .* sqrt(v)\n    return x, y\nend;","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"x_data, y_data = generate_data(0.5, 25.0, 1.0, 250)\n\nscatter(x_data, y_data, title = \"Dataset (City road)\", legend=false)\nxlabel!(\"Speed\")\nylabel!(\"Fuel consumption\")","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"In order to estimate the two parameters with the recorded data, he uses a RxInfer.jl to create the above described model.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"@model function linear_regression(x, y)\n    a ~ Normal(mean = 0.0, variance = 1.0)\n    b ~ Normal(mean = 0.0, variance = 100.0)    \n    y .~ Normal(mean = a .* x .+ b, variance = 1.0)\nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"He is delighted that he can utilize the inference function from this package, saving him the effort of starting from scratch and enabling him to obtain the desired results for this road. He does note that there is a loop in his model, namely all a and b variables are connected over all observations, therefore he needs to initialize one of the messages and run multiple iterations for the loopy belief propagation algorithm. It is worth noting that loopy belief propagation is not guaranteed to converge in general and might be highly influenced by the choice of the initial messages in the initialization argument. He is going to evaluate the convergency performance of the algorithm with the free_energy = true option:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"results = infer(\n    model          = linear_regression(), \n    data           = (y = y_data, x = x_data), \n    initialization = @initialization(μ(b) = NormalMeanVariance(0.0, 100.0)), \n    returnvars     = (a = KeepLast(), b = KeepLast()),\n    iterations     = 20,\n    free_energy    = true\n)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Inference results:\n  Posteriors       | available for (a, b)\n  Free Energy:     | Real[450.062, 8526.84, 4960.42, 2949.02, 1819.14, 1184\n.44, 827.897, 627.595, 515.064, 451.839, 416.313, 396.349, 385.129, 378.821\n, 375.274, 373.279, 372.156, 371.524, 371.167, 370.966]","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"He knows the theoretical coefficients and noise for this car from the manual. He is going to compare the experimental solution with theoretical results.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"pra = plot(range(-3, 3, length = 1000), (x) -> pdf(NormalMeanVariance(0.0, 1.0), x), title=L\"Prior for $a$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$p(a)$\", c=1,)\npra = vline!(pra, [ 0.5 ], label=L\"True $a$\", c = 3)\npsa = plot(range(0.45, 0.55, length = 1000), (x) -> pdf(results.posteriors[:a], x), title=L\"Posterior for $a$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$p(a\\mid y)$\", c=2,)\npsa = vline!(psa, [ 0.5 ], label=L\"True $a$\", c = 3)\n\nplot(pra, psa, size = (1000, 200), xlabel=L\"$a$\", ylabel=L\"$p(a)$\", ylims=[0,Inf])","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"prb = plot(range(-40, 40, length = 1000), (x) -> pdf(NormalMeanVariance(0.0, 100.0), x), title=L\"Prior for $b$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"p(b)\", c=1, legend = :topleft)\nprb = vline!(prb, [ 25 ], label=L\"True $b$\", c = 3)\npsb = plot(range(23, 28, length = 1000), (x) -> pdf(results.posteriors[:b], x), title=L\"Posterior for $b$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"p(b\\mid y)\", c=2, legend = :topleft)\npsb = vline!(psb, [ 25 ], label=L\"True $b$\", c = 3)\n\nplot(prb, psb, size = (1000, 200), xlabel=L\"$b$\", ylabel=L\"$p(b)$\", ylims=[0, Inf])","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"a = results.posteriors[:a]\nb = results.posteriors[:b]\n\nprintln(\"Real a: \", 0.5, \" | Estimated a: \", mean_var(a), \" | Error: \", abs(mean(a) - 0.5))\nprintln(\"Real b: \", 25.0, \" | Estimated b: \", mean_var(b), \" | Error: \", abs(mean(b) - 25.0))","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Real a: 0.5 | Estimated a: (0.501490188462706, 1.9162284531300301e-7) | Err\nor: 0.001490188462705988\nReal b: 25.0 | Estimated b: (24.81264210195605, 0.0040159675312827) | Error\n: 0.18735789804394898","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Based on the Bethe free energy below, John knows that the loopy belief propagation has actually converged after 20 iterations:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"# drop first iteration, which is influenced by the `initmessages`\nplot(2:20, results.free_energy[2:end], title=\"Free energy\", xlabel=\"Iteration\", ylabel=\"Free energy [nats]\", legend=false)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Univariate-regression-with-unknown-noise","page":"Bayesian Linear Regression Tutorial","title":"Univariate regression with unknown noise","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Afterwards, he plans to test the car on a mountain road. However, mountain roads are typically narrow and filled with small stones, which makes it more difficult to establish a clear relationship between fuel consumption and speed, leading to an unknown level of noise in the regression model. Therefore, he design a model with unknown Inverse-Gamma distribution on the variance. beginaligned p(y_n mid a b s) = mathcalN(y_n mid ax_n + b s)\np(s) = mathcalIG(smidalpha theta)\np(a) = mathcalN(a mid m_a v_a) \np(b) = mathcalN(b mid m_b v_b)  endaligned","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"@model function linear_regression_unknown_noise(x, y)\n    a ~ Normal(mean = 0.0, variance = 1.0)\n    b ~ Normal(mean = 0.0, variance = 100.0)\n    s ~ InverseGamma(1.0, 1.0)\n    y .~ Normal(mean = a .* x .+ b, variance = s)\nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"x_data_un, y_data_un = generate_data(0.5, 25.0, 400.0, 250)\n\nscatter(x_data_un, y_data_un, title = \"Dateset with unknown noise (mountain road)\", legend=false)\nxlabel!(\"Speed\")\nylabel!(\"Fuel consumption\")","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"To solve this problem in closed-from we need to resort to a variational approximation. The procedure will be a combination of variational inference and loopy belief propagation. He chooses constraints = MeanField() as a global variational approximation and provides initial marginals with the initialization argument. He is, again, going to evaluate the convergency performance of the algorithm with the free_energy = true option:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"init_unknown_noise = @initialization begin \n    μ(b) = NormalMeanVariance(0.0, 100.0)\n    q(s) = vague(InverseGamma)\nend\n\nresults_unknown_noise = infer(\n    model           = linear_regression_unknown_noise(), \n    data            = (y = y_data_un, x = x_data_un), \n    initialization  = init_unknown_noise, \n    returnvars      = (a = KeepLast(), b = KeepLast(), s = KeepLast()), \n    iterations      = 20,\n    constraints     = MeanField(),\n    free_energy     = true\n)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Inference results:\n  Posteriors       | available for (a, b, s)\n  Free Energy:     | Real[1657.49, 1192.08, 1142.31, 1135.43, 1129.19, 1125\n.47, 1123.34, 1122.13, 1121.44, 1121.05, 1120.82, 1120.69, 1120.61, 1120.56\n, 1120.53, 1120.52, 1120.5, 1120.5, 1120.49, 1120.49]","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Based on the Bethe free energy below, John knows that his algorithm has converged after 20 iterations:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"plot(results_unknown_noise.free_energy, title=\"Free energy\", xlabel=\"Iteration\", ylabel=\"Free energy [nats]\", legend=false)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Below he visualizes the obtained posterior distributions for parameters:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"pra = plot(range(-3, 3, length = 1000), (x) -> pdf(NormalMeanVariance(0.0, 1.0), x), title=L\"Prior for $a$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$p(a)$\", c=1,)\npra = vline!(pra, [ 0.5 ], label=L\"True $a$\", c = 3)\npsa = plot(range(0.45, 0.55, length = 1000), (x) -> pdf(results_unknown_noise.posteriors[:a], x), title=L\"Posterior for $a$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$q(a)$\", c=2,)\npsa = vline!(psa, [ 0.5 ], label=L\"True $a$\", c = 3)\n\nplot(pra, psa, size = (1000, 200), xlabel=L\"$a$\", ylabel=L\"$p(a)$\", ylims=[0, Inf])","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"prb = plot(range(-40, 40, length = 1000), (x) -> pdf(NormalMeanVariance(0.0, 100.0), x), title=L\"Prior for $b$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$p(b)$\", c=1, legend = :topleft)\nprb = vline!(prb, [ 25.0 ], label=L\"True $b$\", c = 3)\npsb = plot(range(23, 28, length = 1000), (x) -> pdf(results_unknown_noise.posteriors[:b], x), title=L\"Posterior for $b$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$q(b)$\", c=2, legend = :topleft)\npsb = vline!(psb, [ 25.0 ], label=L\"True $b$\", c = 3)\n\nplot(prb, psb, size = (1000, 200), xlabel=L\"$b$\", ylabel=L\"$p(b)$\", ylims=[0, Inf])","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"prb = plot(range(0.001, 400, length = 1000), (x) -> pdf(InverseGamma(1.0, 1.0), x), title=L\"Prior for $s$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$p(s)$\", c=1, legend = :topleft)\nprb = vline!(prb, [ 200 ], label=L\"True $s$\", c = 3)\npsb = plot(range(0.001, 400, length = 1000), (x) -> pdf(results_unknown_noise.posteriors[:s], x), title=L\"Posterior for $s$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$q(s)$\", c=2, legend = :topleft)\npsb = vline!(psb, [ 200 ], label=L\"True $s$\", c = 3)\n\nplot(prb, psb, size = (1000, 200), xlabel=L\"$s$\", ylabel=L\"$p(s)$\", ylims=[0, Inf])","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"He sees that in the presence of more noise the inference result is more uncertain about the actual values for a and b parameters.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"John samples a and b and plot many possible regression lines on the same plot:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"as = rand(results_unknown_noise.posteriors[:a], 100)\nbs = rand(results_unknown_noise.posteriors[:b], 100)\np = scatter(x_data_un, y_data_un, title = \"Linear regression with more noise\", legend=false)\nxlabel!(\"Speed\")\nylabel!(\"Fuel consumption\")\nfor (a, b) in zip(as, bs)\n    global p = plot!(p, x_data_un, a .* x_data_un .+ b, alpha = 0.05, color = :red)\nend\n\nplot(p, size = (900, 400))","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"From this plot John can see that many lines do fit the data well and there is no definite \"best\" answer to the regression coefficients. He realize that most of these lines, however, resemble a similar angle and shift.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Multivariate-linear-regression","page":"Bayesian Linear Regression Tutorial","title":"Multivariate linear regression","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"In addition to fuel consumption, he is also interested in evaluating the car's power performance, braking performance, handling stability, smoothness, and other factors. To investigate the car's performance, he includes additional measurements. Essentially, this approach involves performing multiple linear regression tasks simultaneously, using multiple data vectors for x and y with different levels of noise. As in the previous example, he assumes the level of noise to be unknown.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"@model function linear_regression_multivariate(dim, x, y)\n    a ~ MvNormal(mean = zeros(dim), covariance = 100 * diageye(dim))\n    b ~ MvNormal(mean = ones(dim), covariance = 100 * diageye(dim))\n    W ~ InverseWishart(dim + 2, 100 * diageye(dim))\n    y .~ MvNormal(mean = x .* a .+ b, covariance = W)\nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"After received all the measurement records, he plots the measurements and performance index:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"dim_mv = 6\nnr_samples_mv = 50\nrng_mv = StableRNG(42)\na_mv = randn(rng_mv, dim_mv)\nb_mv = 10 * randn(rng_mv, dim_mv)\nv_mv = 100 * rand(rng_mv, dim_mv)\n\nx_data_mv, y_data_mv = collect(zip(generate_data.(a_mv, b_mv, v_mv, nr_samples_mv)...));","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"p = plot(title = \"Multivariate linear regression\", legend = :topleft)\n\nplt = palette(:tab10)\n\ndata_set_label = [\"\"]\n\nfor k in 1:dim_mv\n    global p = scatter!(p, x_data_mv[k], y_data_mv[k], label = \"Measurement #$k\", ms = 2, color = plt[k])\nend\nxlabel!(L\"$x$\")\nylabel!(L\"$y$\")\np","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Before this data can be used to perform inference, John needs to change its format slightly.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"x_data_mv_processed = map(i -> Diagonal([getindex.(x_data_mv, i)...]), 1:nr_samples_mv)\ny_data_mv_processed = map(i -> [getindex.(y_data_mv, i)...], 1:nr_samples_mv);","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"init = @initialization begin \n    q(W) = InverseWishart(dim_mv + 2, 10 * diageye(dim_mv))\n    μ(b) = MvNormalMeanCovariance(ones(dim_mv), 10 * diageye(dim_mv))\nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Initial state: \n  q(W) = InverseWishart{Float64, PDMats.PDMat{Float64, Matrix{Float64}}}(\ndf: 8.0\nΨ: [10.0 0.0 … 0.0 0.0; 0.0 10.0 … 0.0 0.0; … ; 0.0 0.0 … 10.0 0.0; 0.0 0.0\n … 0.0 10.0]\n)\n\n  μ(b) = MvNormalMeanCovariance(\nμ: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\nΣ: [10.0 0.0 … 0.0 0.0; 0.0 10.0 … 0.0 0.0; … ; 0.0 0.0 … 10.0 0.0; 0.0 0.0\n … 0.0 10.0]\n)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"results_mv = infer(\n    model           = linear_regression_multivariate(dim = dim_mv),\n    data            = (y = y_data_mv_processed, x = x_data_mv_processed),\n    initialization  = init,\n    returnvars      = (a = KeepLast(), b = KeepLast(), W = KeepLast()),\n    free_energy     = true,\n    iterations      = 50,\n    constraints     = MeanField()\n)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Inference results:\n  Posteriors       | available for (a, b, W)\n  Free Energy:     | Real[864.485, 789.026, 769.094, 750.865, 737.67, 724.7\n22, 712.341, 700.865, 690.782, 682.505  …  664.434, 664.434, 664.434, 664.4\n34, 664.434, 664.434, 664.434, 664.434, 664.434, 664.434]","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Again, the algorithm nicely converged, because the Bethe free energy reached a plateau. John also draws the results for the linear regression parameters and sees that the lines very nicely follow the provided data.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"p = plot(title = \"Multivariate linear regression\", legend = :topleft, xlabel=L\"$x$\", ylabel=L\"$y$\")\n\n# how many lines to plot\nr = 50\n\ni_a = collect.(eachcol(rand(results_mv.posteriors[:a], r)))\ni_b = collect.(eachcol(rand(results_mv.posteriors[:b], r)))\n\nplt = palette(:tab10)\n\nfor k in 1:dim_mv\n    x_mv_k = x_data_mv[k]\n    y_mv_k = y_data_mv[k]\n\n    for i in 1:r\n        global p = plot!(p, x_mv_k, x_mv_k .* i_a[i][k] .+ i_b[i][k], label = nothing, alpha = 0.05, color = plt[k])\n    end\n\n    global p = scatter!(p, x_mv_k, y_mv_k, label = \"Measurement #$k\", ms = 2, color = plt[k])\nend\n\n# truncate the init step\nf = plot(results_mv.free_energy[2:end], title =\"Bethe free energy convergence\", label = nothing, xlabel = \"Iteration\", ylabel = \"Bethe free energy [nats]\") \n\nplot(p, f, size = (1000, 400))","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"He needs more iterations to converge in comparison to the very first example, but that is expected since the problem became multivariate and, hence, more difficult.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"i_a_mv = results_mv.posteriors[:a]\n\nps_a = []\n\nfor k in 1:dim_mv\n    \n    local _p = plot(title = L\"Estimated $a_{%$k}$\", xlabel=L\"$a_{%$k}$\", ylabel=L\"$p(a_{%$k})$\", xlims = (-1.5,1.5), xticks=[-1.5, 0, 1.5], ylims=[0, Inf])\n\n    local m_a_mv_k = mean(i_a_mv)[k]\n    local v_a_mv_k = std(i_a_mv)[k, k]\n    \n    _p = plot!(_p, Normal(m_a_mv_k, v_a_mv_k), fillalpha=0.3, fillrange = 0, label=L\"$q(a_{%$k})$\", c=2,)\n    _p = vline!(_p, [ a_mv[k] ], label=L\"True $a_{%$k}$\", c = 3)\n           \n    push!(ps_a, _p)\nend\n\nplot(ps_a...)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"i_b_mv = results_mv.posteriors[:b]\n\nps_b = []\n\nfor k in 1:dim_mv\n    \n    local _p = plot(title = L\"Estimated $b_{%$k}$\", xlabel=L\"$b_{%$k}$\", ylabel=L\"$p(b_{%$k})$\", xlims = (-20,20), xticks=[-20, 0, 20], ylims =[0, Inf])\n    local m_b_mv_k = mean(i_b_mv)[k]\n    local v_b_mv_k = std(i_b_mv)[k, k]\n\n    _p = plot!(_p, Normal(m_b_mv_k, v_b_mv_k), fillalpha=0.3, fillrange = 0, label=L\"$q(b_{%$k})$\", c=2,)\n    _p = vline!(_p, [ b_mv[k] ], label=L\"Real $b_{%$k}$\", c = 3)\n           \n    push!(ps_b, _p)\nend\n\nplot(ps_b...)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"He also checks the noise estimation procedure and sees that the noise variance are currently a bit underestimated. Note here that he neglects the covariance terms between the individual elements, which might result in this kind of behaviour.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"scatter(1:dim_mv, v_mv, ylims=(0, 100), label=L\"True $s_d$\")\nscatter!(1:dim_mv, diag(mean(results_mv.posteriors[:W])); yerror=sqrt.(diag(var(results_mv.posteriors[:W]))), label=L\"$\\mathrm{E}[s_d] \\pm \\sigma$\")\nplot!(; xlabel=L\"Dimension $d$\", ylabel=\"Variance\", title=\"Estimated variance of the noise\")","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Part-2.-Hierarchical-Bayesian-Linear-Regression","page":"Bayesian Linear Regression Tutorial","title":"Part 2. Hierarchical Bayesian Linear Regression","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Disclaimer The tutorial below is an adaptation of the Bayesian Hierarchical Linear Regression tutorial implemented in NumPyro. ","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"The original author in NumPyro is Carlos Souza. Updated by Chris Stoafer in NumPyro. Adapted to RxInfer by Dmitry Bagaev.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Probabilistic Machine Learning models can not only make predictions about future data but also model uncertainty. In areas such as personalized medicine, there might be a large amount of data, but there is still a relatively small amount available for each patient. To customize predictions for each person, it becomes necessary to build a model for each individual — considering its inherent uncertainties — and then couple these models together in a hierarchy so that information can be borrowed from other similar individuals [1].","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"The purpose of this tutorial is to demonstrate how to implement a Bayesian Hierarchical Linear Regression model using RxInfer. To provide motivation for the tutorial, I will use the OSIC Pulmonary Fibrosis Progression competition, hosted on Kaggle.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"# https://www.machinelearningplus.com/linear-regression-in-julia/\n# https://nbviewer.org/github/pyro-ppl/numpyro/blob/master/notebooks/source/bayesian_hierarchical_linear_regression.ipynb","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Understanding-the-Task","page":"Bayesian Linear Regression Tutorial","title":"Understanding the Task","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Pulmonary fibrosis is a disorder characterized by scarring of the lungs, and its cause and cure are currently unknown. In this competition, the objective was to predict the severity of decline in lung function for patients. Lung function is assessed based on the output from a spirometer, which measures the forced vital capacity (FVC), representing the volume of air exhaled.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"In medical applications, it is valuable to evaluate a model's confidence in its decisions. As a result, the metric used to rank the teams was designed to reflect both the accuracy and certainty of each prediction. This metric is a modified version of the Laplace Log Likelihood (further details will be provided later).","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Now, let's explore the data and dig deeper into the problem involved.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"dataset = CSV.read(\"../data/hbr/osic_pulmonary_fibrosis.csv\", DataFrame);","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"describe(dataset)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"7×7 DataFrame\n Row │ variable       mean     min                        median   max     \n    ⋯\n     │ Symbol         Union…   Any                        Union…   Any     \n    ⋯\n─────┼─────────────────────────────────────────────────────────────────────\n─────\n   1 │ Patient                 ID00007637202177411956430           ID004266\n372 ⋯\n   2 │ Weeks          31.8618  -5                         28.0     133\n   3 │ FVC            2690.48  827                        2641.0   6399\n   4 │ Percent        77.6727  28.8776                    75.6769  153.145\n   5 │ Age            67.1885  49                         68.0     88      \n    ⋯\n   6 │ Sex                     Female                              Male\n   7 │ SmokingStatus           Currently smokes                    Never sm\noke\n                                                               3 columns om\nitted","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"first(dataset, 5)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"5×7 DataFrame\n Row │ Patient                    Weeks  FVC    Percent  Age    Sex      Sm\noki ⋯\n     │ String31                   Int64  Int64  Float64  Int64  String7  St\nrin ⋯\n─────┼─────────────────────────────────────────────────────────────────────\n─────\n   1 │ ID00007637202177411956430     -4   2315  58.2536     79  Male     Ex\n-sm ⋯\n   2 │ ID00007637202177411956430      5   2214  55.7121     79  Male     Ex\n-sm\n   3 │ ID00007637202177411956430      7   2061  51.8621     79  Male     Ex\n-sm\n   4 │ ID00007637202177411956430      9   2144  53.9507     79  Male     Ex\n-sm\n   5 │ ID00007637202177411956430     11   2069  52.0634     79  Male     Ex\n-sm ⋯\n                                                                1 column om\nitted","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"The dataset provided us with a baseline chest CT scan and relevant clinical information for a group of patients. Each patient has an image taken at Week = 0, and they undergo numerous follow-up visits over approximately 1-2 years, during which their Forced Vital Capacity (FVC) is measured. For the purpose of this tutorial, we will only consider the Patient ID, the weeks, and the FVC measurements, discarding all other information. Restricting our analysis to these specific columns allowed our team to achieve a competitive score, highlighting the effectiveness of Bayesian hierarchical linear regression models, especially when dealing with uncertainty, which is a crucial aspect of the problem.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Since this is real medical data, the relative timing of FVC measurements varies widely, as shown in the 3 sample patients below:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"patientinfo(dataset, patient_id) = filter(:Patient => ==(patient_id), dataset)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"patientinfo (generic function with 1 method)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"function patientchart(dataset, patient_id; line_kws = true)\n    info = patientinfo(dataset, patient_id)\n    x = info[!, \"Weeks\"]\n    y = info[!, \"FVC\"]\n\n    p = plot(tickfontsize = 10, margin = 1Plots.cm, size = (400, 400), titlefontsize = 11)\n    p = scatter!(p, x, y, title = patient_id, legend = false, xlabel = \"Weeks\", ylabel = \"FVC\")\n    \n    if line_kws\n        # Use the `GLM.jl` package to estimate linear regression\n        linearFormulae = @formula(FVC ~ Weeks)\n        linearRegressor = lm(linearFormulae, patientinfo(dataset, patient_id))\n        linearPredicted = predict(linearRegressor)\n        p = plot!(p, x, linearPredicted, color = :red, lw = 3)\n    end\n\n    return p\nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"patientchart (generic function with 1 method)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"p1 = patientchart(dataset, \"ID00007637202177411956430\")\np2 = patientchart(dataset, \"ID00009637202177434476278\")\np3 = patientchart(dataset, \"ID00010637202177584971671\")\n\nplot(p1, p2, p3, layout = @layout([ a b c ]), size = (1200, 400))","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"On average, each of the 176 patients provided in the dataset had 9 visits during which their FVC was measured. These visits occurred at specific weeks within the interval [-12, 133]. The decline in lung capacity is evident, but it also varies significantly from one patient to another.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Our task was to predict the FVC measurements for each patient at every possible week within the [-12, 133] interval, along with providing a confidence score for each prediction. In other words, we were required to fill a matrix, as shown below, with the predicted values and their corresponding confidence scores:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"The task was ideal for applying Bayesian inference. However, the vast majority of solutions shared within the Kaggle community utilized discriminative machine learning models, disregarding the fact that most discriminative methods struggle to provide realistic uncertainty estimates. This limitation stems from their typical training process, which aims to optimize parameters to minimize certain loss criteria (such as predictive error). As a result, these models do not inherently incorporate uncertainty into their parameters or subsequent predictions. While some methods may produce uncertainty estimates as a by-product or through post-processing steps, these are often heuristic-based and lack a statistically principled approach to estimate the target uncertainty distribution [2].","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Modelling:-Bayesian-Hierarchical-Linear-Regression-with-Partial-Pooling","page":"Bayesian Linear Regression Tutorial","title":"Modelling: Bayesian Hierarchical Linear Regression with Partial Pooling","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"In a basic linear regression, which is not hierarchical, the assumption is that all FVC decline curves share the same α and β values. This model is known as the \"pooled model.\" On the other extreme, we could assume a model where each patient has a personalized FVC decline curve, and these curves are entirely independent of one another. This model is referred to as the \"unpooled model,\" where each patient has completely separate regression lines.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"In this analysis, we will adopt a middle ground approach known as \"Partial pooling.\" Specifically, we will assume that while α's and β's are different for each patient, as in the unpooled case, these coefficients share some similarities. This partial pooling will be achieved by modeling each individual coefficient as being drawn from a common group distribution.:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Mathematically, the model is described by the following equations:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"beginequation\n    beginaligned\n        mu_alpha sim mathcalN(mathrmmean = 00 mathrmvariance = 2500000) \n        sigma_alpha sim mathcalGamma(mathrmshape = 175 mathrmscale = 4554) \n        mu_beta sim mathcalN(mathrmmean = 00 mathrmvariance = 90) \n        sigma_beta sim mathcalGamma(mathrmshape = 175 mathrmscale = 136) \n        alpha_i sim mathcalN(mathrmmean = mu_alpha mathrmprecision = sigma_alpha) \n        beta sim mathcalN(mathrmmean = mu_beta mathrmprecision = sigma_beta) \n        sigma sim mathcalGamma(mathrmshape = 175 mathrmscale = 4554) \n        mathrmFVC_ij sim mathcalN(mathrmmean = alpha_i + t beta_i mathrmprecision = sigma)\n    endaligned\nendequation","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"where t is the time in weeks. Those are very uninformative priors, but that's ok: our model will converge!","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Implementing this model in RxInfer is pretty straightforward:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"@model function partially_pooled(patient_codes, weeks, data)\n    μ_α ~ Normal(mean = 0.0, var = 250000.0) # Prior for the mean of α (intercept)\n    μ_β ~ Normal(mean = 0.0, var = 9.0)      # Prior for the mean of β (slope)\n    σ_α ~ Gamma(shape = 1.75, scale = 45.54) # Prior for the precision of α (intercept)\n    σ_β ~ Gamma(shape = 1.75, scale = 1.36)  # Prior for the precision of β (slope)\n\n    n_codes = length(patient_codes)            # Total number of data points\n    n_patients = length(unique(patient_codes)) # Number of unique patients in the data\n\n    local α # Individual intercepts for each patient\n    local β # Individual slopes for each patient\n\n    for i in 1:n_patients\n        α[i] ~ Normal(mean = μ_α, precision = σ_α) # Sample the intercept α from a Normal distribution\n        β[i] ~ Normal(mean = μ_β, precision = σ_β) # Sample the slope β from a Normal distribution\n    end\n\n    σ ~ Gamma(shape = 1.75, scale = 45.54)   # Prior for the standard deviation of the error term\n    \n    local FVC_est\n\n    for i in 1:n_codes\n        FVC_est[i] ~ α[patient_codes[i]] + β[patient_codes[i]] * weeks[i] # FVC estimation using patient-specific α and β\n        data[i] ~ Normal(mean = FVC_est[i], precision = σ)                # Likelihood of the observed FVC data\n    end\nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Variational constraints are used in variational methods to restrict the set of functions or probability distributions that the method can explore during optimization. These constraints help guide the optimization process towards more meaningful and tractable solutions. We need variational constraints to ensure that the optimization converges to valid and interpretable solutions, avoiding solutions that might not be meaningful or appropriate for the given problem. By incorporating constraints, we can control the complexity and shape of the solutions, making them more useful for practical applications. We use the @constraints macro from RxInfer to define approriate variational constraints.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"@constraints function partially_pooled_constraints()\n    # Assume that `μ_α`, `σ_α`, `μ_β`, `σ_β` and `σ` are jointly independent\n    q(μ_α, σ_α, μ_β, σ_β, σ) = q(μ_α)q(σ_α)q(μ_β)q(σ_β)q(σ)\n    # Assume that `μ_α`, `σ_α`, `α` are jointly independent\n    q(μ_α, σ_α, α) = q(μ_α, α)q(σ_α)\n    # Assume that `μ_β`, `σ_β`, `β` are jointly independent\n    q(μ_β, σ_β, β) = q(μ_β, β)q(σ_β)\n    # Assume that `FVC_est`, `σ` are jointly independent\n    q(FVC_est, σ) = q(FVC_est)q(σ) \nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"partially_pooled_constraints (generic function with 1 method)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"These @constraints assume some structural independencies in the resulting variational approximation. For simplicity we can also use constraints = MeanField() in the inference function below. That's all for modelling!","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Inference-in-the-model","page":"Bayesian Linear Regression Tutorial","title":"Inference in the model","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"A significant achievement of Probabilistic Programming Languages, like RxInfer, is the ability to separate model specification and inference. Once I define my generative model with priors, condition statements, and data likelihoods, I can delegate the challenging inference tasks to RxInfer's inference engine.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Calling the inference engine only takes a few lines of code. Before proceeding, let's assign a numerical Patient ID to each patient code, a task that can be easily accomplished using label encoding.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"patient_ids          = dataset[!, \"Patient\"] # get the column of all patients\npatient_code_encoder = Dict(map(((id, patient), ) -> patient => id, enumerate(unique(patient_ids))));\npatient_code_column  = map(patient -> patient_code_encoder[patient], patient_ids)\n\ndataset[!, :PatientCode] = patient_code_column\n\nfirst(patient_code_encoder, 5)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"5-element Vector{Pair{InlineStrings.String31, Int64}}:\n \"ID00197637202246865691526\" => 85\n \"ID00388637202301028491611\" => 160\n \"ID00341637202287410878488\" => 142\n \"ID00020637202178344345685\" => 9\n \"ID00305637202281772703145\" => 127","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"function partially_pooled_inference(dataset)\n\n    patient_codes = values(dataset[!, \"PatientCode\"])\n    weeks = values(dataset[!, \"Weeks\"])\n    FVC_obs = values(dataset[!, \"FVC\"]);\n\n    init = @initialization begin \n        μ(α) = vague(NormalMeanVariance)\n        μ(β) = vague(NormalMeanVariance)\n        q(α) = vague(NormalMeanVariance)\n        q(β) = vague(NormalMeanVariance)\n        q(σ) = vague(Gamma)\n        q(σ_α) = vague(Gamma)\n        q(σ_β) = vague(Gamma)\n    end\n\n    results = infer(\n        model = partially_pooled(patient_codes = patient_codes, weeks = weeks),\n        data = (data = FVC_obs, ),\n        options = (limit_stack_depth = 500, ),\n        constraints = partially_pooled_constraints(),\n        initialization = init,\n        returnvars = KeepLast(),\n        iterations = 100\n    )\n    \nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"partially_pooled_inference (generic function with 1 method)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"We use a hybrid message passing approach combining exact and variational inference. In loopy models, where there are cycles or feedback loops in the graphical model, we need to initialize messages to kick-start the message passing process. Messages are passed between connected nodes in the model to exchange information and update beliefs iteratively. Initializing messages provides a starting point for the iterative process and ensures that the model converges to a meaningful solution.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"In variational inference procedures, we need to initialize marginals because variational methods aim to approximate the true posterior distribution with a simpler, tractable distribution. Initializing marginals involves providing initial estimates for the parameters of this approximating distribution. These initial estimates serve as a starting point for the optimization process, allowing the algorithm to iteratively refine the approximation until it converges to a close approximation of the true posterior distribution. ","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"partially_pooled_inference_results = partially_pooled_inference(dataset)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Inference results:\n  Posteriors       | available for (α, σ_α, σ_β, σ, FVC_est, μ_β, μ_α, β)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Checking-the-model","page":"Bayesian Linear Regression Tutorial","title":"Checking the model","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Inspecting-the-learned-parameters","page":"Bayesian Linear Regression Tutorial","title":"Inspecting the learned parameters","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"# Convert to `Normal` since it supports easy plotting with `StatsPlots`\nlet \n    local μ_α = Normal(mean_std(partially_pooled_inference_results.posteriors[:μ_α])...)\n    local μ_β = Normal(mean_std(partially_pooled_inference_results.posteriors[:μ_β])...)\n    local α = map(d -> Normal(mean_std(d)...), partially_pooled_inference_results.posteriors[:α])\n    local β = map(d -> Normal(mean_std(d)...), partially_pooled_inference_results.posteriors[:β])\n    \n    local p1 = plot(μ_α, title = \"q(μ_α)\", fill = 0, fillalpha = 0.2, label = false)\n    local p2 = plot(μ_β, title = \"q(μ_β)\", fill = 0, fillalpha = 0.2, label = false)\n    \n    local p3 = plot(title = \"q(α)...\", legend = false)\n    local p4 = plot(title = \"q(β)...\", legend = false)\n    \n    foreach(d -> plot!(p3, d), α) # Add each individual `α` on plot `p3`\n    foreach(d -> plot!(p4, d), β) # Add each individual `β` on plot `p4`\n    \n    plot(p1, p2, p3, p4, size = (1200, 400), layout = @layout([ a b; c d ]))\nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Looks like our model learned personalized alphas and betas for each patient!","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Visualizing-FVC-decline-curves-for-some-patients","page":"Bayesian Linear Regression Tutorial","title":"Visualizing FVC decline curves for some patients","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Now, let's visually inspect the FVC decline curves predicted by our model. We will complete the FVC table by predicting all the missing values. To do this, we need to create a table to accommodate the predictions.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"function patientchart_bayesian(results, dataset, encoder, patient_id; kwargs...)\n    info            = patientinfo(dataset, patient_id)\n    patient_code_id = encoder[patient_id]\n\n    patient_α = results.posteriors[:α][patient_code_id]\n    patient_β = results.posteriors[:β][patient_code_id]\n\n    estimated_σ = inv(mean(results.posteriors[:σ]))\n    \n    predict_weeks = range(-12, 134)\n\n    predicted = map(predict_weeks) do week\n        pm = mean(patient_α) + mean(patient_β) * week\n        pv = var(patient_α) + var(patient_β) * week ^ 2 + estimated_σ\n        return pm, sqrt(pv)\n    end\n    \n    p = patientchart(dataset, patient_id; kwargs...)\n    \n    return plot!(p, predict_weeks, getindex.(predicted, 1), ribbon = getindex.(predicted, 2), color = :orange)\nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"patientchart_bayesian (generic function with 1 method)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"p1 = patientchart_bayesian(partially_pooled_inference_results, dataset, patient_code_encoder, \"ID00007637202177411956430\")\np2 = patientchart_bayesian(partially_pooled_inference_results, dataset, patient_code_encoder, \"ID00009637202177434476278\")\np3 = patientchart_bayesian(partially_pooled_inference_results, dataset, patient_code_encoder, \"ID00011637202177653955184\")\n\nplot(p1, p2, p3, layout = @layout([ a b c ]), size = (1200, 400))","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"The results match our expectations perfectly! Let's highlight the observations:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"The model successfully learned Bayesian Linear Regressions! The orange line representing the learned predicted FVC mean closely aligns with the red line representing the deterministic linear regression. More importantly, the model effectively predicts uncertainty, demonstrated by the light orange region surrounding the mean FVC line.\nThe model predicts higher uncertainty in cases where the data points are more dispersed, such as in the 1st and 3rd patients. In contrast, when data points are closely grouped together, as seen in the 2nd patient, the model predicts higher confidence, resulting in a narrower light orange region.\nAdditionally, across all patients, we observe that the uncertainty increases as we look further into the future. The light orange region widens as the number of weeks increases, reflecting the growth of uncertainty over time.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Computing-the-modified-Laplace-Log-Likelihood-and-RMSE","page":"Bayesian Linear Regression Tutorial","title":"Computing the modified Laplace Log Likelihood and RMSE","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"As mentioned earlier, the competition evaluated models using a modified version of the Laplace Log Likelihood, which takes into account both the accuracy and certainty of each prediction—a valuable feature in medical applications.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"To compute the metric, we predicted both the FVC value and its associated confidence measure (standard deviation σ). The metric is given by the formula:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"beginequation\n    beginaligned\n        sigma_mathrmclipped = max(sigma 70) \n        delta = min(vert mathrmFVC_mathrmtrue - mathrmFVC_mathrmpredvert 1000) \n        mathrmmetric = -fracsqrt2deltasigma_mathrmclipped - mathrmln(sqrt2sigma_mathrmclipped) \n    endaligned\nendequation","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"To prevent large errors from disproportionately penalizing results, errors were thresholded at 1000 ml. Additionally, confidence values were clipped at 70 ml to account for the approximate measurement uncertainty in FVC. The final score was determined by averaging the metric across all (Patient, Week) pairs. It is worth noting that metric values will be negative, and a higher score indicates better model performance.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"function FVC_predict(results)\n    return broadcast(results.posteriors[:FVC_est], Ref(results.posteriors[:σ])) do f, s\n        return @call_rule NormalMeanPrecision(:out, Marginalisation) (m_μ = f, q_τ = s)\n    end\nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"FVC_predict (generic function with 1 method)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"function compute_rmse(results, dataset)\n    FVC_predicted = FVC_predict(results)\n    return mean((dataset[!, \"FVC\"] .- mean.(FVC_predicted)) .^ 2) ^ (1/2)\nend\n\nfunction compute_laplace_log_likelihood(results, dataset)\n    FVC_predicted = FVC_predict(results)\n    sigma_c = std.(FVC_predicted)\n    sigma_c[sigma_c .< 70] .= 70\n    delta = abs.(mean.(FVC_predicted) .- dataset[!, \"FVC\"])\n    delta[delta .> 1000] .= 1000\n    return mean(-sqrt(2) .* delta ./ sigma_c .- log.(sqrt(2) .* sigma_c))\nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"compute_laplace_log_likelihood (generic function with 1 method)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"println(\"RMSE: $(compute_rmse(partially_pooled_inference_results, dataset))\")\nprintln(\"Laplace Log Likelihood: $(compute_laplace_log_likelihood(partially_pooled_inference_results, dataset))\")","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"RMSE: 124.01306457993996\nLaplace Log Likelihood: -6.156795767593613","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"What do these numbers signify? They indicate that adopting this approach would lead to outperforming the majority of public solutions in the competition. In several seconds of inference!","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Interestingly, most public solutions rely on a standard deterministic Neural Network and attempt to model uncertainty through a quantile loss, adhering to a frequentist approach. The importance of uncertainty in single predictions is growing in the field of machine learning, becoming a crucial requirement. Especially when the consequences of an inaccurate prediction are significant, knowing the probability distribution of individual predictions becomes essential.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Add-layer-to-model-hierarchy:-Smoking-Status","page":"Bayesian Linear Regression Tutorial","title":"Add layer to model hierarchy: Smoking Status","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"We can enhance the model by incorporating the column \"SmokingStatus\" as a pooling level, where model parameters will be partially pooled within the groups \"Never smoked,\" \"Ex-smoker,\" and \"Currently smokes.\" To achieve this, we need to:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Encode the \"SmokingStatus\" column. Map the patient encoding to the corresponding \"SmokingStatus\" encodings. Refine and retrain the model with the additional hierarchical structure.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"combine(groupby(dataset, \"SmokingStatus\"), nrow)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"3×2 DataFrame\n Row │ SmokingStatus     nrow\n     │ String31          Int64\n─────┼─────────────────────────\n   1 │ Ex-smoker          1038\n   2 │ Never smoked        429\n   3 │ Currently smokes     82","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"smoking_id_mapping   = Dict(map(((code, smoking_status), ) -> smoking_status => code, enumerate(unique(dataset[!, \"SmokingStatus\"]))))\nsmoking_code_encoder = Dict(map(unique(values(patient_ids))) do patient_id\n    smoking_status = first(unique(patientinfo(dataset, patient_id)[!, \"SmokingStatus\"]))\n    return patient_code_encoder[patient_id] => smoking_id_mapping[smoking_status]\nend)\n\nsmoking_status_patient_mapping = map(id -> smoking_code_encoder[id], 1:length(unique(patient_ids)));","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"@model function partially_pooled_with_smoking(patient_codes, smoking_status_patient_mapping, weeks, data)\n    μ_α_global ~ Normal(mean = 0.0, var = 250000.0) # Prior for the mean of α (intercept)\n    μ_β_global ~ Normal(mean = 0.0, var = 250000.0) # Prior for the mean of β (slope)\n    σ_α_global ~ Gamma(shape = 1.75, scale = 45.54) # Corresponds to half-normal with scale 100.0\n    σ_β_global ~ Gamma(shape = 1.75, scale = 1.36)  # Corresponds to half-normal with scale 3.0\n\n    n_codes = length(patient_codes) # Total number of data points\n    n_smoking_statuses = length(unique(smoking_status_patient_mapping)) # Number of different smoking patterns\n    n_patients = length(unique(patient_codes)) # Number of unique patients in the data\n\n    local μ_α_smoking_status  # Individual intercepts for smoking pattern\n    local μ_β_smoking_status  # Individual slopes for smoking pattern\n    \n    for i in 1:n_smoking_statuses\n        μ_α_smoking_status[i] ~ Normal(mean = μ_α_global, precision = σ_α_global)\n        μ_β_smoking_status[i] ~ Normal(mean = μ_β_global, precision = σ_β_global)\n    end\n    \n    local α # Individual intercepts for each patient\n    local β # Individual slopes for each patient\n\n    for i in 1:n_patients\n        α[i] ~ Normal(mean = μ_α_smoking_status[smoking_status_patient_mapping[i]], precision = σ_α_global)\n        β[i] ~ Normal(mean = μ_β_smoking_status[smoking_status_patient_mapping[i]], precision = σ_β_global)\n    end\n\n    σ ~ Gamma(shape = 1.75, scale = 45.54) # Corresponds to half-normal with scale 100.0\n\n    local FVC_est\n\n    for i in 1:n_codes\n        FVC_est[i] ~ α[patient_codes[i]] + β[patient_codes[i]] * weeks[i] # FVC estimation using patient-specific α and β\n        data[i] ~ Normal(mean = FVC_est[i], precision = σ)              # Likelihood of the observed FVC data\n    end\n    \nend\n\n@constraints function partially_pooled_with_smooking_constraints()\n    q(μ_α_global, σ_α_global, μ_β_global, σ_β_global) = q(μ_α_global)q(σ_α_global)q(μ_β_global)q(σ_β_global)\n    q(μ_α_smoking_status, μ_β_smoking_status, σ_α_global, σ_β_global) = q(μ_α_smoking_status)q(μ_β_smoking_status)q(σ_α_global)q(σ_β_global)\n    q(μ_α_global, σ_α_global, μ_β_global, σ_β_global, σ) = q(μ_α_global)q(σ_α_global)q(μ_β_global)q(σ_β_global)q(σ)\n    q(μ_α_global, σ_α_global, α) = q(μ_α_global, α)q(σ_α_global)\n    q(μ_β_global, σ_β_global, β) = q(μ_β_global, β)q(σ_β_global)\n    q(FVC_est, σ) = q(FVC_est)q(σ) \nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"partially_pooled_with_smooking_constraints (generic function with 1 method)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"function partially_pooled_with_smoking(dataset, smoking_status_patient_mapping)\n    patient_codes = values(dataset[!, \"PatientCode\"])\n    weeks = values(dataset[!, \"Weeks\"])\n    FVC_obs = values(dataset[!, \"FVC\"]);\n\n    init = @initialization begin \n        μ(α) = vague(NormalMeanVariance)\n        μ(β) = vague(NormalMeanVariance)\n        q(σ) = Gamma(1.75, 45.54)\n        q(σ_α_global) = Gamma(1.75, 45.54)\n        q(σ_β_global) = Gamma(1.75, 1.36)\n    end\n    \n    return infer(\n        model = partially_pooled_with_smoking(\n            patient_codes = patient_codes, \n            smoking_status_patient_mapping = smoking_status_patient_mapping, \n            weeks = weeks\n        ),\n        data = (data = FVC_obs, ),\n        options = (limit_stack_depth = 500, ),\n        constraints = partially_pooled_with_smooking_constraints(),\n        initialization = init,\n        returnvars = KeepLast(),\n        iterations = 100,\n    )\nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"partially_pooled_with_smoking (generic function with 3 methods)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"partially_pooled_with_smoking_inference_results = partially_pooled_with_smoking(dataset, smoking_status_patient_mapping);","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Inspect-the-learned-parameters","page":"Bayesian Linear Regression Tutorial","title":"Inspect the learned parameters","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"# Convert to `Normal` since it supports easy plotting with `StatsPlots`\nlet \n    local μ_α = Normal(mean_std(partially_pooled_with_smoking_inference_results.posteriors[:μ_α_global])...)\n    local μ_β = Normal(mean_std(partially_pooled_with_smoking_inference_results.posteriors[:μ_β_global])...)\n    local αsmoking = map(d -> Normal(mean_std(d)...), partially_pooled_with_smoking_inference_results.posteriors[:μ_α_smoking_status])\n    local βsmoking = map(d -> Normal(mean_std(d)...), partially_pooled_with_smoking_inference_results.posteriors[:μ_β_smoking_status])\n    local α = map(d -> Normal(mean_std(d)...), partially_pooled_with_smoking_inference_results.posteriors[:α])\n    local β = map(d -> Normal(mean_std(d)...), partially_pooled_with_smoking_inference_results.posteriors[:β])\n    \n    local p1 = plot(μ_α, title = \"q(μ_α_global)\", fill = 0, fillalpha = 0.2, label = false)\n    local p2 = plot(μ_β, title = \"q(μ_β_global)\", fill = 0, fillalpha = 0.2, label = false)\n    \n    local p3 = plot(title = \"q(α)...\", legend = false)\n    local p4 = plot(title = \"q(β)...\", legend = false)\n    \n    foreach(d -> plot!(p3, d), α) # Add each individual `α` on plot `p3`\n    foreach(d -> plot!(p4, d), β) # Add each individual `β` on plot `p4`\n    \n    local p5 = plot(title = \"q(μ_α_smoking_status)...\", legend = false)\n    local p6 = plot(title = \"q(μ_β_smoking_status)...\", legend = false)\n    \n    foreach(d -> plot!(p5, d, fill = 0, fillalpha = 0.2), αsmoking) \n    foreach(d -> plot!(p6, d, fill = 0, fillalpha = 0.2), βsmoking)\n    \n    plot(p1, p2, p3, p4, p5, p6, size = (1200, 600), layout = @layout([ a b; c d; e f ]))\nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Interpret-smoking-status-model-parameters","page":"Bayesian Linear Regression Tutorial","title":"Interpret smoking status model parameters","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"The model parameters for each smoking status reveal intriguing findings, particularly concerning the trend, μ_β_smoking_status. In the summary below, it is evident that the trend for current smokers has a positive mean, while the trend for ex-smokers and those who have never smoked is negative.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"smoking_id_mapping","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Dict{InlineStrings.String31, Int64} with 3 entries:\n  \"Currently smokes\" => 3\n  \"Ex-smoker\"        => 1\n  \"Never smoked\"     => 2","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"posteriors_μ_β_smoking_status = partially_pooled_with_smoking_inference_results.posteriors[:μ_β_smoking_status]\n\nprintln(\"Trend for\")\nforeach(pairs(smoking_id_mapping)) do (key, id)\n    println(\"  $key: $(mean(posteriors_μ_β_smoking_status[id]))\")\nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Trend for\n  Currently smokes: 1.8146982895647819\n  Ex-smoker: -4.5727468087945145\n  Never smoked: -4.447716327127484","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Let's look at these curves for individual patients to help interpret these model results.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"# Never smoked\np1 = patientchart_bayesian(partially_pooled_with_smoking_inference_results, dataset, patient_code_encoder, \"ID00007637202177411956430\") \n# Ex-smoker\np2 = patientchart_bayesian(partially_pooled_with_smoking_inference_results, dataset, patient_code_encoder, \"ID00009637202177434476278\") \n# Currently smokes\np3 = patientchart_bayesian(partially_pooled_with_smoking_inference_results, dataset, patient_code_encoder, \"ID00011637202177653955184\") \n\nplot(p1, p2, p3, layout = @layout([ a b c ]), size = (1200, 400))","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Review-patients-that-currently-smoke","page":"Bayesian Linear Regression Tutorial","title":"Review patients that currently smoke","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"When plotting each patient with the smoking status \"Currently smokes,\" we observe different trends. Some patients show a clear positive trend, while others do not exhibit a clear trend or even have a negative trend. Compared to the unpooled trend lines, the trend lines with partial pooling are less prone to overfitting and display greater uncertainty in both slope and intercept.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Depending on the purpose of the model, we can proceed in different ways:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"If our goal is to gain insights into how different attributes relate to a patient's FVC over time, we can stop here and understand that current smokers might experience an increase in FVC over time when monitored for Pulmonary Fibrosis. We may then formulate hypotheses to explore the reasons behind this observation and design new experiments for further testing.\nHowever, if our aim is to develop a model for generating predictions to treat patients, it becomes crucial to ensure that the model does not overfit and can be trusted with new patients. To achieve this, we could adjust model parameters to shrink the \"Currently smokes\" group's parameters closer to the global parameters, or even consider merging the group with \"Ex-smokers.\" Additionally, collecting more data for current smokers could help in ensuring the model's robustness and preventing overfitting.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"let \n    local plots = []\n\n    for (i, patient) in enumerate(unique(filter(:SmokingStatus => ==(\"Currently smokes\"), dataset)[!, \"Patient\"]))\n        push!(plots, patientchart_bayesian(partially_pooled_with_smoking_inference_results, dataset, patient_code_encoder, patient))\n    end\n\n    plot(plots..., size = (1200, 1200))\nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Modified-Laplace-Log-Likelihood-and-RMSE-for-model-with-Smoking-Status-Level","page":"Bayesian Linear Regression Tutorial","title":"Modified Laplace Log Likelihood and RMSE for model with Smoking Status Level","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"We calculate the metrics for the updated model and compare to the original model.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"println(\"RMSE: $(compute_rmse(partially_pooled_with_smoking_inference_results, dataset))\")\nprintln(\"Laplace Log Likelihood: $(compute_laplace_log_likelihood(partially_pooled_with_smoking_inference_results, dataset))\")","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"RMSE: 124.81131955994066\nLaplace Log Likelihood: -6.165668922454811","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Both the Laplace Log Likelihood and RMSE indicate slightly worse performance for the smoking status model. Adding this hierarchy level as it is did not improve the model's performance significantly. However, we did discover some interesting results from the smoking status level that might warrant further investigation. Additionally, we could attempt to enhance model performance by adjusting priors or exploring different hierarchy levels, such as gender.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#References","page":"Bayesian Linear Regression Tutorial","title":"References","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"[1] Ghahramani, Z. Probabilistic machine learning and artificial intelligence. Nature 521, 452–459 (2015). https://doi.org/10.1038/nature14541","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"[2] Rainforth, Thomas William Gamlen. Automating Inference, Learning, and Design Using Probabilistic Programming. University of Oxford, 2017.","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/#examples-probit-model-(ep)","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"","category":"section"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/#Estimation-of-pollutant","page":"Probit Model (EP)","title":"Estimation of pollutant","text":"","category":"section"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"Mortality y_t of fishs in a lake is observed over time. Mortality rate textBer(Phi(x_t)) is linked to the level of pollutant x_t in the lake according to the probit model (see below). The municipality wants to keep track of the pollution. To do so, the level of pollutant in the lake is tracked over time through observations of the fishs.","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/#Objective","page":"Probit Model (EP)","title":"Objective","text":"","category":"section"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"Probit model aims to infer a random proces value from noisy binary observations of it. RxInfer comes with support for expectation propagation (EP). In this demo we illustrate EP in the context of state-estimation in a linear state-space model that combines a Gaussian state-evolution model with a discrete observation model. Here, the probit function links continuous variable x_t with the discrete variable y_t. The model is defined as:","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"beginaligned\n    u = 01 \n    x_0 sim mathcalN(0 100) \n    x_t sim mathcalN(x_t-1+ u 001) \n    y_t sim mathrmBer(Phi(x_t))\nendaligned","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/#Import-packages","page":"Probit Model (EP)","title":"Import packages","text":"","category":"section"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"using RxInfer, GraphPPL,StableRNGs, Random, Plots, Distributions\nusing StatsFuns: normcdf","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/#Data-generation","page":"Probit Model (EP)","title":"Data generation","text":"","category":"section"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"function generate_data(nr_samples::Int64; seed = 123)\n    \n    rng = StableRNG(seed)\n    \n    # hyper parameters\n    u = 0.1\n\n    # allocate space for data\n    data_x = zeros(nr_samples + 1)\n    data_y = zeros(nr_samples)\n    \n    # initialize data\n    data_x[1] = -2\n    \n    # generate data\n    for k in eachindex(data_y)\n        \n        # calculate new x\n        data_x[k+1] = data_x[k] + u + sqrt(0.01)*randn(rng)\n        \n        # calculate y\n        data_y[k] = normcdf(data_x[k+1]) > rand(rng)\n        \n    end\n    \n    # return data\n    return data_x, data_y\n    \nend;","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"n = 40","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"40","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"data_x, data_y = generate_data(n);","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"p = plot(xlabel = \"t\", ylabel = \"x, y\")\np = scatter!(p, data_y, label = \"y\")\np = plot!(p, data_x[2:end], label = \"x\")","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/#Model-specification","page":"Probit Model (EP)","title":"Model specification","text":"","category":"section"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"@model function probit_model(y, prior_x)\n    \n    # specify uninformative prior\n    x_prev ~ prior_x\n    \n    # create model \n    for k in eachindex(y)\n        x[k] ~ Normal(mean = x_prev + 0.1, precision = 100)\n        y[k] ~ Probit(x[k]) where {\n            # Probit node by default uses RequireMessage pipeline with vague(NormalMeanPrecision) message as initial value for `in` edge\n            # To change initial value user may specify it manually, like. Changes to the initial message may improve stability in some situations\n            dependencies = RequireMessageFunctionalDependencies(in = NormalMeanPrecision(0.0, 0.01))\n        }\n        x_prev = x[k]\n    end\n    \nend;","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/#Probit-Node","page":"Probit Model (EP)","title":"Probit Node","text":"","category":"section"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"Probit node needs an initialisation of the 'in' message because of this computation methodology. The input message is not directly calculated. First the marginal q(in) is computed and then the output message, this using the margianalisation formula. ","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"overrightarrowmu(x) overleftarrowmu(x) = q(x)","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"Consequently an initial message overleftarrowmu(in) is needed to start iterate. It can be speficied as in the above example. Otherwise RxInfer will initiate it at a default value.","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/#Inference","page":"Probit Model (EP)","title":"Inference","text":"","category":"section"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"result = infer(\n    model = probit_model(prior_x=Normal(0.0, 100.0)), \n    data  = (y = data_y, ), \n    iterations = 5, \n    returnvars = (x = KeepLast(),),\n    free_energy  = true\n)","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"Inference results:\n  Posteriors       | available for (x)\n  Free Energy:     | Real[25.6698, 18.0157, 17.9199, 17.9194, 17.9194]","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/#Results","page":"Probit Model (EP)","title":"Results","text":"","category":"section"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"mx = result.posteriors[:x]\n\np = plot(xlabel = \"t\", ylabel = \"x, y\", legend = :bottomright)\np = scatter!(p, data_y, label = \"y\")\np = plot!(p, data_x[2:end], label = \"x\", lw = 2)\np = plot!(mean.(mx)[2:end], ribbon = std.(mx)[2:end], fillalpha = 0.2, label=\"x (inferred mean)\")\n\nf = plot(xlabel = \"t\", ylabel = \"BFE\")\nf = plot!(result.free_energy, label = \"Bethe Free Energy\")\n\nplot(p, f, size = (800, 400))","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#examples-advanced-tutorial","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"using RxInfer, Plots","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"This notebook covers the fundamentals and advanced usage of the RxInfer.jl package.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"This tutorial is also available in the documentation.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#General-model-specification-syntax","page":"Advanced Tutorial","title":"General model specification syntax","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"We use the @model macro from the GraphPPL.jl package to create a probabilistic model p(s y) and we also specify extra constraints on the variational family of distributions mathcalQ, used for approximating intractable posterior distributions. Below there is a simple example of the general syntax for model specification. In this tutorial we do not cover all possible ways to create models or advanced features of GraphPPL.jl.  Instead we refer the interested reader to the documentation for a more rigorous explanation and illustrative examples.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# the `@model` macro accepts a regular Julia function\n@model function test_model1(s_mean, s_precision, y)\n    \n    # the `tilde` operator creates a functional dependency\n    # between variables in our model and can be read as \n    # `sampled from` or `is modeled by`\n    s ~ Normal(mean = s_mean, precision = s_precision)\n    y ~ Normal(mean = s, precision = 1.0)\n    \n    # It is possible to return something from the model specification (including variables and nodes)\n    return \"Hello world\"\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The @model macro creates a function with the same name and with the same set of input arguments as the original function (test_model1(s_mean, s_precision, y) in this example). The arguments are however converted to the keyword arguments. The @model macro does not support positional arguments.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"It is also possible to use control flow statements such as if or for blocks in the model specification function. In general, any valid snippet of Julia code can be used inside the @model block. As an example consider the following (valid!) model:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model2(y)\n    \n    if length(y) <= 1\n        error(\"The `length` of `y` argument must be greater than one.\")\n    end\n    \n    s[1] ~ Normal(mean = 0.0, precision = 0.1)\n    y[1] ~ Normal(mean = s[1], precision = 1.0)\n    \n    for i in eachindex(y)\n        s[i] ~ Normal(mean = s[i - 1], precision = 1.0)\n        y[i] ~ Normal(mean = s[i], precision = 1.0)\n    end\n    \nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"It is also possible to use complex expressions inside the functional dependency expressions","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"y ~ Normal(mean = 2.0 * (s + 1.0), precision = 1.0)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The ~ operator automatically creates a random variable if none was created before with the same name and throws an error if this name already exists","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# `~` creates random variables automatically\ns ~ Normal(mean = 0.0, precision1.0)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#Probabilistic-inference-in-RxInfer.jl","page":"Advanced Tutorial","title":"Probabilistic inference in RxInfer.jl","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"RxInfer.jl uses the Rocket.jl package API for inference routines. Rocket.jl is a reactive programming extension for Julia that is higly inspired by RxJS and similar libraries from the Rx ecosystem. It consists of observables, actors, subscriptions and operators. For more information and rigorous examples see Rocket.jl github page.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#Observables","page":"Advanced Tutorial","title":"Observables","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Observables are lazy push-based collections and they deliver their values over time.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Timer that emits a new value every second and has an initial one second delay \nobservable = timer(300, 300)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"TimerObservable(300, 300)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"A subscription allows us to subscribe on future values of some observable, and actors specify what to do with these new values:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"actor = (value) -> println(value)\nsubscription1 = subscribe!(observable, actor)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"TimerSubscription()","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# We always need to unsubscribe from some observables\nunsubscribe!(subscription1)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# We can modify our observables\nmodified = observable |> filter(d -> rem(d, 2) === 1) |> map(Int, d -> d ^ 2)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"ProxyObservable(Int64, MapProxy(Int64))","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"subscription2 = subscribe!(modified, (value) -> println(value))","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"TimerSubscription()","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"unsubscribe!(subscription2)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#Coin-Toss-Model","page":"Advanced Tutorial","title":"Coin Toss Model","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function coin_toss_model(y)\n    # We endow θ parameter of our model with some prior\n    θ  ~ Beta(2.0, 7.0)\n    # We assume that the outcome of each coin flip \n    # is modeled by a Bernoulli distribution\n    y .~ Bernoulli(θ)\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"We can call the infer function to run inference in such model:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"p = 0.75 # Bias of a coin\n\ndataset = float.(rand(Bernoulli(p), 500));\n\nresult = infer(\n    model = coin_toss_model(),\n    data  = (y = dataset, )\n)\n\nprintln(\"Inferred bias is \", mean(result.posteriors[:θ]), \" with standard deviation is \", std(result.posteriors[:θ]))","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Inferred bias is 0.75049115913556 with standard deviation is 0.019161551535\n43022","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"We can see that the inferred bias is quite close to the actual value we used in the dataset generation with a low standard deviation.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#Reactive-Online-Inference","page":"Advanced Tutorial","title":"Reactive Online Inference","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"RxInfer.jl naturally supports reactive streams of data and it is possible to run reactive inference with some external datasource.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function online_coin_toss_model(θ_a, θ_b, y)\n    θ ~ Beta(θ_a, θ_b)\n    y ~ Bernoulli(θ)\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"autoupdates = @autoupdates begin \n    θ_a, θ_b = params(q(θ))\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(θ_a,θ_b = params(q(θ)),)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"init = @initialization begin\n    q(θ) = vague(Beta)\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Initial state: \n  q(θ) = Beta{Float64}(α=1.0, β=1.0)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"rxresult = infer(\n    model = online_coin_toss_model(),\n    data  = (y = dataset, ),\n    autoupdates = autoupdates,\n    historyvars = (θ = KeepLast(), ),\n    keephistory = length(dataset),\n    initialization = init,\n    autostart = true\n);","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"animation = @animate for i in 1:length(dataset)\n    plot(mean.(rxresult.history[:θ][1:i]), ribbon = std.(rxresult.history[:θ][1:i]), title = \"Online coin bias inference\", label = \"Inferred bias\", legend = :bottomright)\n    hline!([ p ], label = \"Real bias\", size = (600, 200))\nend\n\ngif(animation, \"../pics/online-coin-bias-inference.gif\", fps = 24, show_msg = false);","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In this example we used static dataset and the history field of the reactive inference result, but the rxinference function also supports any real-time reactive stream and can run indefinitely.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"That was an example of exact Bayesian inference with Sum-Product (or Belief Propagation) algorithm. However, RxInfer is not limited to only the sum-product algorithm but it also supports variational message passing with Constrained Bethe Free Energy Minimisation.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#Variational-inference","page":"Advanced Tutorial","title":"Variational inference","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"On a very high-level, RxInfer is aimed to solve the Constrained Bethe Free Energy minimisation problem. For this task we approximate our exact posterior marginal distribution by some family of distributions q in mathcalQ. Often this involves assuming some factorization over q. ","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model6(y)\n    τ ~ Gamma(shape = 1.0, rate = 1.0) \n    μ ~ Normal(mean = 0.0, variance = 100.0)\n    for i in eachindex(y)\n        y[i] ~ Normal(mean = μ, precision = τ)\n    end\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In this example we want to specify extra constraints for q_a for Bethe factorisation:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"beginaligned\nq(s) = prod_a in mathcalV q_a(s_a) prod_i in mathcalE q_i^-1(s_i)\nendaligned","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"RxInfer.jl package exports @constraints macro to simplify factorisation and form constraints specification. Read more about @constraints macro in the corresponding documentation section, here we show a simple example of the same factorisation constraints specification, but with @constraints macro:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"constraints6 = @constraints begin\n     q(μ, τ) = q(μ)q(τ) # Mean-Field over `μ` and `τ`\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Constraints: \n  q(μ, τ) = q(μ)q(τ)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"init = @initialization begin\n    q(μ) = vague(NormalMeanPrecision)\n    q(τ) = vague(GammaShapeRate)\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Initial state: \n  q(μ) = NormalMeanPrecision{Float64}(μ=0.0, w=1.0e-12)\n  q(τ) = GammaShapeRate{Float64}(a=1.0, b=1.0e-12)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#Inference","page":"Advanced Tutorial","title":"Inference","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"To run inference in this model we again need to create a synthetic dataset and call the infer function.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"dataset = rand(Normal(-3.0, inv(sqrt(5.0))), 1000);\nresult = infer(\n    model          = test_model6(),\n    data           = (y = dataset, ),\n    constraints    = constraints6, \n    initialization = init,\n    returnvars     = (μ = KeepLast(), τ = KeepLast()),\n    iterations     = 10,\n    free_energy    = true,\n    showprogress   = true\n)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Inference results:\n  Posteriors       | available for (μ, τ)\n  Free Energy:     | Real[14763.3, 3276.18, 658.598, 617.079, 617.079, 617.\n079, 617.079, 617.079, 617.079, 617.079]","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"println(\"μ: mean = \", mean(result.posteriors[:μ]), \", std = \", std(result.posteriors[:μ]))","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"μ: mean = -3.0147758379910377, std = 0.014074090277465501","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"println(\"τ: mean = \", mean(result.posteriors[:τ]), \", std = \", std(result.posteriors[:τ]))","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"τ: mean = 5.048454829756536, std = 0.22554832794703839","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#Form-constraints","page":"Advanced Tutorial","title":"Form constraints","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In order to support form constraints, the @constraints macro supports additional type specifications for posterior marginals.  For example, here how we can perform the EM algorithm with PointMass form constraint.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model7(y)\n    τ ~ Gamma(shape = 1.0, rate = 1.0) \n    μ ~ Normal(mean = 0.0, variance = 100.0)\n    for i in eachindex(y)\n        y[i] ~ Normal(mean = μ, precision = τ)\n    end\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"As in the previous example we can use @constraints macro to achieve the same goal with a nicer syntax:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"constraints7 = @constraints begin \n    q(μ) :: PointMassFormConstraint()\n    \n    q(μ, τ) = q(μ)q(τ) # Mean-Field over `μ` and `τ`\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Constraints: \n  q(μ, τ) = q(μ)q(τ)\n  q(μ) :: PointMassFormConstraint()","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"dataset = rand(Normal(-3.0, inv(sqrt(5.0))), 1000);\nresult = infer(\n    model          = test_model7(),\n    data           = (y = dataset, ),\n    constraints    = constraints7, \n    initialization = init,\n    returnvars     = (μ = KeepLast(), τ = KeepLast()),\n    iterations     = 10,\n    free_energy    = true,\n    showprogress   = true\n)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Inference results:\n  Posteriors       | available for (μ, τ)\n  Free Energy:     | Real[14766.5, 2041.89, 559.186, 559.186, 559.186, 559.\n186, 559.186, 559.186, 559.186, 559.186]","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"println(\"μ: mean = \", mean(result.posteriors[:μ]), \", std = \", std(result.posteriors[:μ]))","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"μ: mean = -2.9943153620101035, std = 0.0","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"println(\"τ: mean = \", mean(result.posteriors[:τ]), \", std = \", std(result.posteriors[:τ]))","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"τ: mean = 5.634780774155663, std = 0.251743439293131","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#Meta-data-specification","page":"Advanced Tutorial","title":"Meta data specification","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"During model specification some functional dependencies may accept an optional meta object in the where { ... } clause. The purpose of the meta object is to adjust, modify or supply some extra information to the inference backend during the computations of the messages. The meta object for example may contain an approximation method that needs to be used during various approximations or it may specify the tradeoff between accuracy and performance:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# In this example the `meta` object for the autoregressive `AR` node specifies the variate type of \n# the autoregressive process and its order. In addition it specifies that the message computation rules should\n# respect accuracy over speed with the `ARsafe()` strategy. In contrast, `ARunsafe()` strategy tries to speedup computations\n# by cost of possible numerical instabilities during an inference procedure\ns[i] ~ AR(s[i - 1], θ, γ) where { meta = ARMeta(Multivariate, order, ARsafe()) }\n...\ns[i] ~ AR(s[i - 1], θ, γ) where { meta = ARMeta(Univariate, order, ARunsafe()) }","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Another example with GaussianControlledVariance, or simply GCV [see Hierarchical Gaussian Filter], node:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# In this example we specify structured factorisation and flag meta with `GaussHermiteCubature` \n# method with `21` sigma points for approximation of non-lineariety between hierarchy layers\nxt ~ GCV(xt_min, zt, real_k, real_w) where { meta = GCVMetadata(GaussHermiteCubature(21)) }","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The Meta object is useful to pass any extra information to a node that is not a random variable or constant model variable. It may include extra approximation methods, differentiation methods, optional non-linear functions, extra inference parameters etc.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#GraphPPL.jl-@meta-macro","page":"Advanced Tutorial","title":"GraphPPL.jl @meta macro","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Users can use @meta macro from the GraphPPL.jl package to achieve the same goal. Read more about @meta macro in the corresponding documentation section. Here is a simple example of the same meta specification:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@meta begin \n     AR(s, θ, γ) -> ARMeta(Multivariate, 5, ARsafe())\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Meta: \n  AR(s, θ, γ) -> ARMeta{Multivariate, ARsafe}(5, ARsafe())","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#Creating-custom-nodes-and-message-computation-rules","page":"Advanced Tutorial","title":"Creating custom nodes and message computation rules","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/#Custom-nodes","page":"Advanced Tutorial","title":"Custom nodes","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"To create a custom functional form and to make it available during model specification the ReactiveMP inference engine exports the @node macro:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# `@node` macro accepts a name of the functional form, its type, either `Stochastic` or `Deterministic` and an array of interfaces:\n@node NormalMeanVariance Stochastic [ out, μ, v ]\n\n# Interfaces may have aliases for their names that might be convenient for factorisation constraints specification\n@node NormalMeanVariance Stochastic [ out, (μ, aliases = [ mean ]), (v, aliases = [ var ]) ]\n\n# `NormalMeanVariance` structure declaration must exist, otherwise `@node` macro will throw an error\nstruct NormalMeanVariance end \n\n@node NormalMeanVariance Stochastic [ out, μ, v ]\n\n# It is also possible to use function objects as a node functional form\nfunction dot end\n\n# Syntax for functions is a bit differet, as it is necesssary to use `typeof(...)` function for them \n# out = dot(x, a)\n@node typeof(dot) Deterministic [ out, x, a ]","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"After that it is possible to use the newly created node during model specification:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model()\n    ...\n    y ~ dot(x, a)\n    ...\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#Custom-messages-computation-rules","page":"Advanced Tutorial","title":"Custom messages computation rules","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"RxInfer.jl exports the @rule macro to create custom message computation rules. For example let us create a simple + node to be available for usage in the model specification usage. We refer to A Factor Graph Approach to Signal Modelling , System Identification and Filtering [ Sascha Korl, 2005, page 32 ] for a rigorous explanation of the + node in factor graphs. According to Korl, assuming that inputs are Gaussian Sum-Product message computation rule for + node is the following:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"beginaligned\nmu_z = mu_x + mu_y\nV_z = V_x + V_y\nendaligned","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"To specify this in RxInfer.jl we use the @node and @rule macros:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@node typeof(+) Deterministic  [ z, x, y ]\n\n@rule typeof(+)(:z, Marginalisation) (m_x::UnivariateNormalDistributionsFamily, m_y::UnivariateNormalDistributionsFamily) = begin\n    x_mean, x_var = mean_var(m_x)\n    y_mean, y_var = mean_var(m_y)\n    return NormalMeanVariance(x_mean + y_mean, x_var + y_var)\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In this example, for the @rule macro, we specify a type of our functional form: typeof(+). Next, we specify an edge we are going to compute an outbound message for. Marginalisation indicates that the corresponding message respects the marginalisation constraint for posterior over corresponding edge:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"beginaligned\nq(z) = int q(z x y) mathrmdxmathrmdy\nendaligned","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"If we look on difference between sum-product rules and variational rules with mean-field assumption we notice that they require different local information to compute an outgoing message:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(Image: ) (Image: )","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"beginaligned\nmu(z) = int f(x y z)mu(x)mu(y)mathrmdxmathrmdy\nendaligned","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"beginaligned\nnu(z) = exp int log f(x y z)q(x)q(y)mathrmdxmathrmdy \nendaligned","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The @rule macro supports both cases with special prefixes during rule specification:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"m_ prefix corresponds to the incoming message on a specific edge\nq_ prefix corresponds to the posterior marginal of a specific edge","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Example of a Sum-Product rule with m_ messages used:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule NormalMeanPrecision(:μ, Marginalisation) (m_out::UnivariateNormalDistributionsFamily, m_τ::PointMass) = begin \n    m_out_mean, m_out_cov = mean_cov(m_out)\n    return NormalMeanPrecision(m_out_mean, inv(m_out_cov + inv(mean(m_τ))))\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Example of a Variational rule with Mean-Field assumption with q_ posteriors used:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule NormalMeanPrecision(:μ, Marginalisation) (q_out::Any, q_τ::Any) = begin \n    return NormalMeanPrecision(mean(q_out), mean(q_τ))\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"RxInfer.jl also supports structured rules. It is possible to obtain joint marginal over a set of edges:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule NormalMeanPrecision(:τ, Marginalisation) (q_out_μ::Any, ) = begin\n    m, V = mean_cov(q_out_μ)\n    θ = 2 / (V[1,1] - V[1,2] - V[2,1] + V[2,2] + abs2(m[1] - m[2]))\n    α = convert(typeof(θ), 1.5)\n    return Gamma(α, θ)\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"NOTE: In the @rule specification the messages or marginals arguments must be in order with interfaces specification from @node macro:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Inference backend expects arguments in `@rule` macro to be in the same order\n@node NormalMeanPrecision Stochastic [ out, μ, τ ]","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Any rule always has access to the meta information with hidden the meta::Any variable:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any) = begin \n    ...\n    println(meta)\n    ...\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"It is also possible to dispatch on a specific type of a meta object:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any, meta::LaplaceApproximation) = begin \n    ...\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"or","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any, meta::GaussHermiteCubature) = begin \n    ...\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#Customizing-messages-computational-pipeline","page":"Advanced Tutorial","title":"Customizing messages computational pipeline","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In certain situations it might be convenient to customize the default message computational pipeline. RxInfer.jl supports the pipeline keyword in the where { ... } clause to add some extra steps after a message has been computed. A use case might be an extra approximation method to preserve conjugacy in the model, debugging or simple printing.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"<img style=\"display: block;   margin-left: auto;   margin-right: auto;   width: 30%;\" src=\"./pics/pipeline.png\" width=\"20%\" />","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Logs all outbound messages\ny[i] ~ Normal(mean = x[i], precision = 1.0) where { pipeline = LoggerPipelineStage() }\n# In principle, it is possible to approximate outbound messages with Laplace Approximation (this is not an implemented feature, but a concept)\ny[i] ~ Normal(mean = x[i], precision = 1.0) where { pipeline = LaplaceApproximation() }","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Let us return to the coin toss model, but this time we want to print flowing messages:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function coin_toss_model_log(y)\n    θ ~ Beta(2.0, 7.0) where { pipeline = LoggerPipelineStage(\"θ\") }\n    for i in eachindex(y)\n        y[i] ~ Bernoulli(θ)  where { pipeline = LoggerPipelineStage(\"y[$i]\") }\n    end\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"dataset = float.(rand(Bernoulli(p), 5));\nresult = infer(\n    model = coin_toss_model_log(),\n    data  = (y = dataset, )\n)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"[θ][Beta][out]: DeferredMessage([ use `as_message` to compute the message ]\n)\n[y[1]][Bernoulli][p]: DeferredMessage([ use `as_message` to compute the mes\nsage ])\n[y[2]][Bernoulli][p]: DeferredMessage([ use `as_message` to compute the mes\nsage ])\n[y[3]][Bernoulli][p]: DeferredMessage([ use `as_message` to compute the mes\nsage ])\n[y[4]][Bernoulli][p]: DeferredMessage([ use `as_message` to compute the mes\nsage ])\n[y[5]][Bernoulli][p]: DeferredMessage([ use `as_message` to compute the mes\nsage ])\nInference results:\n  Posteriors       | available for (θ)","category":"page"},{"location":"manuals/inference/initialization/#initialization","page":"Initialization","title":"Understating why we need to initialize posteriors or messages in RxInfer","text":"","category":"section"},{"location":"manuals/inference/initialization/","page":"Initialization","title":"Initialization","text":"In certain models, after completing the model specification step and moving on to execute the inference procedure, you may encounter an error prompting you to initialize required marginals and messages. Understanding why this step is necessary can be perplexing. This tutorial is designed to explore the intuition behind model initialization using a practical example","category":"page"},{"location":"manuals/inference/initialization/","page":"Initialization","title":"Initialization","text":"@initialization","category":"page"},{"location":"manuals/inference/initialization/#RxInfer.@initialization","page":"Initialization","title":"RxInfer.@initialization","text":"@initialization\n\nMacro for specifying the initialization state of a model. Accepts either a function or a block of code. Allows the specification of initial messages and marginals that can be applied to a model in the infer function.\n\n\n\n\n\n","category":"macro"},{"location":"manuals/inference/initialization/#Part-1.-Framing-the-problem","page":"Initialization","title":"Part 1. Framing the problem","text":"","category":"section"},{"location":"manuals/inference/initialization/","page":"Initialization","title":"Initialization","text":"John has recently acquired a new car and is keenly interested in its fuel consumption rate. He holds the belief that this rate follows a linear relationship with the variable speed. To validate this hypothesis, he plans to conduct tests by driving his car on the urban roads close to his home, recording both the fuel consumption and speed data. To ascertain the fuel consumption rate, John has opted for Bayesian linear regression as his analytical method.","category":"page"},{"location":"manuals/inference/initialization/","page":"Initialization","title":"Initialization","text":"using Random, Plots, StableRNGs\n\nfunction generate_data(a, b, v, nr_samples; rng = StableRNG(1234))\n    x = float.(collect(1:nr_samples))\n    y = a .* x .+ b .+ randn(rng, nr_samples) .* sqrt(v)\n    return x, y\nend;\n\n# For demonstration purposes we generate some fake data \nx_data, y_data = generate_data(0.5, 25.0, 1.0, 250)\n\nscatter(x_data, y_data, title = \"Dataset (City road)\", legend=false)\nxlabel!(\"Speed\")\nylabel!(\"Fuel consumption\")","category":"page"},{"location":"manuals/inference/initialization/#Univariate-regression-with-known-noise","page":"Initialization","title":"Univariate regression with known noise","text":"","category":"section"},{"location":"manuals/inference/initialization/","page":"Initialization","title":"Initialization","text":"First, he drives the car on a urban road. John enjoys driving on the well-built, wide, and flat urban roads. Urban roads also offer the advantage of precise fuel consumption measurement with minimal noise. Therefore John models the fuel consumption y_ninmathbbR as a normal distribution and treats x_n as a fixed hyperparameter:","category":"page"},{"location":"manuals/inference/initialization/","page":"Initialization","title":"Initialization","text":"beginaligned\np(y_n mid a b) = mathcalN(y_n mid a x_n + b  1)\nendaligned","category":"page"},{"location":"manuals/inference/initialization/","page":"Initialization","title":"Initialization","text":"The recorded speed is denoted as x_n in mathbbR and the recorded fuel consumption as y_n in mathbbR. Prior beliefs on a and b are informed by the vehicle manual.","category":"page"},{"location":"manuals/inference/initialization/","page":"Initialization","title":"Initialization","text":"beginaligned\n    p(a) = mathcalN(a mid m_a v_a) \n    p(b) = mathcalN(b mid m_b v_b) \nendaligned","category":"page"},{"location":"manuals/inference/initialization/","page":"Initialization","title":"Initialization","text":"Together they form the probabilistic model p(y a b) = p(a)p(b) prod_N=1^N p(y_n mid a b) where the goal is to infer the posterior distributions p(a mid y) and p(bmid y).","category":"page"},{"location":"manuals/inference/initialization/","page":"Initialization","title":"Initialization","text":"In order to estimate the two parameters with the recorded data, he uses a RxInfer.jl to create the above described model.","category":"page"},{"location":"manuals/inference/initialization/","page":"Initialization","title":"Initialization","text":"using RxInfer\n\n@model function linear_regression(y, x)\n    a  ~ Normal(mean = 0.0, variance = 1.0)\n    b  ~ Normal(mean = 0.0, variance = 100.0)\n    y .~ Normal(mean = a .* x .+ b, variance = 1.0)\nend","category":"page"},{"location":"manuals/inference/initialization/","page":"Initialization","title":"Initialization","text":"Delighted with the convenience offered by the package's inference function (infer), he appreciates the time saved from building everything from the ground up. This feature allows him to effortlessly obtain the desired results for his specific road. Upon consulting the documentation, he proceeds to run the inference function.","category":"page"},{"location":"manuals/inference/initialization/","page":"Initialization","title":"Initialization","text":"results = infer(\n    model        = linear_regression(), \n    data         = (y = y_data, x = x_data), \n    returnvars   = (a = KeepLast(), b = KeepLast()),\n    iterations   = 20,\n    free_energy  = true\n)","category":"page"},{"location":"manuals/inference/initialization/","page":"Initialization","title":"Initialization","text":"Oeps! Exception?","category":"page"},{"location":"manuals/inference/initialization/","page":"Initialization","title":"Initialization","text":"exception =\n│    Variables [ a, b ] have not been updated after an update event. \n│    Therefore, make sure to initialize all required marginals and messages. See `initialization` keyword argument for the inference function. \n│    See the function documentation for detailed information regarding the initialization.","category":"page"},{"location":"manuals/inference/initialization/","page":"Initialization","title":"Initialization","text":"After running the inference procedure an error appears, which prompts him to initialize all required messages and marginals. Now, John is left pondering the reason behind this requirement. Why is it necessary? Should he indeed initialize all messages and marginals? And if so, how might this impact the inference procedure?","category":"page"},{"location":"manuals/inference/initialization/#Part-2.-Why-and-What-to-initialize","page":"Initialization","title":"Part 2. Why and What to initialize","text":"","category":"section"},{"location":"manuals/inference/initialization/","page":"Initialization","title":"Initialization","text":"Before delving too deeply into the details, it's important to understand that RxInfer constructs a factorized representation of your model using a Forney Style Factor Graph (FFG). In this structure, inference is executed through message passing.","category":"page"},{"location":"manuals/inference/initialization/","page":"Initialization","title":"Initialization","text":"A challenge arises when RxInfer generates the FFG representation with structural loops in certain parts of the graph. These loops indicate that a message or marginal within the loop depends not only on its prior but also on itself. Consequently, proper initialization is crucial for initiating the inference process. Two general rules of thumb guide this initialization, although the intricate details are beyond the scope of this tutorial:","category":"page"},{"location":"manuals/inference/initialization/","page":"Initialization","title":"Initialization","text":"1.\tInitiate as few messages/marginals as possible when dealing with a loop structure, it will be more efficient and accurate. 2.\tPrioritize initializing marginals over messages.","category":"page"},{"location":"manuals/inference/initialization/","page":"Initialization","title":"Initialization","text":"How to identify and handle the loops?","category":"page"},{"location":"manuals/inference/initialization/","page":"Initialization","title":"Initialization","text":"Identifying loops is currently a manual process, as the current version of RxInfer doesn't support a graphical representation of the created graph. As such, the manual process involves:","category":"page"},{"location":"manuals/inference/initialization/","page":"Initialization","title":"Initialization","text":"1.\tDeriving the graphical representation of the model, 2.\tIdentifying loops and the messages or marginals that need to be initialized within the loop.","category":"page"},{"location":"manuals/inference/initialization/","page":"Initialization","title":"Initialization","text":"However, once you receive the message Variables [x, y, z] have not been updated after an update event, it is a good indication that there is a loop in your model. If you see this message, you should check your model for loops and try to initialize the messages and/or marginals that are part of the loop.","category":"page"},{"location":"manuals/inference/initialization/#Deriving-FFG-and-identifying-the-loops","page":"Initialization","title":"Deriving FFG and identifying the loops","text":"","category":"section"},{"location":"manuals/inference/initialization/","page":"Initialization","title":"Initialization","text":"John proceeds to derive the FFG for his problem where he identifies where the loops are:","category":"page"},{"location":"manuals/inference/initialization/","page":"Initialization","title":"Initialization","text":"(Image: Addons_messages)","category":"page"},{"location":"manuals/inference/initialization/","page":"Initialization","title":"Initialization","text":"He does note that there is a loop in his model, namely all a and b variables are connected over all observations, therefore he needs to initialize one of the messages and run multiple iterations for the loopy belief propagation algorithm. It is worth noting that loopy belief propagation is not guaranteed to converge in general and might be highly influenced by the choice of the initial messages in the initialization argument. He is going to evaluate the convergency performance of the algorithm with the free_energy = true option:","category":"page"},{"location":"manuals/inference/initialization/","page":"Initialization","title":"Initialization","text":"init = @initialization begin\n    μ(b) = NormalMeanVariance(0.0, 100.0)\nend\n\nresults = infer(\n    model           = linear_regression(), \n    data            = (y = y_data, x = x_data), \n    initialization  = init, \n    returnvars      = (a = KeepLast(), b = KeepLast()),\n    iterations      = 20,\n    free_energy     = true\n)\n\n# drop first iteration, which is influenced by the `initmessages`\nplot(2:20, results.free_energy[2:end], title=\"Free energy\", xlabel=\"Iteration\", ylabel=\"Free energy [nats]\", legend=false)","category":"page"},{"location":"manuals/inference/initialization/","page":"Initialization","title":"Initialization","text":"Now the inference runs without the error! 🎉","category":"page"},{"location":"manuals/inference/initialization/","page":"Initialization","title":"Initialization","text":"as = rand(results.posteriors[:a], 100)\nbs = rand(results.posteriors[:b], 100)\np = scatter(x_data, y_data, title = \"Linear regression with more noise\", legend=false)\nxlabel!(\"Speed\")\nylabel!(\"Fuel consumption\")\nfor (a, b) in zip(as, bs)\n    global p = plot!(p, x_data, a .* x_data .+ b, alpha = 0.05, color = :red)\nend\nplot(p, size = (900, 400))","category":"page"},{"location":"manuals/inference/initialization/#Implementation-details","page":"Initialization","title":"Implementation details","text":"","category":"section"},{"location":"manuals/inference/initialization/","page":"Initialization","title":"Initialization","text":"RxInfer.InitializationPlugin\nRxInfer.convert_init_object","category":"page"},{"location":"manuals/inference/initialization/#RxInfer.InitializationPlugin","page":"Initialization","title":"RxInfer.InitializationPlugin","text":"MetaPlugin(init)\n\nA plugin that adds a init information to the factor nodes of the model.\n\n\n\n\n\n","category":"type"},{"location":"manuals/inference/initialization/#RxInfer.convert_init_object","page":"Initialization","title":"RxInfer.convert_init_object","text":"convert_init_object(e::Expr)\n\nConverts a variable init or a factor init call on the left hand side of a init specification to a GraphPPL.MetaObject.\n\nArguments\n\ne::Expr: The expression to convert.\n\nReturns\n\nExpr: The resulting expression with the variable reference or factor function call converted to a GraphPPL.MetaObject.\n\nExamples\n\n\n\n\n\n","category":"function"},{"location":"contributing/guide/#contributing-overview","page":"Contribution guide","title":"Contributing","text":"","category":"section"},{"location":"contributing/guide/","page":"Contribution guide","title":"Contribution guide","text":"Welcome to the contribution guide for RxInfer.jl. Here you'll find information on the RxInfer project structure, and how to get started with contributing to the project. For more practical instructions and guidelines, refer to the contribution guidelines.","category":"page"},{"location":"contributing/guide/#Project-structure","page":"Contribution guide","title":"Project structure","text":"","category":"section"},{"location":"contributing/guide/","page":"Contribution guide","title":"Contribution guide","text":"RxInfer.jl is a Julia package that provides a high-level interface for probabilistic programming. It is composed of three major core dependencies:","category":"page"},{"location":"contributing/guide/","page":"Contribution guide","title":"Contribution guide","text":"Rocket.jl: A package for reactive programming, allowing asynchronous data processing\nGraphPPL.jl: A domain-specific language for probabilistic programming, facilitating the @model macro and other crucial user-facing features.\nReactiveMP.jl: Reactive message passing engine, using Rocket.jl to pass messages between nodes in a probabilistic model defined with GraphPPL.jl.","category":"page"},{"location":"contributing/guide/","page":"Contribution guide","title":"Contribution guide","text":"In general, non-inference related functionality is implemented in Rocket.jl and GraphPPL.jl, while inference-related functionality is implemented in ReactiveMP.jl. For example, all factor nodes and inference rules for messages are implemented in ReactiveMP.jl.","category":"page"},{"location":"contributing/guide/#Getting-started","page":"Contribution guide","title":"Getting started","text":"","category":"section"},{"location":"contributing/guide/","page":"Contribution guide","title":"Contribution guide","text":"To familiarize yourself with development in RxInfer, we recommend the following steps:","category":"page"},{"location":"contributing/guide/","page":"Contribution guide","title":"Contribution guide","text":"Familiarize yourself with the collaborative tools used in the project. RxInfer uses GitHub for version control, issue tracking, and pull requests. We aim to maintain the good first issue label on issues that are suitable for new contributors. Furthermore, the core development team tracks the project's progress and development tasks on the project board. Because the project board is overwhelming, we recommend focusing first on issues labeled with the good first issue label. \nRead the contribution guidelines to understand the contribution process and best practices for contributing to RxInfer, as well as coding practices and testing procedures.\nFamiliarize yourself with the RxInfer codebase and its core dependencies. While most information can be found on the RxInfer documentation page, it is also recommended to read the documentation for Rocket.jl, GraphPPL.jl, and ReactiveMP.jl to understand the core functionality and design principles of the project.\nPick an issue to work on. We recommend starting with a good first issue to familiarize yourself with the contribution process. Once you're comfortable with the process, you can move on to more complex issues.","category":"page"},{"location":"contributing/guide/#Contribution-guidelines","page":"Contribution guide","title":"Contribution guidelines","text":"","category":"section"},{"location":"contributing/guide/","page":"Contribution guide","title":"Contribution guide","text":"The contribution guidelines provide detailed instructions on how to contribute effectively to the project. They cover reporting bugs, suggesting features, and contributing code. For more information, refer to the contribution guidelines.","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/#examples-simple-nonlinear-node","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"","category":"section"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"using RxInfer, Random, StableRNGs","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"Here is an example of creating custom node with nonlinear function approximation with samplelist.","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/#Custom-node-creation","page":"Simple Nonlinear Node","title":"Custom node creation","text":"","category":"section"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"struct NonlinearNode end # Dummy structure just to make Julia happy\n\nstruct NonlinearMeta{R, F}\n    rng      :: R\n    fn       :: F   # Nonlinear function, we assume 1 float input - 1 float output\n    nsamples :: Int # Number of samples used in approximation\nend","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"@node NonlinearNode Deterministic [ out, in ]","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"We need to define two Sum-product message computation rules for our new custom node","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"Rule for outbound message on out edge given inbound message on in edge\nRule for outbound message on in edge given inbound message on out edge\nBoth rules accept optional meta object","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"# Rule for outbound message on `out` edge given inbound message on `in` edge\n@rule NonlinearNode(:out, Marginalisation) (m_in::NormalMeanVariance, meta::NonlinearMeta) = begin \n    samples = rand(meta.rng, m_in, meta.nsamples)\n    return SampleList(map(meta.fn, samples))\nend","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"# Rule for outbound message on `in` edge given inbound message on `out` edge\n@rule NonlinearNode(:in, Marginalisation) (m_out::Gamma, meta::NonlinearMeta) = begin     \n    return ContinuousUnivariateLogPdf((x) -> logpdf(m_out, meta.fn(x)))\nend","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/#Model-specification","page":"Simple Nonlinear Node","title":"Model specification","text":"","category":"section"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"After we have defined our custom node with custom rules we may proceed with a model specification:","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"beginaligned\np(theta) = mathcalN(thetamu_theta sigma_theta)\np(m) = mathcalN(thetamu_m sigma_m)\np(w) = f(theta)\np(y_im w) = mathcalN(y_im w)\nendaligned","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"Given this IID model, we aim to estimate the precision of a Gaussian distribution. We pass a random variable theta through a non-linear transformation f to make it positive and suitable for a precision parameter of a Gaussian distribution. We, later on, will estimate the posterior of theta. ","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"@model function nonlinear_estimation(y, θ_μ, m_μ, θ_σ, m_σ)\n    \n    # define a distribution for the two variables\n    θ ~ Normal(mean = θ_μ, variance = θ_σ)\n    m ~ Normal(mean = m_μ, variance = m_σ)\n\n    # define a nonlinear node\n    w ~ NonlinearNode(θ)\n\n    # We consider the outcome to be normally distributed\n    for i in eachindex(y)\n        y[i] ~ Normal(mean = m, precision = w)\n    end\n    \nend","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"@constraints function nconstsraints(nsamples)\n    q(θ) :: SampleListFormConstraint(nsamples, LeftProposal())\n    q(w) :: SampleListFormConstraint(nsamples, RightProposal())\n    \n    q(θ, w, m) = q(θ)q(m)q(w)\nend","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"nconstsraints (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"@meta function nmeta(fn, nsamples)\n    NonlinearNode(θ, w) -> NonlinearMeta(StableRNG(123), fn, nsamples)\nend","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"nmeta (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"@initialization function ninit()\n    q(m) = vague(NormalMeanPrecision)\n    q(w) = vague(Gamma)\nend","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"ninit (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"Here we generate some data","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"nonlinear_fn(x) = abs(exp(x) * sin(x))","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"nonlinear_fn (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"seed = 123\nrng  = StableRNG(seed)\n\nniters   = 15 # Number of VMP iterations\nnsamples = 5_000 # Number of samples in approximation\n\nn = 500 # Number of IID samples\nμ = -10.0\nθ = -1.0\nw = nonlinear_fn(θ)\n\ndata = rand(rng, NormalMeanPrecision(μ, w), n);","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"now that synthetic data/constriants/model is defined, lets infer:","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"result = infer(\n    model = nonlinear_estimation(θ_μ = 0.0, m_μ = 0.0, θ_σ=100.0, m_σ=1.0),\n    meta =  nmeta(nonlinear_fn, nsamples),\n    constraints = nconstsraints(nsamples),\n    data = (y = data, ), \n    initialization = ninit(),\n    returnvars = (θ = KeepLast(), ),\n    iterations = niters,  \n    showprogress = true\n)","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"Inference results:\n  Posteriors       | available for (θ)","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"we can check the posterior now","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"θposterior = result.posteriors[:θ]","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"SampleList(Univariate, 5000)","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"Let's us visualise the results","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"using Plots, StatsPlots\n\nestimated = Normal(mean_std(θposterior)...)\n\nplot(estimated, title=\"Posterior for θ\", label = \"Estimated\", legend = :bottomright, fill = true, fillopacity = 0.2, xlim = (-3, 3), ylim = (0, 2))\nvline!([ θ ], label = \"Real value of θ\")","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"(Image: )","category":"page"},{"location":"contributing/external-examples/#external-examples","page":"External examples","title":"External Examples","text":"","category":"section"},{"location":"contributing/external-examples/","page":"External examples","title":"External examples","text":"RxInfer.jl welcomes contributions of external examples that demonstrate the use of the package across a variety of probabilistic models.","category":"page"},{"location":"contributing/external-examples/#Featured-Examples","page":"External examples","title":"Featured Examples","text":"","category":"section"},{"location":"contributing/external-examples/","page":"External examples","title":"External examples","text":"Active Inference with RxInfer.jl - Dive into the realm of Active Inference guided by Kobus Esterhuysen at Learnable Loop.\nTutorial Series on RxInfer.jl - Explore a series of engaging tutorial videos on RxInfer.jl's functionalities, presented by @doggotodjl.","category":"page"},{"location":"contributing/external-examples/","page":"External examples","title":"External examples","text":"note: Note\nIf you're interested in contributing an external example, we'd love to hear from you! Please initiate an issue or start a new discussion on our GitHub repository to get involved.","category":"page"},{"location":"library/exported-methods/#lib-using-methods","page":"Exported methods","title":"Using methods from RxInfer","text":"","category":"section"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"In the Julia programming language (in contrast to Python for example) the most common way of loading a module is:","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"using RxInfer","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"A nice explanation about how modules/packages work in Julia can be found in the official documentation.","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"In a nutshell, Julia automatically resolves all name collisions and there is no a lot of benefit of importing specific names, e.g.:","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"import RxInfer: mean","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"One of the reasons for that is that Julia uses multiple-dispatch capabilities to merge names automatically and will indicate (with a warning) if something went wrong or names have unresolvable collisions on types. As a small example of this feature consider the following small import example:","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"import RxInfer: mean as mean_from_rxinfer\nimport Distributions: mean as mean_from_distributions\n\nmean_from_rxinfer === mean_from_distributions","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"Even though we import mean function from two different packages they actually refer to the same object. Worth noting that this is not always the case - Julia will print a warning in case it finds unresolvable conflicts and usage of such functions will be disallowed unless user import them specifically. Read more about this in the section of the Julia's documentation.","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"# It is easier to let Julia resolve names automatically\n# Julia will not overwrite `mean` that is coming from both packages\nusing RxInfer, Distributions ","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"mean(Normal(0.0, 1.0)) # `Normal` is an object from `Distributions.jl`","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"mean(NormalMeanVariance(0.0, 1.0)) # `NormalMeanVariance` is an object from `RxInfer.jl`","category":"page"},{"location":"library/exported-methods/#lib-list-methods","page":"Exported methods","title":"List of available methods","text":"","category":"section"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"Below you can find a list of exported methods from RxInfer.jl. All methods (even private) can be always accessed with RxInfer. prefix, e.g RxInfer.mean.","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"note: Note\nSome exported names are (for legacy reasons) intended for private usage only. As a result some of these methods do not have a proper associated documentation with them. We constantly improve RxInfer.jl library and continue to add better documentation for many exported methods, but a small portion of these methods could be removed from this list in the future.","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"using RxInfer #hide\nforeach(println, names(RxInfer))","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/#examples-nonlinear-sensor-fusion","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"","category":"section"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"using RxInfer, Random, LinearAlgebra, Distributions, Plots, StatsPlots, Optimisers\nusing DataFrames, DelimitedFiles, StableRNGs","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"In a secret ongoing mission to Mars, NASA has deployed its custom lunar roving vehicle, called WALL-E, to explore the area and to discover hidden minerals. During one of the solar storm, WALL-E's GPS unit got damaged, preventing it from accurately locating itself. The engineers at NASA were devastated as they developed the project over the past couple of years and spend most of their funding on it. Without being able to locate WALL-E, they were unable to complete their mission.","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"A smart group of engineers came up with a solution to locate WALL-E. They decided to repurpose 3 nearby satelites as beacons for WALL-E, allowing it to detect its relative location to these beacons. However, these satelites were old and therefore WALL-E was only able to obtain noisy estimates of its distance to these beacons. These distances were communicated back to earth, where the engineers tried to figure our WALL-E's location. Luckily they knew the locations of these satelites and together with the noisy estimates of the distance to WALL-E they can infer the exact location of the moving WALL-E.","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"To illustrate these noisy measurements, the engineers decided to plot them:","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"# fetch measurements\nbeacon_locations = readdlm(\"../data/sensor_fusion/beacons.txt\")\ndistances = readdlm(\"../data/sensor_fusion/distances.txt\")\nposition = readdlm(\"../data/sensor_fusion/position.txt\")\nnr_observations = size(distances, 1);","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"# plot beacon and actual location of WALL-E\np1 = scatter(beacon_locations[:,1], beacon_locations[:,2], markershape=:utriangle, markersize=10, legend=:topleft, label=\"beacon locations\")\nplot!(position[1,:], position[2,:], label=\"actual location\", linewidth=3, linestyle=:dash, arrow=(:closed, 2.0), aspect_ratio=1.0)\nxlabel!(\"longitude [m]\"), ylabel!(\"latitude [m]\")\n\n# plot noisy distance measurements\np2 = plot(distances, legend=:topleft, linewidth=3, label=[\"distance to beacon 1\" \"distance to beacon 2\" \"distance to beacon 3\"])\nxlabel!(\"time [sec]\"), ylabel!(\"distance [m]\")\n\nplot(p1, p2, size=(1200, 500))","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"In order to track the location of WALL-E based on the noisy distance measurements to the beacon, the engineers developed a probabilistic model for the movements for WALL-E and the distance measurements that followed from this. The engineers assumed that the position of WALL-E at time t, denoted by z_t, follows a 2-dimensional normal random walk:","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"beginaligned\n  p(z_t mid z_t - 1) = mathcalN(z_t mid z_t-1mathrmI_2)\nendaligned","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"where mathrmI_2 denotes the 2-dimensional identity matrix. From the current position of WALL-E, we specify our noisy distance measurements y_t as a noisy set of the distances between WALL-E and the beacons, specified by s_i:","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"beginaligned\n  p(y_t mid z_t)  = mathcalN left (y_t left vert beginbmatrix  z_t - s_1  z_t - s_2  z_t - s_3endbmatrixmathrmI_3 right  right)\nendaligned","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"The engineers are smart enough to automate the probabilistic inference procedure using RxInfer.jl. They specify the probabilistic model as:","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"# function to compute distance to beacons\nfunction compute_distances(z)    \n    distance1 = norm(z - beacon_locations[1,:])\n    distance2 = norm(z - beacon_locations[2,:])\n    distance3 = norm(z - beacon_locations[3,:])\n    distances = [distance1, distance2, distance3]\nend;","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"@model function random_walk_model(y, W, R)\n    # specify initial estimates of the location\n    z[1] ~ MvNormalMeanCovariance(zeros(2), diageye(2)) \n    y[1] ~ MvNormalMeanCovariance(compute_distances(z[1]), diageye(3))\n\n    # loop over time steps\n    for t in 2:length(y)\n\n        # specify random walk state transition model\n        z[t] ~ MvNormalMeanPrecision(z[t-1], W)\n\n        # specify non-linear distance observations model\n        y[t] ~ MvNormalMeanPrecision(compute_distances(z[t]), R)\n        \n    end\n\nend;","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"Due to non-linearity, exact probabilistic inference is intractable in this model. Therefore we resort to Conjugate-Computational Variational Inference (CVI) following the paper Probabilistic programming with stochastic variational message passing. This requires setting the @meta macro in RxInfer.jl.","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"Please note that we permit improper messages within the CVI procedure in this example by providing Val(false) to CVI constructor:","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"    compute_distances(z) -> CVI(..., Val(false), ...)","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"This move may lead to numerical instabilities in other scenarios, however dissallowing improper messages in this case can lead to a biased estimates of posterior distribution. So, as a rule of thumb, you should try the default setting, and if it fails to find an unbiased result, enable improper messages.","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"@meta function random_walk_model_meta(nr_samples, nr_iterations, rng)\n    compute_distances(z) -> CVI(rng, nr_samples, nr_iterations, Optimisers.Descent(0.1), ForwardDiffGrad(), 1, Val(false), false)\nend;","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"NOTE: You can try out different meta for approximating the nonlinearity, e.g.","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"@meta function random_walk_linear_meta()\n    compute_distances(z) -> Linearization()\nend;","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"@meta function random_walk_unscented_meta()\n    compute_distances(z) -> Unscented()\nend;","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"init = @initialization begin \n    μ(z) = MvNormalMeanPrecision(ones(2), 0.1 * diageye(2))\nend","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"Initial state: \n  μ(z) = MvNormalMeanPrecision(\nμ: [1.0, 1.0]\nΛ: [0.1 0.0; 0.0 0.1]\n)","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"results_fast = infer(\n    model = random_walk_model(W = diageye(2), R = diageye(3)),\n    meta = random_walk_model_meta(1, 3, StableRNG(42)), # or random_walk_unscented_meta()\n    data = (y = [distances[t,:] for t in 1:nr_observations],),\n    iterations = 20,\n    free_energy = false,\n    returnvars = (z = KeepLast(),),\n    initialization = init,\n);","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"results_accuracy = infer(\n    model = random_walk_model(W = diageye(2), R = diageye(3)),\n    meta = random_walk_model_meta(1000, 100, StableRNG(42)),\n    data = (y = [distances[t,:] for t in 1:nr_observations],),\n    iterations = 20,\n    free_energy = false,\n    returnvars = (z = KeepLast(),),\n    initialization = init,\n);","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"After running this fast inference procedure, the engineers plot the results and evaluate the performance:","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"# plot beacon and actual and estimated location of WALL-E (fast inference)\np1 = scatter(beacon_locations[:,1], beacon_locations[:,2], markershape=:utriangle, markersize=10, legend=:topleft, label=\"beacon locations\")\nplot!(position[1,:], position[2,:], label=\"actual location\", linewidth=3, linestyle=:dash, arrow=(:closed, 2.0), aspect_ratio=1.0)\nmap(posterior -> covellipse!(mean(posterior), cov(posterior), color=\"red\", label=\"\", n_std=2), results_fast.posteriors[:z])\nxlabel!(\"longitude [m]\"), ylabel!(\"latitude [m]\"), title!(\"Fast (1 sample, 3 iterations)\"); p1.series_list[end][:label] = \"estimated location ±2σ\"\n\n# plot beacon and actual and estimated location of WALL-E (accurate inference)\np2 = scatter(beacon_locations[:,1], beacon_locations[:,2], markershape=:utriangle, markersize=10, legend=:topleft, label=\"beacon locations\")\nplot!(position[1,:], position[2,:], label=\"actual location\", linewidth=3, linestyle=:dash, arrow=(:closed, 2.0), aspect_ratio=1.0)\nmap(posterior -> covellipse!(mean(posterior), cov(posterior), color=\"red\", label=\"\", n_std=2), results_accuracy.posteriors[:z])\nxlabel!(\"longitude [m]\"), ylabel!(\"latitude [m]\"), title!(\"Accurate (1000 samples, 100 iterations)\"); p2.series_list[end][:label] = \"estimated location ±2σ\"\n\nplot(p1, p2, size=(1200, 500))","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"The engineers were very happy with the solution, as it meant that the Mars mission could continue. However, they noted that the estimates began to deviate after WALL-E moved further away from the beacons. They deemed this was likely due to the noise in the distance measurements. Therefore, the engineers decided to adapt the model, such that they would also infer the process and observation noise precision matrices, Q and R respectively. They did this by adding Wishart priors to those matrices:","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"beginaligned\n  p(Q) = mathcalW(Q mid 3 mathrmI_2) \n  p(R) = mathcalW(R mid 4 mathrmI_3) \n  p(z_t mid z_t - 1 Q) = mathcalN(z_t mid z_t-1 Q^-1)\n  p(y_t mid z_t R)  = mathcalN left (y_t left vert beginbmatrix  z_t - s_1  z_t - s_2  z_t - s_3endbmatrixR^-1 right  right)\nendaligned","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"@model function random_walk_model_wishart(y)\n    # set priors on precision matrices\n    Q ~ Wishart(3, diageye(2))\n    R ~ Wishart(4, diageye(3))\n\n    # specify initial estimates of the location\n    z[1] ~ MvNormalMeanCovariance(zeros(2), diageye(2)) \n    y[1] ~ MvNormalMeanCovariance(compute_distances(z[1]), diageye(3))\n\n    # loop over time steps\n    for t in 2:length(y)\n\n        # specify random walk state transition model\n        z[t] ~ MvNormalMeanPrecision(z[t-1], Q)\n\n        # specify non-linear distance observations model\n        y[t] ~ MvNormalMeanPrecision(compute_distances(z[t]), R)\n        \n    end\n\nend;","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"meta = @meta begin \n    compute_distances(z) -> CVI(StableRNG(42), 1000, 100, Optimisers.Descent(0.01), ForwardDiffGrad(), 1, Val(false), false)\nend;","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"Because of the added complexity with the Wishart distributions, the engineers simplify the problem by employing a structured mean-field factorization:","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"constraints = @constraints begin\n    q(z, Q, R) = q(z)q(Q)q(R)\nend;","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"init = @initialization begin \n    μ(z) = MvNormalMeanPrecision(zeros(2), 0.01 * diageye(2))\n    q(R) = Wishart(4, diageye(3))\n    q(Q) = Wishart(3, diageye(2))\nend;","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"The engineers run the inference procedure again and decide to track the inference performance using the Bethe free energy.","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"results_wishart = infer(\n    model = random_walk_model_wishart(),\n    data = (y = [distances[t,:] for t in 1:nr_observations],),\n    iterations = 20,\n    free_energy = true,\n    returnvars = (z = KeepLast(),),\n    constraints = constraints,\n    meta = meta,\n    initialization = init,\n);","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"They plot the new estimates and the performance over time, and luckily WALL-E is found!","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"# plot beacon and actual and estimated location of WALL-E (fast inference)\np1 = scatter(beacon_locations[:,1], beacon_locations[:,2], markershape=:utriangle, markersize=10, legend=:topleft, label=\"beacon locations\")\nplot!(position[1,:], position[2,:], label=\"actual location\", linewidth=3, linestyle=:dash, arrow=(:closed, 2.0), aspect_ratio=1.0)\nmap(posterior -> covellipse!(mean(posterior), cov(posterior), color=\"red\", label=\"\", n_std=2), results_wishart.posteriors[:z])\nxlabel!(\"longitude [m]\"), ylabel!(\"latitude [m]\"); p1.series_list[end][:label] = \"estimated location ±2σ\"\n\n# plot bethe free energy performance\np2 = plot(results_wishart.free_energy[2:end], label = \"\")\nxlabel!(\"iteration\"), ylabel!(\"Bethe free energy [nats]\")\n\nplot(p1, p2, size=(1200, 500))","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/overview/#examples-problem_specific-overview","page":"Overview","title":"Problem specific","text":"","category":"section"},{"location":"examples/problem_specific/overview/","page":"Overview","title":"Overview","text":"This section contains a set of examples for Bayesian Inference with RxInfer package in various probabilistic models.","category":"page"},{"location":"examples/problem_specific/overview/","page":"Overview","title":"Overview","text":"note: Note\nAll examples have been pre-generated automatically from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/problem_specific/overview/","page":"Overview","title":"Overview","text":"Problem specific examples contain specialized models and inference for various domains.","category":"page"},{"location":"examples/problem_specific/overview/","page":"Overview","title":"Overview","text":"Autoregressive Models: An example of Bayesian treatment of latent AR and ARMA models. Reference: Albert Podusenko, Message Passing-Based Inference for Time-Varying Autoregressive Models.\nGamma Mixture Model: This example implements one of the Gamma mixture experiments outlined in https://biaslab.github.io/publication/mp-based-inference-in-gmm/ .\nGaussian Mixture: This example implements variational Bayesian inference in univariate and multivariate Gaussian mixture models with mean-field assumption.\nHierarchical Gaussian Filter: An example of online inference procedure for Hierarchical Gaussian Filter with univariate noisy observations using Variational Message Passing algorithm. Reference: Ismail Senoz, Online Message Passing-based Inference in the Hierarchical Gaussian Filter.\nInvertible neural networks: a tutorial: An example of variational Bayesian Inference with invertible neural networks. Reference: Bart van Erp, Hybrid Inference with Invertible Neural Networks in Factor Graphs.\nProbit Model (EP): In this demo we illustrate EP in the context of state-estimation in a linear state-space model that combines a Gaussian state-evolution model with a discrete observation model.\nRTS vs BIFM Smoothing: This example performs BIFM Kalman smoother on a factor graph using message passing and compares it with the RTS implementation.\nSimple Nonlinear Node: In this example we create a non-conjugate model and use a nonlinear link function between variables. We show how to extend the functionality of RxInfer and to create a custom factor node with arbitrary message passing update rules.\nUniversal Mixtures: Universal mixture modeling.","category":"page"},{"location":"manuals/customization/custom-node/#create-node","page":"Defining a custom node and rules","title":"Creating your own custom nodes","text":"","category":"section"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Welcome to the RxInfer documentation on creating custom factor graph nodes. In RxInfer, factor nodes represent functional relationships between variables, also known as factors. Together, these factors define your probabilistic model. Quite often these factors represent distributions, denoting how a certain parameter affects another. However, other factors are also possible, such as ones specifying linear or non-linear relationships. RxInfer already supports a lot of factor nodes, however, depending on the problem that you are trying to solve, you may need to create a custom node that better fits the specific requirements of your model. This tutorial will guide you through the process of defining a custom node in RxInfer, step by step. By the end of this tutorial, you will be able to create your own custom node and integrate it into your model.","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"To create a custom node in RxInfer, 4 steps are required:","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Create your custom node in RxInfer using the @node macro.\nDefine the corresponding message passing update rules with the @rule macro. These rules specify how the node processes information in the form of messages, and how it communicates the results to adjacent parts of the model.\nSpecify computations for marginal distributions of the relevant variables with the @marginalrule macro.\nImplement the computation of the Free Energy in a node with the @average_energy macro.","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Throughout this tutorial, we will create a node for the Bernoulli distribution. The Bernoulli distribution is a commonly used distribution in statistical modeling that is often used to model a binary outcome, such as a coin flip. By recreating this node, we will be able to demonstrate the process of creating a custom node, from notifying RxInfer of the nodes existence to implementing the required methods. While this tutorial focuses on the Bernoulli distribution, the principles can be applied to creating custom nodes for other distributions as well. So let's get started!","category":"page"},{"location":"manuals/customization/custom-node/#Problem-statement","page":"Defining a custom node and rules","title":"Problem statement","text":"","category":"section"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Jane wants to determine whether a coin is a fair coin, meaning that is equally likely to land on heads or tails. In order to determine this, she will throw the coin K=20 times and write down how often it lands on heads and tails. The result of this experiment is a realization of the underlying stochastic process. Jane models the outcome of the experiment x_kin01 using the Bernoulli distribution as","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"p(x_k mid pi) = mathrmBer(x_k mid pi) = pi^x_k (1-pi)^1-x_k","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"where pi in01 denotes the probability that she throws heads, also known as the success probability. Jane also has a prior belief (initial guess) about the value of pi which she models using the Beta distribution as","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"p(pi) = mathrmBeta(pi mid 4 8)","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"With this prior belief, the total probabilistic model that she has for this experiment is given by","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"p(x_1K pi) = p(pi) prod_k=1^K p(x_k mid pi)","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Jane is interested in determining the fairness of the coin. Therefore she aims to infer (calculate) the posterior belief of pi, p(pi mid x_1K), denoting how pi is distributed after we have seen the data.","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"","category":"page"},{"location":"manuals/customization/custom-node/#Step-1:-Creating-the-custom-node","page":"Defining a custom node and rules","title":"Step 1: Creating the custom node","text":"","category":"section"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"note: Note\nIn this example we will assume that the Bernoulli node and distribution do not yet exist. The RxInfer already defines the node for the Bernoulli distribution from the Distributions.jl package.","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"First things first, let's import RxInfer:","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"using RxInfer","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"In order to define a custom node using the @node macro from ReactiveMP, we need the following three arguments:","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"The name of the node.\nWhether the node is Deterministic or Stochastic.\nThe interfaces of the node and any potential aliases.","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"For the name of the node we wish to use MyBernoulli in this tutorial (Bernoulli already exists). However, the corresponding distribution does not yet exist. Therefore we need to specify it first as","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"# struct for Bernoulli distribution with success probability π\nstruct MyBernoulli{T <: Real} <: ContinuousUnivariateDistribution\n    π :: T\nend\n\n# for simplicity, let's also specify the mean of the distribution\nDistributions.mean(d::MyBernoulli) = d.π\n\nnothing # hide","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"note: Note\nYou can use regular functions, e.g + as a node type. Their Julia type, however, is written with the typeof(_) specification, e.g. typeof(+)","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"For our node we are dealing with a stochastic node, because the node forms a probabilistic relationship. This means that for a given value of pi, we do know the corresponding value of the output, but we do have some belief about this. Deterministic nodes include for example linear and non-linear transformation, such as + or *.","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"The interfaces specify what variables are connected to the node. The first argument is its output by convention. The ordering is important for both the model specification as the rule definition. As an example consider the NormalMeanVariance factor node. This factor node has interfaces [out, μ, v] and can be called in the model specification language as x ~ NormalMeanVariance(μ, v). It is also possible to use aliases for the interfaces, which can be specified in a tuple as you will see below.","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Concluding, we can create the MyBernoulli factor node as","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"@node MyBernoulli Stochastic [out, (π, aliases = [p])]","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Cool! Step 1 is done, we have created a custom node.","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"","category":"page"},{"location":"manuals/customization/custom-node/#Step-2:-Defining-rules-for-our-node","page":"Defining a custom node and rules","title":"Step 2: Defining rules for our node","text":"","category":"section"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"In order for RxInfer to perform probabilistic inference and compute posterior distributions, such as p(pimid x_1K), we need to tell it how to perform inference locally around our node. This localization is what makes RxInfer achieve high performance. In our message passing-based paradigm, we need to describe how the node processes incoming information in the form of messages (or marginals). Here we will highlight two different message passing strategies: sum-product message passing and variational message passing.","category":"page"},{"location":"manuals/customization/custom-node/#Sum-product-message-passing-update-rules","page":"Defining a custom node and rules","title":"Sum-product message passing update rules","text":"","category":"section"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"In sum-product message passing we compute outgoing messages to our node as","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"vecmu(x) propto int mathrmBer(xmid pi) vecmu(pi) mathrmdx","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"overleftarrowmu(pi) propto sum_x in 01 mathrmBer(xmid pi) overleftarrowmu(x)","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"This integral does not always have nice tractable solutions. However, for some forms of the incoming messages, it does yield a tractable solution.","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"For the case of a Beta message coming into our node, the outgoing message will be the predictive posterior of the Bernoulli distribution with a Beta prior. Here we obtain pi = fracalphaalpha + beta, which coincides with the mean of the Beta distribution. Hence, we can write down the first update rule using the @rule macro as","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"@rule MyBernoulli(:out, Marginalisation) (m_π :: Beta,) = MyBernoulli(mean(m_π))","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Here, :out refers to the interface of the outgoing message. The second argument denotes the incoming messages (which can be typed) as a tuple. Therefore make sure that it has a trailing , when there is a single message coming in. m_π is shorthand for the incoming message on interface π. As we will see later, the structured approximation update rule for incoming message from π will have q_π as parameter.","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"The second rule is also straightforward; if π is a PointMass and therefore fixed, the outgoing message will be MyBernoulli(π):","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"@rule MyBernoulli(:out, Marginalisation) (m_π :: PointMass,) = MyBernoulli(mean(m_π))","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Continuing with the sum-product update rules, we now have to define the update rules towards the π interface. We can only do exact inference if the incoming message is known, which in the case of the Bernoulli distribution, means that the out message is a PointMass distribution that is either 0 or 1. The updated Beta distribution for π will be:","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"overleftarrowmu(π) propto mathrmBeta(1 + x 2 - x)","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Which gives us the following update rule:","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"@rule MyBernoulli(:π, Marginalisation) (m_out :: PointMass,) = begin\n    p = mean(m_out)\n    return Beta(one(p) + p, 2one(p) - p)\nend","category":"page"},{"location":"manuals/customization/custom-node/#Variational-message-passing-update-rules","page":"Defining a custom node and rules","title":"Variational message passing update rules","text":"","category":"section"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"We will now cover our second set of update rules. The sum-product messages are not always tractable and therefore we may need to resort to approximations. Here we highlight the variational approximation. In variational message passing we compute outgoing messages to our node as","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"vecnu(x) propto exp int q(pi) ln mathrmBer(xmid pi) mathrmdx","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"overleftarrownu(pi) propto exp sum_x in 01 q(x) ln mathrmBer(xmid pi)","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"These messages depend on the marginals on the adjacent edges and not on the incoming messages as was the case with sum-product message passing. Update rules that operate on the marginals instead of the incoming messages are specified with the q_{interface} argument names. With these update rules, we can often support a wider family of distributions. Below we directly give the variational update rules. Deriving them yourself will be a nice challenge.","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"#rules towards out\n@rule MyBernoulli(:out, Marginalisation) (q_π :: PointMass,) = MyBernoulli(mean(q_π))\n\n@rule Bernoulli(:out, Marginalisation) (q_π::Any,) = begin\n    rho_1 = mean(log, q_π)          # E[ln(x)]\n    rho_2 = mean(mirrorlog, q_π)    # E[log(1-x)]\n    m = max(rho_1, rho_2)\n    tmp = exp(rho_1 - m)\n    p = clamp(tmp / (tmp + exp(rho_2 - m)), tiny, one(m))\n    return Bernoulli(p)\nend\n\n#rules towards π\n@rule MyBernoulli(:π, Marginalisation) (q_out :: Any,) = begin\n    p = mean(q_out)\n    return Beta(one(p) + p, 2one(p) - p)\nend","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"note: Note\nTypically, the type of the variational distributions q_ does not matter in the real computations, but only their statistics, e.g mean or var. Thus, in this case, we may safely use ::Any.","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"In the example that we will show later on, we solely use sum-product message passing. Variational message passing requires us to set the local constraints in our model, something which is out of scope of this tutorial.","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"","category":"page"},{"location":"manuals/customization/custom-node/#Step-3:-Defining-joint-marginals-for-our-node","page":"Defining a custom node and rules","title":"Step 3: Defining joint marginals for our node","text":"","category":"section"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"The entire probabilistic model can be scored using the Bethe free energy, which bounds the log-evidence for acyclic graphs. This Bethe free energy consists out of the sum of node-local entropies, negative node-local average energies and edge specific entropies. Formally we can denote this by","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Fqf = - sum_ainmathcalV mathrmHq_a(s_a) - sum_ainmathcalVmathrmE_q_a(s_a)ln f_a(s_a) + sum_iinmathcalEmathrmHq_i(s_i)","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Here we call q_a(s_a) the joint marginals around a node and -mathrmE_q_a(s_a)ln f_a(s_a) we term the average energy.","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"In order to be able to compute the Bethe free energy, we need to first describe how to compute q_a(s_a), defined in our case as ","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"q(x_k pi) = vecmu(pi) overleftarrowmu(x_k) mathrmBer(x_k mid pi)","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"To calculate the updated posterior marginal for our custom distribution, we need to return joint posterior marginals for the interfaces of our node. In our case, the posterior marginal for the observation is still the same PointMass distribution. However, to calculate the posterior marginal over π, we use RxInfer's built-in prod functionality to multiply the Beta prior with the Beta likelihood. This gives us the updated posterior distribution, which is also a Beta distribution. We use PreserveTypeProd(Distribution) parameter to ensure that we multiply the two distributions analytically. This is done as follows:","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"@marginalrule MyBernoulli(:out_π) (m_out::PointMass, m_π::Beta) = begin\n    r = mean(m_out)\n    p = prod(PreserveTypeProd(Distribution), Beta(one(r) + r, 2one(r) - r), m_π)\n    return (out = m_out, p = p)\nend","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"In this code :out_π describes the arguments of the joint marginal distribution. The second argument contains the incoming messages. Here we know from the model specification that we observe out and therefore this has to be a PointMass. Because it is a PointMass, the joint marginal automatically factorizes as q(x_k pi) = q(x_k)q(pi). These are the distributions that we return in a form of the NamedTuple. NamedTuple is used only in cases where we know that the joint marginal factorizes further, but typically it should be a full distribution. For computing q(pi) we need to compute the product vecmu(pi)overleftarrowmu(pi). We already know how overleftarrowmu(pi) looks like from the previous step, so we can just use the prod function.","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"","category":"page"},{"location":"manuals/customization/custom-node/#Step-4:-Defining-the-average-energy-for-our-node","page":"Defining a custom node and rules","title":"Step 4: Defining the average energy for our node","text":"","category":"section"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"To complete the computation of the Bethe free energy, we also need to compute the average energy term. The average energy in our MyBernoulli example can be computed as -mathrmE_q(x_k pi)ln p(x_k mid pi), however, because we know that we observe x_k and therefore q(x_k pi) factorizes, we can instead compute beginaligned -mathrmE_q(x_k)q(pi)ln p(x_k mid pi) = -mathrmE_q(x_k)q(pi) ln (pi^x_k (1-pi)^1 - x_k) \n= -mathrmE_q(x_k)q(pi) x_k ln(pi) + (1-x_k) ln(1-pi) \n= -mathrmE_q(x_k)x_k mathrmE_q(pi) ln(pi) - (1-mathrmE_q(x_k)x_k) mathrmE_q(pi)ln(1-pi) endaligned","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Which is what we implemented below. Note that mean(mirrorlog, q(x)) is equal to mathrmE_q(x)1-logx.","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"@average_energy Bernoulli (q_out::Any, q_π::Any) = -mean(q_out) * mean(log, q_π) - (1.0 - mean(q_out)) * mean(mirrorlog, q_π)","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"In the case that the interfaces do not factorize, we would get something like @average_energy MyBernoulli (q_out_π::Any,) = begin ... end.","category":"page"},{"location":"manuals/customization/custom-node/#Using-our-node-in-a-model","page":"Defining a custom node and rules","title":"Using our node in a model","text":"","category":"section"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"With all the necessary functions defined, we can proceed to test our custom node in an experiment. For this experiment, we will generate a dataset from a Bernoulli distribution with a fixed success probability of 0.75. Next, we will define a probabilistic model that has a Beta prior and a MyBernoulli likelihood. The Beta prior will be used to model our prior belief about the probability of success. The MyBernoulli likelihood will be used to model the generative process of the observed data. We start by generating the dataset:","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"using Random\n\nrng = MersenneTwister(42)\nn = 500\nπ_real = 0.75\ndistribution = Bernoulli(π_real)\n\ndataset = float.(rand(rng, distribution, n))\n\nnothing # hide","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Next, we define our model. Note that we use the MyBernoulli node in the model. The model consists of a single latent variable π, which has a Beta prior and is the parameter of the MyBernoulli likelihood. The MyBernoulli node takes the value of π as its parameter and returns a binary observation. We set the hyperparameters of the Beta prior to be 4 and 8, respectively, which correspond to a distribution slightly biased towards higher values of π. The model is defined as follows:","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"@model function coin_model_mybernoulli(y)\n    # We endow θ parameter of our model with some prior\n    π ~ Beta(4.0, 8.0)\n    # We assume that outcome of each coin flip is governed by the MyBernoulli distribution\n    for i in eachindex(y)\n        y[i] ~ MyBernoulli(π)\n    end\nend","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Finally, we can run inference with this model and the generated dataset:","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"result_mybernoulli = infer(\n    model = coin_model_mybernoulli(), \n    data  = (y = dataset, ),\n)","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"We have now completed our experiment and obtained the posterior marginal distribution for p through inference. To evaluate the performance of our inference, we can compare the estimated posterior to the true value. In our experiment, the true value for p is 0.75, and we can see that the estimated posterior has a mean close to this value, which shows that our custom node was able to successfully pass messages towards the π variable in order to learn the true value of the parameter.","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"using Plots\n\nrθ = range(0, 1, length = 1000)\n\np = plot(title = \"Inference results\")\n\nplot!(rθ, (x) -> pdf(result_mybernoulli.posteriors[:π], x), fillalpha=0.3, fillrange = 0, label=\"p(π|x)\", c=3)\nvline!([π_real], label=\"Real π\")","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"As a sanity check, we can create the same model with the RxInfer built-in node Bernoulli and compare the resulting posterior distribution with the one obtained using our custom MyBernoulli node. This will give us confidence that our custom node is working correctly. We use the Bernoulli node with the same Beta prior and the observed data, and then run inference. We can compare the two posterior distributions and observe that they are exactly the same, which indicates that our custom node is performing as expected.","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"@model function coin_model(y)\n    p ~ Beta(4.0, 8.0)\n    for i in eachindex(y)\n        y[i] ~ Bernoulli(p)\n    end\nend\n\nresult_bernoulli = infer(\n    model = coin_model(), \n    data  = (y = dataset, ),\n)\n\nif !(result_bernoulli.posteriors[:p] == result_mybernoulli.posteriors[:π])\n    error(\"Results are not identical\")\nelse \n    println(\"Results are identical 🎉🎉🎉\")\nend\n\nnothing # hide","category":"page"},{"location":"manuals/customization/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Congratulations! You have successfully implemented your own custom node in RxInfer. We went through the definition of a node to the implementation of the update rules and marginal posterior calculations. Finally we tested our custom node in a model and checked if we implemented everything correctly.","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/#examples-coin-toss-model-(beta-bernoulli)","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"","category":"section"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"In this example, we are going to perform an exact inference for a coin-toss model that can be represented as:","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"beginaligned\np(theta) = mathrmBeta(thetaa b)\np(y_itheta) = mathrmBernoulli(y_itheta)\nendaligned","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"where y_i in 0 1 is a binary observation induced by Bernoulli likelihood while theta is a Beta prior distribution on the parameter of Bernoulli. We are interested in inferring the posterior distribution of theta.","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"We start with importing all needed packages:","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"using RxInfer, Random","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"Let's generate some synthetic observations using Bernoulli distribution for a biased coin-tosses that are independent and identically distributed (IID).","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"rng = MersenneTwister(42)\nn = 500\nθ_real = 0.75\ndistribution = Bernoulli(θ_real)\n\ndataset = float.(rand(rng, Bernoulli(θ_real), n));","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"Once we generate the dataset, now we define a coin-toss model using the @model macro from RxInfer.jl","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"# GraphPPL.jl export `@model` macro for model specification\n# It accepts a regular Julia function and builds an FFG under the hood\n@model function coin_model(y, a, b)\n\n    # We endow θ parameter of our model with \"a\" prior\n    θ ~ Beta(a, b)\n    # note that, in this particular case, the `Uniform(0.0, 1.0)` prior will also work.\n    # θ ~ Uniform(0.0, 1.0)\n\n    # here, the outcome of each coin toss is governed by the Bernoulli distribution\n    for i in eachindex(y)\n        y[i] ~ Bernoulli(θ)\n    end\n\nend","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"Now, once the model is defined, we perform a (perfect) inference:","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"result = infer(\n    model = coin_model(a=4.0, b = 8.0), \n    data  = (y = dataset,)\n)","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"Inference results:\n  Posteriors       | available for (θ)","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"Once the result is calculated, we can focus on the posteriors","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"θestimated = result.posteriors[:θ]","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"Beta{Float64}(α=365.0, β=147.0)","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"and visualisation of the results","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"using Plots\n\nrθ = range(0, 1, length = 1000)\n\np = plot(title = \"Inference results\")\n\nplot!(rθ, (x) -> pdf(Beta(2.0, 7.0), x), fillalpha=0.3, fillrange = 0, label=\"P(θ)\", c=1,)\nplot!(rθ, (x) -> pdf(θestimated, x), fillalpha=0.3, fillrange = 0, label=\"P(θ|y)\", c=3)\nvline!([θ_real], label=\"Real θ\")","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"(Image: )","category":"page"},{"location":"manuals/inference/static/#manual-static-inference","page":"Static inference","title":"Static Inference","text":"","category":"section"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"This guide explains how to use the infer function for static datasets. We'll show how RxInfer can estimate posterior beliefs given a set of observations. We'll use a simple Beta-Bernoulli model as an example, which has been covered in the Getting Started section, but keep in mind that these techniques can apply to any model.","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"Also read about Streaming Inference or checkout more complex examples.","category":"page"},{"location":"manuals/inference/static/#manual-static-inference-model-spec","page":"Static inference","title":"Model specification","text":"","category":"section"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"Also read the Model Specification section.","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"In static inference, we want to update our prior beliefs about certain hidden states given some dataset. To achieve this, we include data as an argument in our model specification:","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"using RxInfer\nusing Test #hide\n\n@model function beta_bernoulli(y, a, b)\n    θ ~ Beta(a, b)  \n    for i in 1:length(y)\n        y[i] ~ Bernoulli(θ)\n    end\nend","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"In this model, we assume that y is a collection of data points, and a and b are just numbers. To run inference in this model, we have to call the infer function with the data argument provided.","category":"page"},{"location":"manuals/inference/static/#manual-static-inference-dataset","page":"Static inference","title":"Dataset of observations","text":"","category":"section"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"For demonstration purposes, we will use hand crafted dataset:","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"using Distributions, StableRNGs\n\nhidden_θ       = 1 / 3.1415\ndistribution   = Bernoulli(hidden_θ)\nrng            = StableRNG(43)\nn_observations = 1_000\ndataset        = rand(rng, distribution, n_observations)\nnothing #hide","category":"page"},{"location":"manuals/inference/static/#manual-static-inference-infer","page":"Static inference","title":"Calling the inference procedure","text":"","category":"section"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"Everything is ready to run inference in our simple model. In order to run inference with static dataset using the infer function, we need to use the data argument. The data argument  expects a NamedTuple where keys correspond to the names of the model arguments. In our case the model arguments were a, b and y. We treat a and b as hyperparameters and pass them directly to the model constructor and we treat y as our observations, thus we pass it to the data argument as follows:","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"results = infer(\n    model = beta_bernoulli(a = 1.0, b = 1.0),\n    data  = (y = dataset, )\n)\n@test results isa InferenceResult #hide\nresults #hide","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"note: Note\ny inside the @model specification is not the same data collection as provided in the data argument. Inside the @model, y is a collection of nodes in the corresponding factor graph, but it will have exactly the same shape as the collection provided in the data argument, hence we can use some basic Julia function, e.g. length. ","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"Note, that we could also pass a and b as data:","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"results = infer(\n    model = beta_bernoulli(),\n    data  = (y = dataset, a = 1.0, b = 1.0)\n)\n@test results isa InferenceResult #hide\nresults #hide","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"The infer function, however, requires at least one data argument to be present in the supplied data. The difference between hyperparameters and observations is purely semantic and should not have real influence on the result of the inference procedure. ","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"note: Note\nThe inference procedure uses reactive message passing protocol and may decide to optimize and precompute certain messages that use fixed hyperparameters, hence changing the order of computed messages. The order of computations may change the convergence properties for some complex models.","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"In case of inference with static datasets, the infer function will return the InferenceResult structure. This structure has the .posteriors field, which is a Dict like structure that maps names of latent states to their corresponding posteriors. For example:","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"@test results.posteriors[:θ] isa Beta #hide\nresults.posteriors[:θ]","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"InferenceResult","category":"page"},{"location":"manuals/inference/static/#RxInfer.InferenceResult","page":"Static inference","title":"RxInfer.InferenceResult","text":"InferenceResult\n\nThis structure is used as a return value from the infer function for static datasets. \n\nPublic Fields\n\nposteriors: Dict or NamedTuple of 'random variable' - 'posterior' pairs. See the returnvars argument for infer.\npredictions: (optional) Dict or NamedTuple of 'data variable' - 'prediction' pairs. See the predictvars argument for infer.\nfree_energy: (optional) An array of Bethe Free Energy values per VMP iteration. See the free_energy argument for infer.\nmodel: FactorGraphModel object reference.\nerror: (optional) A reference to an exception, that might have occurred during the inference. See the catch_exception argument for infer.\n\nSee also: infer\n\n\n\n\n\n","category":"type"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"We can also visualize our posterior results with the Plots.jl package. We  used Beta(a = 1.0, b = 1.0) as a prior, lets compare our prior and posterior beliefs:","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"using Plots\n\nrθ = range(0, 1, length = 1000)\n\np = plot()\np = plot!(p, rθ, (x) -> pdf(Beta(1.0, 1.0), x), title=\"Prior\", fillalpha=0.3, fillrange = 0, label=\"P(θ)\", c=1,)\np = plot!(p, rθ, (x) -> pdf(results.posteriors[:θ], x), title=\"Posterior\", fillalpha=0.3, fillrange = 0, label=\"P(θ|y)\", c=3)","category":"page"},{"location":"manuals/inference/static/#manual-static-inference-variational-inference","page":"Static inference","title":"Variational Inference with static datasets","text":"","category":"section"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"The example above is quite simple and performs exact Bayesian inference. However, for more complex model, we may need to specify variational constraints and perform variational inference. To demonstrate this, we will use a slightly more complex model, where we need to estimate mean and the precision of IID samples drawn from the Normal distribution:","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"@model function iid_estimation(y)\n    μ  ~ Normal(mean = 0.0, precision = 0.1)\n    τ  ~ Gamma(shape = 1.0, rate = 1.0)\n    y .~ Normal(mean = μ, precision = τ)\nend","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"In this model, we have two latent variables μ and τ and a set of observations y. Note  that we used the broadcasting syntax, which is roughly equivalent to the manual for loop shown in the previous example. Let's try to run the inference in this model, but first, we need to create our observations:","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"# `ExponentialFamily` package expors different parametrizations \n# for the Normal distribution\nusing ExponentialFamily\n\nhidden_μ       = 3.1415\nhidden_τ       = 2.7182\ndistribution   = NormalMeanPrecision(hidden_μ, hidden_τ)\nrng            = StableRNG(42)\nn_observations = 1_000\ndataset        = rand(rng, distribution, n_observations)\nnothing #hide","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"And finally we run the inference procedure:","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"results = infer(\n    model = iid_estimation(),\n    data  = (y = dataset, )\n)","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"ERROR: Variables [ μ, τ ] have not been updated after an update event. \nTherefore, make sure to initialize all required marginals and messages. See `initialization` keyword argument for the inference function. \nSee the official documentation for detailed information regarding the initialization.\n\nStacktrace:\n [1] error(s::String)\n   @ Base ./error.jl:35","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"# This is just a test to ensure that the example above does\n# indeed fail with the exact same error\nusing RxInfer, Test \n@model function iid_estimation(y)\n    μ  ~ Normal(mean = 0.0, precision = 0.1)\n    τ  ~ Gamma(shape = 1.0, rate = 1.0)\n    y .~ Normal(mean = μ, precision = τ)\nend\n@test_throws \"Variables [ μ, τ ] have not been updated after an update event.\" infer(\n    model = iid_estimation(),\n    data  = (y = rand(10), )\n)\nnothing","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"Huh? We get an error saying that the inference could not update the latent variables. This is happened because our model contain loops in its structure, therefore it requires the initialization. Read more about the initialization in the corresponding section in the documentation.","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"We have two options here, either we initialize the messages and perform Loopy Belief Propagation in this model or we break the loops with variational constraints and perform variational inference. In this tutorial, we will choose the second option. For this we need to specify factorization constraints with the @constraints macro.","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"# Specify mean-field constraint over the joint variational posterior\nconstraints = @constraints begin \n    q(μ, τ) = q(μ)q(τ)\nend\n# Specify initial posteriors for variational iterations\ninitialization = @initialization begin \n    q(μ) = vague(NormalMeanPrecision)\n    q(τ) = vague(GammaShapeRate)\nend\nnothing #hide","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"With this, we can use the constraints and initialization keyword arguments in the infer function. We also specify the number of variational iterations with the iterations keyword argument:","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"results = infer(\n    model          = iid_estimation(),\n    data           = (y = dataset, ),\n    constraints    = constraints,\n    iterations     = 100,\n    initialization = initialization\n)","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"Nice! Now, we have some result. Let's for example inspect the posterior results for μ.","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"results.posteriors[:μ]","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"In constrast to the previous example, now we have an array of posteriors for μ, not just a single value. Each posterior in the collection corresponds to the intermediate variational update for each variational iteration. Let's visualize how our posterior over μ has been changing during the variational optimization:","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"@gif for (i, intermediate_posterior) in enumerate(results.posteriors[:μ])\n    rμ = range(0, 5, length = 1000)\n    plot(rμ, (x) -> pdf(intermediate_posterior, x), title=\"Posterior on iteration $(i)\", fillalpha=0.3, fillrange = 0, label=\"P(μ|y)\", c=3)\n    vline!([hidden_μ], label = \"Real μ\")\nend","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"It seems that the posterior has converged to a stable distribution pretty fast.  We are going to verify the converge in the next section. If, for example, we are not interested in intermediate updates, but just in the final posterior, we could use the returnvars option in the infer function and use the KeepLast option for μ:","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"results_keep_last = infer(\n    model          = iid_estimation(),\n    data           = (y = dataset, ),\n    constraints    = constraints,\n    iterations     = 100,\n    returnvars     = (μ = KeepLast(), ),\n    initialization = initialization\n)","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"We can also verify that the got exactly the same result:","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"@test results_keep_last.posteriors[:μ] == last(results.posteriors[:μ]) #hide\nresults_keep_last.posteriors[:μ] == last(results.posteriors[:μ])","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"Let's also verify that the posteriors are consistent with the real hidden values used in the dataset generation:","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"println(\"Real μ was \", hidden_μ)\nprintln(\"Inferred mean for μ is \", mean(last(results.posteriors[:μ])), \" with standard deviation \", std(last(results.posteriors[:μ])))\n\nprintln(\"Real τ was \", hidden_τ)\nprintln(\"Inferred mean for τ is \", mean(last(results.posteriors[:τ])), \" with standard deviation \", std(last(results.posteriors[:τ])))\n\n@test abs(mean(last(results.posteriors[:μ])) - hidden_μ) < 3std(last(results.posteriors[:μ])) #hide\n@test abs(mean(last(results.posteriors[:τ])) - hidden_τ) < 3std(last(results.posteriors[:τ])) #hide\nnothing #hide","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"rμ = range(2, 4, length = 1000)\npμ = plot(rμ, (x) -> pdf(last(results.posteriors[:μ]), x), title=\"Posterior for μ\", fillalpha=0.3, fillrange = 0, label=\"P(μ|y)\", c=3)\npμ = vline!(pμ, [ hidden_μ ], label = \"Real μ\")\n\nrτ = range(2, 4, length = 1000)\npτ = plot(rτ, (x) -> pdf(last(results.posteriors[:τ]), x), title=\"Posterior for τ\", fillalpha=0.3, fillrange = 0, label=\"P(τ|y)\", c=3)\npτ = vline!(pτ, [ hidden_τ ], label = \"Real τ\")\n\nplot(pμ, pτ)","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"Nice result! Our posteriors are pretty close to the actual values of the parameters used for dataset generation.","category":"page"},{"location":"manuals/inference/static/#manual-static-inference-bfe","page":"Static inference","title":"Convergence and Bethe Free Energy","text":"","category":"section"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"Read also the Bethe Free Energy section.","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"In contrast to Loopy Belief Propagation, the variational inference is set to converge to a stable point during variational inference. In order to verify the convergence for this particular model, we can check the convergence of the Bethe Free Enegrgy values. By default, infer function does not compute the Bethe Free Energy values. In order to compute those, we must set the free_energy flag explicitly to true:","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"results = infer(\n    model          = iid_estimation(),\n    data           = (y = dataset, ),\n    constraints    = constraints,\n    iterations     = 100,\n    initialization = initialization,\n    free_energy    = true\n)","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"Now, we can access the free_energy field of the results and verify if the inference procedure has converged or not:","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"plot(results.free_energy, label = \"Bethe Free Energy\")","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"Well, it seems that 100 iterations was too much for this simple problem and we could do much less iterations in order to converge to a stable point. The animation above also suggested that the posterior for μ has converged pretty fast to a stable point.","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"# Let's try to use only 5 iterations\nresults = infer(\n    model          = iid_estimation(),\n    data           = (y = dataset, ),\n    constraints    = constraints,\n    iterations     = 5,\n    initialization = initialization,\n    free_energy    = true\n)","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"plot(results.free_energy, label = \"Bethe Free Energy\")","category":"page"},{"location":"manuals/inference/static/#manual-static-inference-callbacks","page":"Static inference","title":"Callbacks","text":"","category":"section"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"The infer function has its own lifecycle, consisting of multiple steps. A user is free to inject some extra logic during the inference procedure, e.g. for debugging purposes. By supplying callbacks, users can inject custom logic on specific moments during the inference procedure. Here are available callbacks that can be used together with the static datasets:","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"using RxInfer, Test, Markdown\n# Update the documentation below if this test does not pass\n@test RxInfer.available_callbacks(RxInfer.batch_inference) === Val((\n    :on_marginal_update,\n    :before_model_creation,\n    :after_model_creation,\n    :before_inference,\n    :before_iteration,\n    :before_data_update,\n    :after_data_update,\n    :after_iteration,\n    :after_inference\n)) \nnothing","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"before_model_creation()","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"Calls before the model is going to be created, does not accept any arguments.","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"after_model_creation(model::ProbabilisticModel)","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"Calls right after the model has been created, accepts a single argument, the model.","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"before_inference(model::ProbabilisticModel)","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"Calls before the inference procedure starts, accepts a single argument, the model.","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"after_inference(model::ProbabilisticModel)","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"Calls after the inference procedure ends, accepts a single argument, the model.","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"before_iteration(model::ProbabilisticModel, iteration::Int)","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"Calls before each iteration, accepts two arguments: the model and the current iteration number.","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"after_iteration(model::ProbabilisticModel, iteration::Int)","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"Calls after each iteration, accepts two arguments: the model and the current iteration number.","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"before_data_update(model::ProbabilisticModel, data)","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"Calls before each data update, accepts two arguments: the model and the updated data.","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"after_data_update(model::ProbabilisticModel, data)","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"Calls after each data update, accepts two arguments: the model and the updated data.","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"on_marginal_update(model::ProbabilisticModel, name, update)","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"Calls after each marginal update, accepts three arguments: the model, the name of the updated marginal, and the updated marginal itself.","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"Here is an example usage of the outlined callbacks:","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"before_model_creation_called = Ref(false) #hide\nafter_model_creation_called = Ref(false) #hide\nbefore_inference_called = Ref(false) #hide\nafter_inference_called = Ref(false) #hide\nbefore_iteration_called = Ref(false) #hide\nafter_iteration_called = Ref(false) #hide\nbefore_data_update_called = Ref(false) #hide\nafter_data_update_called = Ref(false) #hide\non_marginal_update_called = Ref(false) #hide\n\nfunction before_model_creation()\n    before_model_creation_called[] = true #hide\n    println(\"The model is about to be created\")\nend\n\nfunction after_model_creation(model::ProbabilisticModel)\n    after_model_creation_called[] = true #hide\n    println(\"The model has been created\")\n    println(\"  The number of factor nodes is: \", length(RxInfer.getfactornodes(model)))\n    println(\"  The number of latent states is: \", length(RxInfer.getrandomvars(model)))\n    println(\"  The number of data points is: \", length(RxInfer.getdatavars(model)))\n    println(\"  The number of constants is: \", length(RxInfer.getconstantvars(model)))\nend\n\nfunction before_inference(model::ProbabilisticModel)\n    before_inference_called[] = true #hide\n    println(\"The inference procedure is about to start\")\nend\n\nfunction after_inference(model::ProbabilisticModel)\n    after_inference_called[] = true #hide\n    println(\"The inference procedure has ended\")\nend\n\nfunction before_iteration(model::ProbabilisticModel, iteration::Int)\n    before_iteration_called[] = true #hide\n    println(\"The iteration \", iteration, \" is about to start\")\nend\n\nfunction after_iteration(model::ProbabilisticModel, iteration::Int)\n    after_iteration_called[] = true #hide\n    println(\"The iteration \", iteration, \" has ended\")\nend\n\nfunction before_data_update(model::ProbabilisticModel, data)\n    before_data_update_called[] = true #hide\n    println(\"The data is about to be processed\")\nend\n\nfunction after_data_update(model::ProbabilisticModel, data)\n    after_data_update_called[] = true #hide\n    println(\"The data has been processed\")\nend\n\nfunction on_marginal_update(model::ProbabilisticModel, name, update)\n    on_marginal_update_called[] = true #hide\n    println(\"New marginal update for \", name, \" \", update)\nend","category":"page"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"results = infer(\n    model          = iid_estimation(),\n    data           = (y = dataset, ),\n    constraints    = constraints,\n    iterations     = 5,\n    initialization = initialization,\n    free_energy    = true,\n    callbacks      = (\n        before_model_creation = before_model_creation,\n        after_model_creation = after_model_creation,\n        before_inference = before_inference,\n        after_inference = after_inference,\n        before_iteration = before_iteration,\n        after_iteration = after_iteration,\n        before_data_update = before_data_update,\n        after_data_update = after_data_update,\n        on_marginal_update = on_marginal_update\n    )\n)\n\n@test before_model_creation_called[] #hide\n@test after_model_creation_called[] #hide\n@test before_inference_called[] #hide\n@test after_inference_called[] #hide\n@test before_iteration_called[] #hide\n@test after_iteration_called[] #hide\n@test before_data_update_called[] #hide\n@test after_data_update_called[] #hide\n@test on_marginal_update_called[] #hide\n\nnothing #hide","category":"page"},{"location":"manuals/inference/static/#manual-static-inference-where-to-go","page":"Static inference","title":"Where to go next?","text":"","category":"section"},{"location":"manuals/inference/static/","page":"Static inference","title":"Static inference","text":"This guide covered some fundamental usages of the infer function in the context of inference with static datasets,  but did not cover all the available keyword arguments of the function. Read more explanation about the other keyword arguments  in the Overview section or check out the Streaming Inference section. Also check out more complex examples.","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/#examples-invertible-neural-networks:-a-tutorial","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"","category":"section"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Table of contents","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Introduction\nModel specification\nModel compilation\nProbabilistic inference\nParameter estimation","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/#Introduction","page":"Invertible neural networks: a tutorial","title":"Introduction","text":"","category":"section"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/#Load-required-packages","page":"Invertible neural networks: a tutorial","title":"Load required packages","text":"","category":"section"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Before we can start, we need to import some packages:","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"using RxInfer\nusing Random\nusing StableRNGs\n\nusing ReactiveMP        # ReactiveMP is included in RxInfer, but we explicitly use some of its functionality\nusing LinearAlgebra     # only used for some matrix specifics\nusing Plots             # only used for visualisation\nusing Distributions     # only used for sampling from multivariate distributions\nusing Optim             # only used for parameter optimisation","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/#Model-specification","page":"Invertible neural networks: a tutorial","title":"Model specification","text":"","category":"section"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Specifying an invertible neural network model is easy. The general recipe looks like follows: model = FlowModel(input_dim, (layer1(options), layer2(options), ...)). Here the first argument corresponds to the input dimension of the model and the second argument is a tuple of layers. An example model can be defined as ","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"model = FlowModel(2,\n    (\n        AdditiveCouplingLayer(PlanarFlow()),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false)\n    )\n);","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Alternatively, the input_dim can also be passed as an InputLayer layer as ","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"model = FlowModel(\n    (\n        InputLayer(2),\n        AdditiveCouplingLayer(PlanarFlow()),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false)\n    )\n);","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"In the above AdditiveCouplingLayer layers the input bfx = x_1 x_2 ldots x_N is partitioned into chunks of unit length. These partitions are additively coupled to an output bfy = y_1 y_2 ldots y_N as ","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"beginaligned\n    y_1 = x_1 \n    y_2 = x_2 + f_1(x_1) \n    vdots \n    y_N = x_N + f_N-1(x_N-1)\nendaligned","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Importantly, this structure can easily be converted as ","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"beginaligned\n    x_1 = y_1 \n    x_2 = y_2 - f_1(x_1) \n    vdots \n    x_N = y_N - f_N-1(x_N-1)\nendaligned","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"f_n","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"is an arbitrarily complex function, here chosen to be a PlanarFlow, but this can be interchanged for any function or neural network. The permute keyword argument (which defaults to true) specifies whether the output of this layer should be randomly permuted or shuffled. This makes sure that the first element is also transformed in consecutive layers.","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"A permutation layer can also be added by itself as a PermutationLayer layer with a custom permutation matrix if desired.","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"model = FlowModel(\n    (\n        InputLayer(2),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false),\n        PermutationLayer(PermutationMatrix(2)),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false)\n    )\n);","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/#Model-compilation","page":"Invertible neural networks: a tutorial","title":"Model compilation","text":"","category":"section"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"In the current models, the layers are setup to work with the passed input dimension. This means that the function f_n is repeated input_dim-1 times for each of the partitions. Furthermore the permutation layers are set up with proper permutation matrices. If we print the model we get","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"model","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"FlowModel{3, Tuple{ReactiveMP.AdditiveCouplingLayerEmpty{Tuple{ReactiveMP.P\nlanarFlowEmpty{1}}}, PermutationLayer{Int64}, ReactiveMP.AdditiveCouplingLa\nyerEmpty{Tuple{ReactiveMP.PlanarFlowEmpty{1}}}}}(2, (ReactiveMP.AdditiveCou\nplingLayerEmpty{Tuple{ReactiveMP.PlanarFlowEmpty{1}}}(2, (ReactiveMP.Planar\nFlowEmpty{1}(),), 1), PermutationLayer{Int64}(2, [0 1; 1 0]), ReactiveMP.Ad\nditiveCouplingLayerEmpty{Tuple{ReactiveMP.PlanarFlowEmpty{1}}}(2, (Reactive\nMP.PlanarFlowEmpty{1}(),), 1)))","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"The text below describes the terms above. Please note the distinction in typing and elements, i.e. FlowModel{types}(elements):","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"FlowModel - specifies that we are dealing with a flow model.\n3 - Number of layers.\nTuple{AdditiveCouplingLayerEmpty{...},PermutationLayer{Int64},AdditiveCouplingLayerEmpty{...}} - tuple of layer types.\nTuple{ReactiveMP.PlanarFlowEmpty{1},ReactiveMP.PlanarFlowEmpty{1}} - tuple of functions f_n.\nPermutationLayer{Int64}(2, [0 1; 1 0]) - permutation layer with input dimension 2 and permutation matrix [0 1; 1 0].","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"From inspection we can see that the AdditiveCouplingLayerEmpty and PlanarFlowEmpty objects are different than before. They are initialized for the correct dimension, but they do not have any parameters registered to them. This is by design to allow for separating the model specification from potential optimization procedures. Before we perform inference in this model, the parameters should be initialized. We can randomly initialize the parameters as","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"compiled_model = compile(model)","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"CompiledFlowModel{3, Tuple{AdditiveCouplingLayer{Tuple{PlanarFlow{Float64, \nFloat64}}}, PermutationLayer{Int64}, AdditiveCouplingLayer{Tuple{PlanarFlow\n{Float64, Float64}}}}}(2, (AdditiveCouplingLayer{Tuple{PlanarFlow{Float64, \nFloat64}}}(2, (PlanarFlow{Float64, Float64}(-0.5530639593835304, -0.2163616\n102550308, 1.2521116434309154),), 1), PermutationLayer{Int64}(2, [0 1; 1 0]\n), AdditiveCouplingLayer{Tuple{PlanarFlow{Float64, Float64}}}(2, (PlanarFlo\nw{Float64, Float64}(0.34763211887023543, -0.9116171319236555, -0.0789378301\n24018),), 1)))","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Now we can see that random parameters have been assigned to the individual functions inside of our model. Alternatively if we would like to pass our own parameters, then this is also possible. You can easily find the required number of parameters using the nr_params(model) function.","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"compiled_model = compile(model, randn(StableRNG(321), nr_params(model)))","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"CompiledFlowModel{3, Tuple{AdditiveCouplingLayer{Tuple{PlanarFlow{Float64, \nFloat64}}}, PermutationLayer{Int64}, AdditiveCouplingLayer{Tuple{PlanarFlow\n{Float64, Float64}}}}}(2, (AdditiveCouplingLayer{Tuple{PlanarFlow{Float64, \nFloat64}}}(2, (PlanarFlow{Float64, Float64}(0.7296412319250487, -0.97673361\n28037319, -0.4749869451771002),), 1), PermutationLayer{Int64}(2, [0 1; 1 0]\n), AdditiveCouplingLayer{Tuple{PlanarFlow{Float64, Float64}}}(2, (PlanarFlo\nw{Float64, Float64}(0.3490911082645933, -0.8184067956921087, -1.45782147323\n52386),), 1)))","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/#Probabilistic-inference","page":"Invertible neural networks: a tutorial","title":"Probabilistic inference","text":"","category":"section"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"We can perform inference in our compiled model through standard usage of RxInfer and its underlying ReactiveMP inference engine. Let's first generate some random 2D data which has been sampled from a standard normal distribution and is consecutively passed through an invertible neural network. Using the forward(model, data) function we can propagate data in the forward direction.","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"function generate_data(nr_samples::Int64, model::CompiledFlowModel; seed = 123)\n\n    rng = StableRNG(seed)\n    \n    # specify latent sampling distribution\n    dist = MvNormal([1.5, 0.5], I)\n\n    # sample from the latent distribution\n    x = rand(rng, dist, nr_samples)\n\n    # transform data\n    y = zeros(Float64, size(x))\n    for k = 1:nr_samples\n        y[:,k] .= ReactiveMP.forward(model, x[:,k])\n    end\n\n    # return data\n    return y, x\n\nend;","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"# generate data\ny, x = generate_data(1000, compiled_model)\n\n# plot generated data\np1 = scatter(x[1,:], x[2,:], alpha=0.3, title=\"Original data\", size=(800,400))\np2 = scatter(y[1,:], y[2,:], alpha=0.3, title=\"Transformed data\", size=(800,400))\nplot(p1, p2, legend = false)","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"The probabilistic model for doing inference can be described as ","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"@model function invertible_neural_network(y)\n\n    # specify prior\n    z_μ ~ MvNormalMeanCovariance(zeros(2), huge*diagm(ones(2)))\n    z_Λ ~ Wishart(2.0, tiny*diagm(ones(2)))\n\n    # specify observations\n    for k in eachindex(y)\n\n        # specify latent state\n        x[k] ~ MvNormalMeanPrecision(z_μ, z_Λ)\n\n        # specify transformed latent value\n        y_lat[k] ~ Flow(x[k])\n\n        # specify observations\n        y[k] ~ MvNormalMeanCovariance(y_lat[k], tiny*diagm(ones(2)))\n\n    end\n\nend;","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Here the model is passed inside a meta data object of the flow node. Inference then resorts to","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"observations = [y[:,k] for k=1:size(y,2)]\n\nfmodel         = invertible_neural_network()\ndata           = (y = observations, )\ninitialization = @initialization begin \n    q(z_μ) = MvNormalMeanCovariance(zeros(2), huge*diagm(ones(2)))\n    q(z_Λ) = Wishart(2.0, tiny*diagm(ones(2)))\nend\nreturnvars     = (z_μ = KeepLast(), z_Λ = KeepLast(), x = KeepLast(), y_lat = KeepLast())\n\nconstraints = @constraints begin\n    q(z_μ, x, z_Λ) = q(z_μ)q(z_Λ)q(x)\nend\n\n@meta function fmeta(model)\n    compiled_model = compile(model, randn(StableRNG(321), nr_params(model)))\n    Flow(y_lat, x) -> FlowMeta(compiled_model) # defaults to FlowMeta(compiled_model; approximation=Linearization()). \n                                               # other approximation methods can be e.g. FlowMeta(compiled_model; approximation=Unscented(input_dim))\nend\n\n# First execution is slow due to Julia's initial compilation \nresult = infer(\n    model          = fmodel, \n    data           = data,\n    constraints    = constraints,\n    meta           = fmeta(model),\n    initialization = initialization,\n    returnvars     = returnvars,\n    free_energy    = true,\n    iterations     = 10, \n    showprogress   = false\n)","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Inference results:\n  Posteriors       | available for (z_μ, z_Λ, y_lat, x)\n  Free Energy:     | Real[29485.3, 23762.9, 23570.6, 23570.6, 23570.6, 2357\n0.6, 23570.6, 23570.6, 23570.6, 23570.6]","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"fe_flow = result.free_energy\nzμ_flow = result.posteriors[:z_μ]\nzΛ_flow = result.posteriors[:z_Λ]\nx_flow  = result.posteriors[:x]\ny_flow  = result.posteriors[:y_lat];","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"As we can see, the variational free energy decreases inside of our model.","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"plot(1:10, fe_flow/size(y,2), xlabel=\"iteration\", ylabel=\"normalized variational free energy [nats/sample]\", legend=false)","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"If we plot a random noisy observation and its approximated transformed uncertainty we obtain:","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"# pick a random observation\nid = rand(StableRNG(321), 1:size(y,2))\nrand_observation = MvNormal(y[:,id], 5e-1*diagm(ones(2)))\nwarped_observation = MvNormal(ReactiveMP.backward(compiled_model, y[:,id]), ReactiveMP.inv_jacobian(compiled_model, y[:,id])*5e-1*diagm(ones(2))*ReactiveMP.inv_jacobian(compiled_model, y[:,id])');\n\np1 = scatter(x[1,:], x[2,:], alpha=0.1, title=\"Latent distribution\", size=(1200,500), label=\"generated data\")\ncontour!(-5:0.1:5, -5:0.1:5, (x, y) -> pdf(MvNormal([1.5, 0.5], I), [x, y]), c=:viridis, colorbar=false, linewidth=2)\nscatter!([mean(zμ_flow)[1]], [mean(zμ_flow)[2]], color=\"red\", markershape=:x, markersize=5, label=\"inferred mean\")\ncontour!(-5:0.01:5, -5:0.01:5, (x, y) -> pdf(warped_observation, [x, y]), colors=\"red\", levels=1, linewidth=2, colorbar=false)\nscatter!([mean(warped_observation)[1]], [mean(warped_observation)[2]], color=\"red\", label=\"transformed noisy observation\")\np2 = scatter(y[1,:], y[2,:], alpha=0.1, label=\"generated data\")\nscatter!([ReactiveMP.forward(compiled_model, mean(zμ_flow))[1]], [ReactiveMP.forward(compiled_model, mean(zμ_flow))[2]], color=\"red\", marker=:x, label=\"inferred mean\")\ncontour!(-10:0.1:10, -10:0.1:10, (x, y) -> pdf(MvNormal([1.5, 0.5], I), ReactiveMP.backward(compiled_model, [x, y])), c=:viridis, colorbar=false, linewidth=2)\ncontour!(-10:0.1:10, -10:0.1:10, (x, y) -> pdf(rand_observation, [x, y]), colors=\"red\", levels=1, linewidth=2, label=\"random noisy observation\", colorba=false)\nscatter!([mean(rand_observation)[1]], [mean(rand_observation)[2]], color=\"red\", label=\"random noisy observation\")\nplot(p1, p2, legend = true)","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/#Parameter-estimation","page":"Invertible neural networks: a tutorial","title":"Parameter estimation","text":"","category":"section"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"The flow model is often used to learn unknown probabilistic mappings. Here we will demonstrate it as follows for a binary classification task with the following data:","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"function generate_data(nr_samples::Int64; seed = 123)\n    \n    rng = StableRNG(seed)\n\n    # sample weights\n    w = rand(rng, nr_samples, 2)\n\n    # sample appraisal\n    y = zeros(Float64, nr_samples)\n    for k = 1:nr_samples\n        y[k] = 1.0*(w[k,1] > 0.5)*(w[k,2] < 0.5)\n    end\n\n    # return data\n    return y, w\n\nend;","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"data_y, data_x = generate_data(50);\nscatter(data_x[:,1], data_x[:,2], marker_z=data_y, xlabel=\"w1\", ylabel=\"w2\", colorbar=false, legend=false)","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"We will then specify a possible model as","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"# specify flow model\nmodel = FlowModel(2,\n    (\n        AdditiveCouplingLayer(PlanarFlow()), # defaults to AdditiveCouplingLayer(PlanarFlow(); permute=true)\n        AdditiveCouplingLayer(PlanarFlow()),\n        AdditiveCouplingLayer(PlanarFlow()),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false)\n    )\n);","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"The corresponding probabilistic model for the binary classification task can be created as","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"@model function invertible_neural_network_classifier(x, y)\n\n    # specify observations\n    for k in eachindex(x)\n\n        # specify latent state\n        x_lat[k] ~ MvNormalMeanPrecision(x[k], 1e3*diagm(ones(2)))\n\n        # specify transformed latent value\n        y_lat1[k] ~ Flow(x_lat[k])\n        y_lat2[k] ~ dot(y_lat1[k], [1, 1])\n\n        # specify observations\n        y[k] ~ Probit(y_lat2[k]) # default: where { pipeline = RequireMessage(in = NormalMeanPrecision(0, 1.0)) }\n\n    end\n\nend;","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"fcmodel       = invertible_neural_network_classifier()\ndata          = (y = data_y, x = [data_x[k,:] for k=1:size(data_x,1)], )\n\n@meta function fmeta(model, params)\n    compiled_model = compile(model, params)\n    Flow(y_lat1, x_lat) -> FlowMeta(compiled_model)\nend","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"fmeta (generic function with 2 methods)","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Here we see that the compilation occurs inside of our probabilistic model. As a result we can pass parameters (and a model) to this function which we wish to opmize for some criterium, such as the variational free energy. Inference can be described as","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"For the optimization procedure, we will simplify our inference loop, such that it only accepts parameters as an argument (which is wishes to optimize) and outputs a performance metric.","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"function f(params)\n    Random.seed!(123) # Flow uses random permutation matrices, which is not good for the optimisation procedure\n    result = infer(\n        model                   = fcmodel, \n        data                    = data,\n        meta                    = fmeta(model, params),\n        free_energy             = true,\n        free_energy_diagnostics = nothing, # Free Energy can be set to NaN due to optimization procedure\n        iterations              = 10, \n        showprogress            = false\n    );\n    \n    result.free_energy[end]\nend;","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Optimization can be performed using the Optim package. Alternatively, other (custom) optimizers can be implemented, such as:","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"res = optimize(f, randn(StableRNG(42), nr_params(model)), GradientDescent(), Optim.Options(store_trace = true, show_trace = true, show_every = 50), autodiff=:forward)","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"uses finitediff and is slower/less accurate.","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"or","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"# create gradient function\ng = (x) -> ForwardDiff.gradient(f, x);\n\n# specify initial params\nparams = randn(nr_params(model))\n\n# create custom optimizer (here Adam)\noptimizer = Adam(params; λ=1e-1)\n\n# allocate space for gradient\n∇ = zeros(nr_params(model))\n\n# perform optimization\nfor it = 1:10000\n\n    # backward pass\n    ∇ .= ForwardDiff.gradient(f, optimizer.x)\n\n    # gradient update\n    ReactiveMP.update!(optimizer, ∇)\n\nend\n","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"res = optimize(f, randn(StableRNG(42), nr_params(model)), GradientDescent(), Optim.Options(f_tol = 1e-3, store_trace = true, show_trace = true, show_every = 100), autodiff=:forward)","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Iter     Function value   Gradient norm \n     0     5.888958e+02     8.943663e+02\n * time: 0.023761987686157227\n   100     1.059826e+01     4.120074e+00\n * time: 15.219419002532959\n * Status: success\n\n * Candidate solution\n    Final objective value:     1.010211e+01\n\n * Found with\n    Algorithm:     Gradient Descent\n\n * Convergence measures\n    |x - x'|               = 1.59e-03 ≰ 0.0e+00\n    |x - x'|/|x'|          = 7.59e-04 ≰ 0.0e+00\n    |f(x) - f(x')|         = 7.92e-03 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 7.84e-04 ≤ 1.0e-03\n    |g(x)|                 = 2.26e+00 ≰ 1.0e-08\n\n * Work counters\n    Seconds run:   17  (vs limit Inf)\n    Iterations:    113\n    f(x) calls:    301\n    ∇f(x) calls:   301","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"optimization results are then given as","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"params = Optim.minimizer(res)\ninferred_model = compile(model, params)\ntrans_data_x_1 = hcat(map((x) -> ReactiveMP.forward(inferred_model, x), [data_x[k,:] for k=1:size(data_x,1)])...)'\ntrans_data_x_2 = map((x) -> dot([1, 1], x), [trans_data_x_1[k,:] for k=1:size(data_x,1)])\ntrans_data_x_2_split = [trans_data_x_2[data_y .== 1.0], trans_data_x_2[data_y .== 0.0]]\np1 = scatter(data_x[:,1], data_x[:,2], marker_z = data_y, size=(1200,400), c=:viridis, colorbar=false, title=\"original data\")\np2 = scatter(trans_data_x_1[:,1], trans_data_x_1[:,2], marker_z = data_y, c=:viridis, size=(1200,400), colorbar=false, title=\"|> warp\")\np3 = histogram(trans_data_x_2_split; stacked=true, bins=50, size=(1200,400), title=\"|> dot\")\nplot(p1, p2, p3, layout=(1,3), legend=false)","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"using StatsFuns: normcdf\np1 = scatter(data_x[:,1], data_x[:,2], marker_z = data_y, title=\"original labels\", xlabel=\"weight 1\", ylabel=\"weight 2\", size=(1200,400), c=:viridis)\np2 = scatter(data_x[:,1], data_x[:,2], marker_z = normcdf.(trans_data_x_2), title=\"predicted labels\", xlabel=\"weight 1\", ylabel=\"weight 2\", size=(1200,400), c=:viridis)\np3 = contour(0:0.01:1, 0:0.01:1, (x, y) -> normcdf(dot([1,1], ReactiveMP.forward(inferred_model, [x,y]))), title=\"Classification map\", xlabel=\"weight 1\", ylabel=\"weight 2\", size=(1200,400), c=:viridis)\nplot(p1, p2, p3, layout=(1,3), legend=false)","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"(Image: )","category":"page"},{"location":"contributing/new-example/#contributing-new-example","page":"Contributing to the examples","title":"Contributing to the examples","text":"","category":"section"},{"location":"contributing/new-example/","page":"Contributing to the examples","title":"Contributing to the examples","text":"We welcome all possible contributors. This page details some of the guidelines that should be followed when adding a new example (in the examples/ folder) to this package.","category":"page"},{"location":"contributing/new-example/","page":"Contributing to the examples","title":"Contributing to the examples","text":"In order to add a new example simply create a new Jupyter notebook with your experiments in the examples/\"subcategory\" folder. When creating a new example add a descriptive explanation of your experiments, model specification, inference constraints decisions and add appropriate results analysis. We expect examples to be readable for the general public and therefore highly value descriptive comments. If a submitted example only contains code, we will kindly request some changes to improve the readability.","category":"page"},{"location":"contributing/new-example/","page":"Contributing to the examples","title":"Contributing to the examples","text":"After preparing the new example it is necessary to modify the examples/.meta.jl file.","category":"page"},{"location":"contributing/new-example/","page":"Contributing to the examples","title":"Contributing to the examples","text":"Make sure that the very first cell of the notebook contains ONLY # <title> in it and has the markdown cell type. This is important for generating links in our documentation.\nThe path option must be set to a local path in a category sub-folder.\nThe text in the description option will be used on the Examples page in the documentation.\nSet category = :hidden_examples to hide a certain example in the documentation (the example will be executed to ensure it runs without errors).\nPlease do no use Overview as a name for the new example, the title Overview is reserved.\nUse the following template for equations, note that $$ and both \\begin and \\end commands are on the same line (check other examples if you are not sure). This is important, because otherwise formulas may not render correctly. Inline equations may use $...$ template.","category":"page"},{"location":"contributing/new-example/","page":"Contributing to the examples","title":"Contributing to the examples","text":"$$\\begin{aligned}\n      <latex equations here>\n\\end{aligned}$$","category":"page"},{"location":"contributing/new-example/","page":"Contributing to the examples","title":"Contributing to the examples","text":"When using equations, make sure not to follow the left-hand $$ or $ with a space, but instead directly start the equation, e.g. not $$ a + b $$, but $$a + b$$. For equations that are supposed to be on a separate line, make sure $$...$$ is preceded and followed by an empty line.\nNotebooks and plain Julia have different scoping rules for global variables. It may happen that the generation of your example fails due to an UndefVarError or other scoping issues. In these cases we recommend using let ... end blocks to enforce local scoping or use the global keyword to disambiguate the scoping rules, e.g.","category":"page"},{"location":"contributing/new-example/","page":"Contributing to the examples","title":"Contributing to the examples","text":"variable = 0\nfor i in 1:10\n    global variable = variable + i\nend","category":"page"},{"location":"contributing/new-example/","page":"Contributing to the examples","title":"Contributing to the examples","text":"All examples must use and activate the local environment specified by Project.toml in the second cell (see 1.). Please have a look at the existing notebooks for an example on how to activate this local environment. If you need additional packages, you can add then to the (examples) project.\nAll plots should be displayed automatically. In special cases, if needed, save figures in the ../pics/figure-name.ext format. Might be useful for saving gifs. Use ![](../pics/figure-name.ext) to display a static image.","category":"page"},{"location":"contributing/new-example/","page":"Contributing to the examples","title":"Contributing to the examples","text":"note: Note\nPlease avoid adding PyPlot in the (examples) project. Installing and building PyPlot dependencies takes several minutes on every CI run. Use Plots instead.","category":"page"},{"location":"contributing/new-example/","page":"Contributing to the examples","title":"Contributing to the examples","text":"note: Note\nUse make examples to run all examples or make examples specific=MyNewCoolNotebook to run any notebook that includes MyNewCoolNotebook in its file name.","category":"page"},{"location":"manuals/getting-started/#user-guide-getting-started","page":"Getting started","title":"Getting started","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"RxInfer.jl is a Julia package for Bayesian Inference on Factor Graphs by Message Passing.  It supports both exact and variational inference algorithms and forms an ecosystem around three main packages: ","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"ReactiveMP.jl - the underlying message passing-based inference engine\nGraphPPL.jl - model and constraints specification package\nRocket.jl - reactive extensions package for Julia ","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"This page provides the necessary information you need to get started with Rxinfer. We will show the general approach to solving inference problems with RxInfer by means of a running example: inferring the bias of a coin using a simple Beta-Bernoulli model.","category":"page"},{"location":"manuals/getting-started/#Installation","page":"Getting started","title":"Installation","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"RxInfer is an officially registered Julia package. Install RxInfer through the Julia package manager by using the following command from the package manager mode:","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"] add RxInfer","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Alternatively:","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"julia> import Pkg\n\njulia> Pkg.add(\"RxInfer\")","category":"page"},{"location":"manuals/getting-started/#Importing-RxInfer","page":"Getting started","title":"Importing RxInfer","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"To add RxInfer package (and all associated packages) into a running Julia session simply run:","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"using RxInfer","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Read more about about using in the Using methods from RxInfer section of the documentation.","category":"page"},{"location":"manuals/getting-started/#Example:-Inferring-the-bias-of-a-coin","page":"Getting started","title":"Example: Inferring the bias of a coin","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"The RxInfer approach to solving inference problems consists of three phases:","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Model specification: RxInfer uses GraphPPL package for model specification part. It offers a domain-specific language to specify your probabilistic model.\nInference specification: RxInfer inference API uses ReactiveMP inference engine under the hood and has been designed to be as flexible as possible. It is compatible both with asynchronous infinite data streams and with static datasets. For most of the use cases it consists of the same simple building blocks. In this example we will show one of the many possible ways to infer your quantities of interest.\nInference execution: Given model specification and inference procedure it is pretty straightforward to use package's API to pass data to the inference backend and to run actual inference.","category":"page"},{"location":"manuals/getting-started/#user-guide-getting-started-coin-flip-simulation","page":"Getting started","title":"Coin flip simulation","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Let's start by creating some dataset. One approach could be flipping a coin N times and recording each outcome. For simplicity in this example we will use static pre-generated dataset. Each sample can be thought of as the outcome of single flip which is either heads or tails (1 or 0). We will assume that our virtual coin is biased, and lands heads up on 75% of the trials (on average).","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"First let's setup our environment by importing all needed packages:","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"using RxInfer, Distributions, Random","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Next, let's define our dataset:","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"# Random number generator for reproducibility\nrng            = MersenneTwister(42)\n# Number of coin flips (observations)\nn_observations = 10\n# The bias of a coin used in the demonstration\ncoin_bias      = 0.75\n# We assume that the outcome of each coin flip is \n# distributed as the `Bernoulli` distrinution\ndistribution   = Bernoulli(coin_bias)\n# Simulated coin flips\ndataset        = float.(rand(rng, distribution, n_observations))","category":"page"},{"location":"manuals/getting-started/#getting-started-model-specification","page":"Getting started","title":"Model specification","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"In a Bayesian setting, the next step is to specify our probabilistic model. This amounts to specifying the joint probability of the random variables of the system.","category":"page"},{"location":"manuals/getting-started/#Likelihood","page":"Getting started","title":"Likelihood","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"We've made an assumption that the outcome of each coin flip is governed by the Bernoulli distribution, i.e.","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"y_i sim mathrmBernoulli(theta)","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"where y_i = 1 represents \"heads\", y_i = 0 represents \"tails\". The underlying probability of the coin landing heads up for a single coin flip is theta in 01.","category":"page"},{"location":"manuals/getting-started/#Prior","page":"Getting started","title":"Prior","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"We will choose the conjugate prior of the Bernoulli likelihood function defined above, namely the Beta distribution, i.e.","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"theta sim Beta(a b)","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"where a and b are the hyperparameters that encode our prior beliefs about the possible values of theta. We will assign values to the hyperparameters in a later step.   ","category":"page"},{"location":"manuals/getting-started/#Joint-probability","page":"Getting started","title":"Joint probability","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"The joint probability is given by the multiplication of the likelihood and the prior, i.e.","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"P(y_1N θ) = P(θ) prod_i=1^N P(y_i  θ)","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Now let's see how to specify this model using GraphPPL's package syntax.","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"# GraphPPL.jl export `@model` macro for model specification\n# It accepts a regular Julia function and builds an FFG under the hood\n@model function coin_model(y, a, b)\n    # We endow θ parameter of our model with some prior\n    θ ~ Beta(a, b)\n    # or, in this particular case, the `Uniform(0.0, 1.0)` prior also works:\n    # θ ~ Uniform(0.0, 1.0)\n\n    # We assume that outcome of each coin flip is governed by the Bernoulli distribution\n    for i in eachindex(y)\n        y[i] ~ Bernoulli(θ)\n    end\nend","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"As you can see, RxInfer offers a model specification syntax that resembles closely to the mathematical equations defined above. Alternatively, we could use a broadcasting syntax:","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"@model function coin_model(y, a, b) \n    θ  ~ Beta(a, b)\n    y .~ Bernoulli(θ) \nend","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"note: Note\nTo quickly check the list of all available factor nodes that can be used in the model specification language call ?ReactiveMP.is_predefined_node or Base.doc(ReactiveMP.is_predefined_node).","category":"page"},{"location":"manuals/getting-started/#getting-started-conditioning","page":"Getting started","title":"Conditioning on data and inspecting the model structure","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Given the model specification we can construct an actual model graph and visualize it. In order to do that we can use the | operator to condition on data and the RxInfer.create_model function to create the graph. Read more about condition in the corresponding section of the documentation.","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"conditioned = coin_model(a = 2.0, b = 7.0) | (y = [ 1.0, 0.0, 1.0 ], )","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"We can use GraphPPL.jl visualisation capabilities to show the structure of the resulting graph. For that we need two extra packages installed: Cairo and GraphPlot. Note, that those packages are not included in the RxInfer package and must be installed separately.","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"using Cairo, GraphPlot\n\n# `Create` the actual graph of the model conditioned on the data\nmodel = RxInfer.create_model(conditioned)\n\n# Call `gplot` function from `GraphPlot` to visualise the structure of the graph\nGraphPlot.gplot(RxInfer.getmodel(model))","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"In addition, we can also programatically query the structure of the graph:","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"RxInfer.getrandomvars(model)","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"RxInfer.getdatavars(model)","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"RxInfer.getconstantvars(model)","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"RxInfer.getfactornodes(model)","category":"page"},{"location":"manuals/getting-started/#Conditioning-on-data-that-is-not-available-at-model-creation-time","page":"Getting started","title":"Conditioning on data that is not available at model creation time","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Sometimes the data is not known at model creation time, for example, during reactive inference. For that purpose RxInfer uses RxInfer.DefferedDataHandler structure.","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"# The only difference here is that we do not specify `a` and `b` as hyperparameters \n# But rather indicate that the data for them will be available later during the inference\nconditioned_with_deffered_data = coin_model() | (\n    y = [ 1.0, 0.0, 1.0 ], \n    a = RxInfer.DefferedDataHandler(), \n    b = RxInfer.DefferedDataHandler()\n)\n\n# The graph creation API does not change\nmodel_with_deffered_data = RxInfer.create_model(conditioned_with_deffered_data)\n\n# We can visualise the graph with missing data handles as well\nGraphPlot.gplot(RxInfer.getmodel(model_with_deffered_data))","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"From the model structure visualisation we can see now that both a and b are no longer indicated as constants. Read more about the structure of the graph in GraphPPL documentation.","category":"page"},{"location":"manuals/getting-started/#getting-started-inference-specification","page":"Getting started","title":"Inference specification","text":"","category":"section"},{"location":"manuals/getting-started/#Automatic-inference-specification","page":"Getting started","title":"Automatic inference specification","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Once we have defined our model, the next step is to use RxInfer API to infer quantities of interests. To do this we can use a generic infer function that supports static datasets. Read more information about the infer function in the Inference Execution documentation section.","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"result = infer(\n    model = coin_model(a = 2.0, b = 7.0),\n    data  = (y = dataset, )\n)","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"As you can see we don't need to condition on the data manually, the infer function will do it automatically. After the inference is complete we can fetch the results from the .posterior field with the name of the latent state:","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"θestimated = result.posteriors[:θ]","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"We can also compute some statistical properties of the result:","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"println(\"Real bias is \", coin_bias)\nprintln(\"Estimated bias is \", mean(θestimated))\nprintln(\"Standard deviation \", std(θestimated))\nnothing #hide","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Let's also visualize the resulting posteriors:","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"using Plots\n\nrθ = range(0, 1, length = 1000)\n\np1 = plot(rθ, (x) -> pdf(Beta(2.0, 7.0), x), title=\"Prior\", fillalpha=0.3, fillrange = 0, label=\"P(θ)\", c=1,)\np2 = plot(rθ, (x) -> pdf(θestimated, x), title=\"Posterior\", fillalpha=0.3, fillrange = 0, label=\"P(θ|y)\", c=3)\n\nplot(p1, p2, layout = @layout([ a; b ]))","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"In our dataset we used 10 coin flips and skewed prior to estimate the bias of a coin.  It resulted in a vague posterior distribution, however RxInfer scales very well for large models and factor graphs.  We may use more coin flips in our dataset for better posterior distribution estimates:","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"dataset_100   = float.(rand(rng, Bernoulli(coin_bias), 100))\ndataset_1000  = float.(rand(rng, Bernoulli(coin_bias), 1000))\ndataset_10000 = float.(rand(rng, Bernoulli(coin_bias), 10000))\nnothing # hide","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"θestimated_100   = infer(model = coin_model(a = 2.0, b = 7.0), data  = (y = dataset_100, ))\nθestimated_1000  = infer(model = coin_model(a = 2.0, b = 7.0), data  = (y = dataset_1000, ))\nθestimated_10000 = infer(model = coin_model(a = 2.0, b = 7.0), data  = (y = dataset_10000, ))\nnothing #hide","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Let's investigate how the number of observation affects the estimated posterior:","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"p3 = plot(title = \"Posterior\", legend = :topleft)\np3 = plot!(p3, rθ, (x) -> pdf(θestimated_100.posteriors[:θ], x), fillalpha = 0.3, fillrange = 0, label = \"P(θ|y_100)\", c = 4)\np3 = plot!(p3, rθ, (x) -> pdf(θestimated_1000.posteriors[:θ], x), fillalpha = 0.3, fillrange = 0, label = \"P(θ|y_1000)\", c = 5)\np3 = plot!(p3, rθ, (x) -> pdf(θestimated_10000.posteriors[:θ], x), fillalpha = 0.3, fillrange = 0, label = \"P(θ|y_10000)\", c = 6)\n\nplot(p1, p3, layout = @layout([ a; b ]))","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"We can see that with larger dataset our posterior marginal estimate becomes more and more accurate and represents real value of the bias of a coin.","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"println(\"Real bias is \", coin_bias)\nprintln(\"Estimated bias is \", mean(θestimated_10000.posteriors[:θ]))\nprintln(\"Standard deviation \", std(θestimated_10000.posteriors[:θ]))\nnothing #hide","category":"page"},{"location":"manuals/getting-started/#Where-to-go-next?","page":"Getting started","title":"Where to go next?","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"There are a set of examples available in RxInfer repository that demonstrate the more advanced features of the package for various problems. Alternatively, you can head to the Model specification which provides more detailed information of how to use RxInfer to specify probabilistic models. Inference execution section provides a documentation about RxInfer API for running reactive Bayesian inference. Also read the Comparison to compare RxInfer with other probabilistic programming libraries.","category":"page"},{"location":"manuals/debugging/#user-guide-debugging","page":"Debugging","title":"Debugging","text":"","category":"section"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"Debugging inference in RxInfer can be quite challenging, mostly due to the reactive nature of the inference, undefined order of computations, the use of observables, and generally hard-to-read stack traces in Julia. Below we discuss ways to help you find problems in your model that prevents you from getting the results you want. ","category":"page"},{"location":"manuals/debugging/#Requesting-a-trace-of-messages","page":"Debugging","title":"Requesting a trace of messages","text":"","category":"section"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"RxInfer provides a way that allows to save the history of the computations leading up to the computed messages and marginals in the inference procedure. This history is added on top of messages and marginals and is referred to as a Memory Addon. Below is an example explaining how you can extract this history and use it to fix a bug.","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"note: Note\nAddons is a feature of ReactiveMP. Read more about implementing custom addons in the corresponding section of ReactiveMP package.","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"We show the application of the Memory Addon on the coin toss example from earlier in the documentation. We model the binary outcome x (heads or tails) using a Bernoulli distribution, with a parameter theta that represents the probability of landing on heads. We have a Beta prior distribution for the theta parameter, with a known shape alpha and rate beta parameter.","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"theta sim mathrmBeta(a b)","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"x_i sim mathrmBernoulli(theta)","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"where x_i in 0 1 are the binary observations (heads = 1, tails = 0). This is the corresponding RxInfer model:","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"using RxInfer, Random, Plots\n\nn = 4\nθ_real = 0.3\ndataset = float.(rand(Bernoulli(θ_real), n))\n\n@model function coin_model(x)\n    θ  ~ Beta(4, huge)\n    x .~ Bernoulli(θ)\nend\n\nresult = infer(\n    model = coin_model(), \n    data  = (x = dataset, ),\n)","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"The model will run without errors. But when we plot the posterior distribution for theta, something's wrong. The posterior seems to be a flat distribution:","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"\nrθ = range(0, 1, length = 1000)\n\nplot(rθ, (rvar) -> pdf(result.posteriors[:θ], rvar), label=\"Infered posterior\")\nvline!([θ_real], label=\"Real θ\", title = \"Inference results\")","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"We can figure out what's wrong by tracing the computation of the posterior with the Memory Addon.  To obtain the trace, we have to add addons = (AddonMemory(),) as an argument to the inference function.  Note, that the argument to the addons keyword argument must be a tuple, because multiple addons can be activated  at the same time. Here, we create a tuple with a single element however.","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"result = infer(\n    model = coin_model(), \n    data  = (x = dataset, ),\n    addons = (AddonMemory(),)\n)","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"Now we have access to the messages that led to the marginal posterior:","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"RxInfer.ReactiveMP.getaddons(result.posteriors[:θ])","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"(Image: Addons_messages)","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"The messages in the factor graph are marked in color. If you're interested in the mathematics behind these results, consider verifying them manually using the general equation for sum-product messages:","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"underbraceoverrightarrowmu_θ(θ)_substack textoutgoing textmessage = sum_x_1ldotsx_n underbraceoverrightarrowmu_X_1(x_1)cdots overrightarrowmu_X_n(x_n)_substacktextincoming  textmessages cdot underbracef(θx_1ldotsx_n)_substacktextnode textfunction","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"(Image: Graph)","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"Note that the posterior (yellow) has a rate parameter on the order of 1e12. Our plot failed because a Beta distribution with such a rate parameter cannot be accurately depicted using the range of theta we used in the code block above. So why does the posterior have this rate parameter?","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"All the observations (purple, green, pink, blue) have much smaller rate parameters. It seems the prior distribution (red) has an unusual rate parameter, namely 1e12. If we look back at the model, the parameter was set to huge (which is a reserved keyword meaning 1e12). Reducing the prior rate parameter will ensure the posterior has a reasonable rate parameter as well.","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"@model function coin_model(x)\n    θ  ~ Beta(4, 100)\n    x .~ Bernoulli(θ)\nend\n\nresult = infer(\n    model = coin_model(), \n    data  = (x = dataset, ),\n)","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"rθ = range(0, 1, length = 1000)\n\nplot(rθ, (rvar) -> pdf(result.posteriors[:θ], rvar), fillalpha = 0.4, fill = 0, label=\"Infered posterior\")\nvline!([θ_real], label=\"Real θ\", title = \"Inference results\")","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"Now the posterior has much more sensible shape thus confirming that we have identified the original issue correctly.  We can run the model with more observations, to get an even better posterior:","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"result = infer(\n    model = coin_model(), \n    data  = (x = float.(rand(Bernoulli(θ_real), 1000)), ),\n)\n\nrθ = range(0, 1, length = 1000)\nplot(rθ, (rvar) -> pdf(result.posteriors[:θ], rvar), fillalpha = 0.4, fill = 0, label=\"Infered posterior (1000 observations)\")\nvline!([θ_real], label=\"Real θ\", title = \"Inference results\")","category":"page"},{"location":"manuals/debugging/#user-guide-debugging-callbacks","page":"Debugging","title":"Using callbacks in the infer function","text":"","category":"section"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"Another way to inspect the inference procedure is to use the callbacks or events from the infer function. Read more about callbacks in the documentation to the infer function. Here, we show a simple application of callbacks to a simple IID inference problem. We start with model specification:","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"using RxInfer\n\n@model function iid_normal(y)\n    μ  ~ Normal(mean = 0.0, variance = 100.0)\n    γ  ~ Gamma(shape = 1.0, rate = 1.0)\n    y .~ Normal(mean = μ, precision = γ)\nend","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"Next, let us define a syntehtic dataset:","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"dataset = rand(NormalMeanPrecision(3.1415, 30.0), 100)\nnothing #hide","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"Now, we can use the callbacks argument of the infer function to track the order of posteriors computation and their intermediate values for each variational iteration:","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"# A callback that will be called every time before a variational iteration starts\nfunction before_iteration_callback(model, iteration)\n    println(\"Starting iteration \", iteration)\nend\n\n# A callback that will be called every time after a variational iteration finishes\nfunction after_iteration_callback(model, iteration)\n    println(\"Iteration \", iteration, \" has been finished\")\nend\n\n# A callback that will be called every time a posterior is updated\nfunction on_marginal_update_callback(model, variable_name, posterior)\n    println(\"Latent variable \", variable_name, \" has been updated. Estimated mean is \", mean(posterior), \" with standard deviation \", std(posterior))\nend","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"After we have defined all callbacks of interest, we can call the infer function passing them in the callback argument as a named tuple:","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"init = @initialization begin \n    q(μ) = vague(NormalMeanVariance)\nend\n\nresult = infer(\n    model = iid_normal(),\n    data  = (y = dataset, ),\n    constraints = MeanField(),\n    iterations = 5,\n    initialization = init,\n    returnvars = KeepLast(),\n    callbacks = (\n        on_marginal_update = on_marginal_update_callback,\n        before_iteration   = before_iteration_callback,\n        after_iteration    = after_iteration_callback\n    )\n)\nnothing #hide","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"We can see that the callback has been correctly executed for each intermediate variational iteration.","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"println(\"Estimated mean: \", mean(result.posteriors[:μ]))\nprintln(\"Estimated precision: \", mean(result.posteriors[:γ]))\nnothing #hide","category":"page"},{"location":"manuals/debugging/#Using-LoggerPipelineStage","page":"Debugging","title":"Using LoggerPipelineStage","text":"","category":"section"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"ReactiveMP inference engine allows attaching extra computations to the default computational pipeline of message passing.  Read more about pipelines in the corresponding section of ReactiveMP. Here we show how to use LoggerPipelineStage to trace the order of message passing updates for debugging purposes. We start with model specification:","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"using RxInfer\n\n@model function iid_normal_with_pipeline(y)\n    μ  ~ Normal(mean = 0.0, variance = 100.0)\n    γ  ~ Gamma(shape = 1.0, rate = 1.0)\n    y .~ Normal(mean = μ, precision = γ) where { pipeline = LoggerPipelineStage() }\nend","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"Next, let us define a syntehtic dataset:","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"# We use less data points in the dataset to reduce the amount of text printed\n# during the inference\ndataset = rand(NormalMeanPrecision(3.1415, 30.0), 5)\nnothing #hide","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"Now, we can call the infer function. We combine the pipeline logger stage with the callbacks, which were introduced in the previous section:","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"result = infer(\n    model = iid_normal_with_pipeline(),\n    data  = (y = dataset, ),\n    constraints = MeanField(),\n    iterations = 5,\n    initialization = init,\n    returnvars = KeepLast(),\n    callbacks = (\n        on_marginal_update = on_marginal_update_callback,\n        before_iteration   = before_iteration_callback,\n        after_iteration    = after_iteration_callback\n    )\n)\nnothing #hide","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"We can see the order of message update events. Note that ReactiveMP may decide to compute messages lazily, in which case the actual computation of the value of a message will be deffered until later moment. In this case, LoggerPipelineStage will report DefferedMessage.","category":"page"},{"location":"manuals/inference/streamlined/#manual-online-inference","page":"Streamline inference","title":"Streaming (online) inference","text":"","category":"section"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"This guide explains how to use the infer function for dynamic datasets. We show how RxInfer can continuously update beliefs asynchronously whenever a new observation arrives. We use a simple Beta-Bernoulli model as an example, which has been covered in the Getting Started section,  however, these techniques can be applied to any model","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Also read about Static Inference or checkout more complex examples.","category":"page"},{"location":"manuals/inference/streamlined/#manual-online-inference-model-spec","page":"Streamline inference","title":"Model specification","text":"","category":"section"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Also read the Model Specification section.","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"In online inference, we want to continuously update our prior beliefs about certain hidden states.  To achieve this, we include extra arguments in our model specification to allow for dynamic prior changes:","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"using RxInfer\nusing Test #hide\n\n@model function beta_bernoulli_online(y, a, b)\n    θ ~ Beta(a, b)  \n    y ~ Bernoulli(θ)\nend","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"In this model, we assume we only have one observation y at a time, and the a and b parameters are not fixed to specific values but rather are arguments of the model itself.","category":"page"},{"location":"manuals/inference/streamlined/#manual-online-inference-autoupdates","page":"Streamline inference","title":"Automatic prior update","text":"","category":"section"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Next, we want to enable RxInfer to automatically update the a and b parameters as soon as a new posterior for θ is available. To accomplish this, we utilize the @autoupdates macro.","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"beta_bernoulli_autoupdates = @autoupdates begin \n    # We want to update `a` and `b` to be equal to the parameters \n    # of the current posterior for `θ`\n    a, b = params(q(θ))\nend","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"This specification instructs RxInfer to update a and b parameters automatically as as soon as a new posterior for θ is available.","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"@autoupdates","category":"page"},{"location":"manuals/inference/streamlined/#RxInfer.@autoupdates","page":"Streamline inference","title":"RxInfer.@autoupdates","text":"@autoupdates\n\nCreates the auto-updates specification for the infer function. In the online-streaming Bayesian inference procedure it is important to update your priors for the future  states based on the new updated posteriors. The @autoupdates structure simplify such a specification. It accepts a single block of code where each line defines how to update  arguments in the probabilistic model specification. \n\nEach line of code in the auto-update specification refers to model's arguments, which need to be updated, on the left hand side of the equality expression and the update function on the right hand side of the expression. The update function operates on posterior marginals in the form of the q(symbol) expression.\n\nFor example:\n\n@autoupdates begin \n    x = f(q(z))\nend\n\nThis structure specifies to automatically update argument x as soon as the inference engine computes new posterior over z variable. It then applies the f function to the new posterior and updates the value of x automatically. \n\nAs an example consider the following model and auto-update specification:\n\n@model function kalman_filter(y, x_current_mean, x_current_var)\n    x_current ~ Normal(mean = x_current_mean, var = x_current_var)\n    x_next    ~ Normal(mean = x_current, var = 1.0)\n    y         ~ Normal(mean = x_next, var = 1.0)\nend\n\nThis model has two arguments that represent our prior knowledge of the x_current state of the system.  The x_next random variable represent the next state of the system that  is connected to the observed variable y. The auto-update specification could look like:\n\nautoupdates = @autoupdates begin\n    x_current_mean, x_current_var = mean_var(q(x_next))\nend\n\nThis structure specifies to update our prior as soon as we have a new posterior q(x_next). It then applies the mean_var function on the  updated posteriors and updates x_current_mean and x_current_var automatically.\n\nSee also: infer\n\n\n\n\n\n","category":"macro"},{"location":"manuals/inference/streamlined/#manual-online-inference-async-datastream","page":"Streamline inference","title":"Asynchronous data stream of observations","text":"","category":"section"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"For demonstration purposes, we use a handcrafted stream of observations with the Rocket.jl library","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"using Rocket, Distributions, StableRNGs\n\nhidden_θ     = 1 / 3.1415\ndistribution = Bernoulli(hidden_θ)\nrng          = StableRNG(43)\ndatastream   = RecentSubject(Bool)","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"The infer function expects the datastream to emit values in the form of the NamedTuples. To simplify this process, Rocket.jl exports labeled function. We also use the combineLatest function to convert a stream of Bools to a stream of Tuple{Bool}s. Read more about these function in the documentation to Rocket.jl.","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"observations = labeled(Val((:y, )), combineLatest(datastream))","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Let's verify that our datastream does indeed produce NamedTuples","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"test_values = [] #hide\ntest_subscription = subscribe!(observations, (new_observation) -> push!(test_values, new_observation)) #hide\nsubscription = subscribe!(observations, \n    (new_observation) -> println(\"Got new observation \", new_observation, \" 🎉\")\n)\nnothing #hide","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"for i in 1:5\n    next!(datastream, rand(rng, distribution))\nend\n@test length(test_values) === 5 #hide\n@test all(value -> haskey(value, :y) && (isone(value[:y]) || iszero(value[:y])), test_values) #hide \nnothing #hide","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Nice! Our data stream produces events in a form of the NamedTuples, which is compatible with the infer function.","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"unsubscribe!(test_subscription) #hide\n# It is important to keep track of the existing susbcriptions\n# and unsubscribe to reduce the usage of computational resources\nunsubscribe!(subscription)","category":"page"},{"location":"manuals/inference/streamlined/#manual-online-inference-inst-reactive-engine","page":"Streamline inference","title":"Instantiating the reactive inference engine","text":"","category":"section"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Now, we have everything ready to start running the inference with RxInfer on dynamic datasets with the infer function:","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"engine = infer(\n    model          = beta_bernoulli_online(),\n    datastream     = observations,\n    autoupdates    = beta_bernoulli_autoupdates,\n    returnvars     = (:θ, ),\n    initialization = @initialization(q(θ) = Beta(1, 1)),\n    autostart      = false\n)","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"In the code above, there are several notable differences compared to running inference for static datasets. Firstly, we utilized the autoupdates argument as discussed previously. Secondly, we employed the @initialization macro to initialize the posterior over θ. This is necessary for the @autoupdates macro, as it needs to initialize the a and b parameters before the data becomes available. Thirdly, we set autostart = false to indicate that we do not want to immediately subscribe to the datastream, but rather do so manually later using the RxInfer.start function. The returnvars specification differs a little from Static Inference. In reactive inference, the returnvars = (:θ, ) must be a tuple of Symbols and specifies that we would be interested to get a stream of posteriors update for θ. The returnvars specification is optional and the inference engine will create reactive streams for all latent states if ommited.","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"RxInferenceEngine\nRxInfer.start\nRxInfer.stop","category":"page"},{"location":"manuals/inference/streamlined/#RxInfer.RxInferenceEngine","page":"Streamline inference","title":"RxInfer.RxInferenceEngine","text":"RxInferenceEngine\n\nThe return value of the infer function in case of streamlined inference. \n\nPublic fields\n\nposteriors: Dict or NamedTuple of 'random variable' - 'posterior stream' pairs. See the returnvars argument for the infer.\nfree_energy: (optional) A stream of Bethe Free Energy values per VMP iteration. See the free_energy argument for the infer.\nhistory: (optional) Saves history of previous marginal updates. See the historyvars and keephistory arguments for the infer.\nfree_energy_history: (optional) Free energy history, averaged across variational iterations value for all observations  \nfree_energy_raw_history: (optional) Free energy history, returns returns computed values of all variational iterations for each data event (if available)\nfree_energy_final_only_history: (optional) Free energy history, returns computed values of final variational iteration for each data event (if available)\nevents: (optional) A stream of events send by the inference engine. See the events argument for the infer.\nmodel: ProbabilisticModel object reference.\n\nUse the RxInfer.start(engine) function to subscribe on the datastream source and start the inference procedure.  Use RxInfer.stop(engine) to unsubscribe from the datastream source and stop the inference procedure.  Note, that it is not always possible to start/stop the inference procedure.\n\nSee also: infer, RxInferenceEvent, RxInfer.start, RxInfer.stop\n\n\n\n\n\n","category":"type"},{"location":"manuals/inference/streamlined/#RxInfer.start","page":"Streamline inference","title":"RxInfer.start","text":"start(engine::RxInferenceEngine)\n\nStarts the RxInferenceEngine by subscribing to the data source, instantiating free energy (if enabled) and starting the event loop. Use RxInfer.stop to stop the RxInferenceEngine. Note that it is not always possible to stop/restart the engine and this depends on the data source type.\n\nSee also: RxInfer.stop\n\n\n\n\n\n","category":"function"},{"location":"manuals/inference/streamlined/#RxInfer.stop","page":"Streamline inference","title":"RxInfer.stop","text":"stop(engine::RxInferenceEngine)\n\nStops the RxInferenceEngine by unsubscribing to the data source, free energy (if enabled) and stopping the event loop. Use RxInfer.start to start the RxInferenceEngine again. Note that it is not always possible to stop/restart the engine and this depends on the data source type.\n\nSee also: RxInfer.start\n\n\n\n\n\n","category":"function"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Given the engine, we now can subscribe on the posterior updates:","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"θ_updates_for_testing_the_example  = [] #hide\nθ_updates_for_testing_subscription = subscribe!(engine.posteriors[:θ], (new_posterior_for_θ) -> push!(θ_updates_for_testing_the_example, new_posterior_for_θ)) #hide\nθ_updates_subscription = subscribe!(engine.posteriors[:θ], \n    (new_posterior_for_θ) -> println(\"A new posterior for θ is \", new_posterior_for_θ, \" 🤩\")\n)\nnothing #hide","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"In this setting, we should get a message every time a new posterior is available for θ. Let's try to generate a new observation!","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"next!(datastream, rand(rng, distribution))\n@test isempty(θ_updates_for_testing_the_example) #hide\nnothing #hide","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Hmm, nothing happened...? Oh, we forgot to start the engine with the RxInfer.start function. Let's do that now:","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"RxInfer.start(engine)\n@test length(θ_updates_for_testing_the_example) === 1 #hide\nnothing #hide","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Ah, as soon as we start our engine, we receive the posterior for θ. This occurred because we initialized our stream as RecentSubject, which retains the most recent value and emits it upon subscription. Our engine automatically subscribed to the observations and obtained the most recent value, initiating inference. Let's see if we can add more observations:","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"next!(datastream, rand(rng, distribution))\n@test length(θ_updates_for_testing_the_example) === 2 #hide\nnothing #hide","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Great! We got another posterior! Let's try a few more observations:","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"for i in 1:5\n    next!(datastream, rand(rng, distribution))\nend\n@test length(θ_updates_for_testing_the_example) === 7 #hide\nnothing #hide","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"As demonstrated, the reactive engine reacts to new observations and performs inference as soon as a new observation is available. But what if we want to maintain a history of posteriors? The infer function supports the historyvars and keephistory arguments precisely for that purpose. In the next section we reinstantiate our engine, with the keephistory argument enabled, but first, we must shutdown the previous engine and unsubscribe from its posteriors:","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"RxInfer.stop(engine)\nunsubscribe!(θ_updates_subscription)\nunsubscribe!(θ_updates_for_testing_subscription) #hide\nnothing #hide","category":"page"},{"location":"manuals/inference/streamlined/#manual-online-inference-history","page":"Streamline inference","title":"Keeping the history of posteriors","text":"","category":"section"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"To retain the history of posteriors within the engine, we can utilize the keephistory and historyvars arguments. The keephistory parameter specifies the length of the circular buffer for storing the history of posterior updates, while historyvars determines what variables to save in the history and how often to save them (e.g., every iteration or only at the end of iterations).","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"engine = infer(\n    model          = beta_bernoulli_online(),\n    datastream     = observations,\n    autoupdates    = beta_bernoulli_autoupdates,\n    initialization = @initialization(q(θ) = Beta(1, 1)),\n    keephistory    = 100,\n    historyvars    = (θ = KeepLast(), ),\n    autostart      = true\n)","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"In the example above, we specified that we want to store at most 100 posteriors for θ, and KeepLast() indicates that we are only interested in the final value of θ and not in intermediate values during variational iterations. We also specified the autostart = true to start the engine automatically without need for RxInfer.start and RxInfer.stop.","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"note: Note\nIn this model, we do not utilize the iterations argument, indicating that we perform a single VMP iteration. If multiple iterations were employed, engine.posteriors[:θ] would emit every intermediate value.","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Now, we can feed some more observations to the datastream:","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"for i in 1:5\n    next!(datastream, rand(rng, distribution))\nend","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"And inspect the engine.history[:θ] buffer:","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"@test length(engine.history[:θ]) === 6 #hide\nengine.history[:θ]","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"As we can see the buffer correctly saved the posteriors in the .history buffer.","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"note: Note\nWe have 6 entries, despite having only 5 new observations. As mentioned earlier, this occurs because we initialized our datastream as a RecentSubject, which retains the most recent observation and emits it each time a new subscription occurs.","category":"page"},{"location":"manuals/inference/streamlined/#manual-online-inference-history-visualization","page":"Streamline inference","title":"Visualizing the history of posterior estimation","text":"","category":"section"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Let's feed more observation and visualize how the posterior changes over time:","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"for i in 1:94\n    next!(datastream, rand(rng, distribution))\nend\n@test length(engine.history[:θ]) === 100 #hide\nnothing #hide","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"To visualize the history of posteriors we use the @gif macro from the Plots package:","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"using Plots\n\n@gif for posterior in engine.history[:θ]\n    rθ = range(0, 1, length = 1000)\n    pθ = plot(rθ, (x) -> pdf(posterior, x), fillalpha=0.3, fillrange = 0, label=\"P(θ|y)\", c=3)\n    pθ = vline!(pθ, [ hidden_θ ], label = \"Real value of θ\")\n\n    plot(pθ)\nend","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"We can keep feeding data to our datastream, but only last 100 posteriors will be saved in the history buffer:","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"for i in 1:200\n    next!(datastream, rand(rng, distribution))\nend\n\n@test length(engine.history[:θ]) === 100 #hide\n@gif for posterior in engine.history[:θ]\n    rθ = range(0, 1, length = 1000)\n    pθ = plot(rθ, (x) -> pdf(posterior, x), fillalpha=0.3, fillrange = 0, label=\"P(θ|y)\", c=3)\n    pθ = vline!(pθ, [ hidden_θ ], label = \"Real value of θ\")\n\n    plot(pθ)\nend","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"note: Note\nIt is also possible to visualize the inference estimation continously with manual subscription to engine.posteriors[:θ].","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"As previously it is important to shutdown the inference engine when it becomes unnecessary:","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"RxInfer.stop(engine)","category":"page"},{"location":"manuals/inference/streamlined/#manual-online-inference-free-energy","page":"Streamline inference","title":"Subscribing on the stream of free energy","text":"","category":"section"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"To obtain a continuous stream of updates for the Bethe Free Energy, we need to initialize the engine with the free_energy argument set to true:","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"engine = infer(\n    model          = beta_bernoulli_online(),\n    datastream     = observations,\n    autoupdates    = beta_bernoulli_autoupdates,\n    initialization = @initialization(q(θ) = Beta(1, 1)),\n    keephistory    = 5,\n    autostart      = true,\n    free_energy    = true\n)","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"note: Note\nIt's important to use the keephistory argument alongside the free_energy argument because setting free_energy = true also maintains an internal circular buffer to track its previous updates.","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"free_energy_for_testing = [] #hide\nfree_energy_for_testing_subscription = subscribe!(engine.free_energy, (v) -> push!(free_energy_for_testing, v)) #hide\nfree_energy_subscription = subscribe!(engine.free_energy, \n    (bfe_value) -> println(\"New value of Bethe Free Energy has been computed \", bfe_value, \" 👩‍🔬\")\n)\n@test length(free_energy_for_testing) === 1 #hide\nnothing #hide","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Let's emit more observations:","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"for i in 1:5\n    next!(datastream, rand(rng, distribution))\nend\n@test length(free_energy_for_testing) === 6 #hide\nnothing #hide","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"In this particular example, we do not perform any variational iterations and do not use any variational constraints, hence, the inference is exact. In this case the BFE values are equal to the minus log-evidence of the model given new observation.  We can also track history of Bethe Free Energy values with the following fields of the engine:","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"free_energy_history: free energy history, averaged across variational iterations value for all observations  \nfree_energy_raw_history: free energy history, returns returns computed values of all variational iterations for each data event (if available)\nfree_energy_final_only_history: free energy history, returns computed values of final variational iteration for each data event (if available)","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"@test length(engine.free_energy_history) === 1 #hide\nengine.free_energy_history","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"@test length(engine.free_energy_raw_history) === 5 #hide\nengine.free_energy_raw_history","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"@test length(engine.free_energy_final_only_history) === 5 #hide\nengine.free_energy_final_only_history","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"unsubscribe!(free_energy_for_testing_subscription) #hide\n# Stop the engine when not needed as usual\nRxInfer.stop(engine)\nunsubscribe!(free_energy_subscription)","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"As has been mentioned, in this particular example we do not perform variational iterations, hence, there is little different between different representations of the BFE history buffers. However, when performing variational inference with the iterations argument, those buffers will be different. To demonstrate this difference let's build a slightly more complex model with variational constraints:","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"@model function iid_normal(y, mean_μ, var_μ, shape_τ, rate_τ)\n    μ ~ Normal(mean = mean_μ, var = var_μ)\n    τ ~ Gamma(shape = shape_τ, rate = rate_τ)\n    y ~ Normal(mean = μ, precision = τ)\nend\n\niid_normal_constraints = @constraints begin\n    q(μ, τ) = q(μ)q(τ)\nend\n\niid_normal_autoupdates = @autoupdates begin \n    mean_μ  = mean(q(μ))\n    var_μ   = var(q(μ))\n    shape_τ = shape(q(τ))\n    rate_τ  = rate(q(τ))\nend\n\niid_normal_hidden_μ       = 3.1415\niid_normal_hidden_τ       = 0.0271\niid_normal_distribution   = NormalMeanPrecision(iid_normal_hidden_μ, iid_normal_hidden_τ)\niid_normal_rng            = StableRNG(123)\niid_normal_datastream     = RecentSubject(Float64)\niid_normal_observations   = labeled(Val((:y, )), combineLatest(iid_normal_datastream))\niid_normal_initialization = @initialization begin \n    q(μ) = NormalMeanPrecision(0.0, 0.001)\n    q(τ) = GammaShapeRate(10.0, 10.0)\nend\n\niid_normal_engine  = infer(\n    model          = iid_normal(),\n    datastream     = iid_normal_observations,\n    autoupdates    = iid_normal_autoupdates,\n    constraints    = iid_normal_constraints,\n    initialization = iid_normal_initialization,\n    historyvars    = (\n        μ = KeepLast(),\n        τ = KeepLast(),\n    ),\n    keephistory    = 100,\n    iterations     = 10,\n    free_energy    = true,\n    autostart      = true\n)","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"The notable differences with the previous example is the use of the constraints and iterations arguments. Read more about constraints in the Constraints Specification section of the documentation. We have also indicated in the historyvars that we want to keep track of posteriors only from the last variational iteration in the history buffer.","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Now we can feed some observations to the datastream:","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"for i in 1:100\n    next!(iid_normal_datastream, rand(iid_normal_rng, iid_normal_distribution))\nend","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Let's inspect the differences in the free_energy buffers:","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"@test all(v -> v <= 0.0, diff(iid_normal_engine.free_energy_history)) #hide\n@test length(iid_normal_engine.free_energy_history) === 10 #hide\niid_normal_engine.free_energy_history","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"@test length(iid_normal_engine.free_energy_raw_history) === 1000 #hide\niid_normal_engine.free_energy_raw_history","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"@test length(iid_normal_engine.free_energy_final_only_history) === 100 #hide\niid_normal_engine.free_energy_final_only_history","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"We can also visualize different representations:","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"plot(iid_normal_engine.free_energy_history, label = \"Bethe Free Energy (averaged)\")","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"note: Note\nIn general, the averaged Bethe Free Energy values must decrease and converge to a stable point.","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"plot(iid_normal_engine.free_energy_raw_history, label = \"Bethe Free Energy (raw)\")","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"plot(iid_normal_engine.free_energy_final_only_history, label = \"Bethe Free Energy (last per observation)\")","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"As we can see, in the case of the variational iterations those buffers are quite different and represent different representations of the same Bethe Free Energy stream (which corresponds to the .free_energy_raw_history). As a sanity check, we could also visualize the history of our posterior estimations in the same way  as we did for a simpler previous example:","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"@test length(iid_normal_engine.history[:μ]) === 100 #hide\n@test length(iid_normal_engine.history[:τ]) === 100 #hide\n@gif for (μ_posterior, τ_posterior) in zip(iid_normal_engine.history[:μ], iid_normal_engine.history[:τ])\n    rμ = range(0, 10, length = 1000)\n    rτ = range(0, 1, length = 1000)\n\n    pμ = plot(rμ, (x) -> pdf(μ_posterior, x), fillalpha=0.3, fillrange = 0, label=\"P(μ|y)\", c=3)\n    pμ = vline!(pμ, [ iid_normal_hidden_μ ], label = \"Real value of μ\")\n\n    pτ = plot(rτ, (x) -> pdf(τ_posterior, x), fillalpha=0.3, fillrange = 0, label=\"P(τ|y)\", c=3)\n    pτ = vline!(pτ, [ iid_normal_hidden_τ ], label = \"Real value of τ\")\n\n    plot(pμ, pτ, layout = @layout([ a; b ]))\nend","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Nice, the history of the estimated posteriors aligns well with the real (hidden) values of the underlying parameters.","category":"page"},{"location":"manuals/inference/streamlined/#manual-online-inference-callbacks","page":"Streamline inference","title":"Callbacks","text":"","category":"section"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"The RxInferenceEngine has its own lifecycle. The callbacks differ a little bit from Using callbacks with Static Inference.  Here are available callbacks that can be used together with the streaming inference:","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"using RxInfer, Test, Markdown\n# Update the documentation below if this test does not pass\n@test RxInfer.available_callbacks(RxInfer.streaming_inference) === Val((:before_model_creation, :after_model_creation, :before_autostart, :after_autostart))\nnothing","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"before_model_creation()","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Calls before the model is going to be created, does not accept any arguments.","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"after_model_creation(model::ProbabilisticModel)","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Calls right after the model has been created, accepts a single argument, the model.","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"before_autostart(engine::RxInferenceEngine)","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Calls before the RxInfer.start() function, if autostart is set to true.","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"after_autostart(engine::RxInferenceEngine)","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Calls after the RxInfer.start() function, if autostart is set to true.","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Here is an example usage of the outlined callbacks:","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"before_model_creation_called = Ref(false) #hide\nafter_model_creation_called = Ref(false) #hide\nbefore_autostart_called = Ref(false) #hide\nafter_autostart_called = Ref(false) #hide\n\nfunction before_model_creation()\n    before_model_creation_called[] = true #hide\n    println(\"The model is about to be created\")\nend\n\nfunction after_model_creation(model::ProbabilisticModel)\n    after_model_creation_called[] = true #hide\n    println(\"The model has been created\")\n    println(\"  The number of factor nodes is: \", length(RxInfer.getfactornodes(model)))\n    println(\"  The number of latent states is: \", length(RxInfer.getrandomvars(model)))\n    println(\"  The number of data points is: \", length(RxInfer.getdatavars(model)))\n    println(\"  The number of constants is: \", length(RxInfer.getconstantvars(model)))\nend\n\nfunction before_autostart(engine::RxInferenceEngine)\n    before_autostart_called[] = true #hide\n    println(\"The reactive inference engine is about to start\")\nend\n\nfunction after_autostart(engine::RxInferenceEngine)\n    after_autostart_called[] = true #hide\n    println(\"The reactive inference engine has been started\")\nend\n\nengine = infer(\n    model          = beta_bernoulli_online(),\n    datastream     = observations,\n    autoupdates    = beta_bernoulli_autoupdates,\n    initialization = @initialization(q(θ) = Beta(1, 1)),\n    keephistory    = 5,\n    autostart      = true,\n    free_energy    = true,\n    callbacks      = (\n        before_model_creation = before_model_creation,\n        after_model_creation  = after_model_creation,\n        before_autostart      = before_autostart,\n        after_autostart       = after_autostart\n    )\n)\n\n@test before_model_creation_called[] #hide\n@test after_model_creation_called[] #hide\n@test before_autostart_called[] #hide\n@test after_autostart_called[] #hide\n\nRxInfer.stop(engine) #hide\nnothing #hide","category":"page"},{"location":"manuals/inference/streamlined/#manual-online-inference-event-loop","page":"Streamline inference","title":"Event loop","text":"","category":"section"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"In constrast to Static Inference, the streaming version of the infer function  does not provide callbacks such as on_marginal_update, since it is possible to subscribe directly on those updates with the  engine.posteriors field. However, the reactive inference engine provides an ability to listen to its internal event loop, that also includes \"pre\" and \"post\" events for posterior updates.","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"RxInferenceEvent","category":"page"},{"location":"manuals/inference/streamlined/#RxInfer.RxInferenceEvent","page":"Streamline inference","title":"RxInfer.RxInferenceEvent","text":"RxInferenceEvent{T, D}\n\nThe RxInferenceEngine sends events in a form of the RxInferenceEvent structure. T represents the type of an event, D represents the type of a data associated with the event. The type of data depends on the type of an event, but usually represents a tuple, which can be unrolled automatically with the Julia's splitting syntax, e.g. model, iteration = event.  See the documentation of the rxinference function for possible event types and their associated data types.\n\nThe events system itself uses the Rocket.jl library API. For example, one may create a custom event listener in the following way:\n\nusing Rocket\n\nstruct MyEventListener <: Rocket.Actor{RxInferenceEvent}\n    # ... extra fields\nend\n\nfunction Rocket.on_next!(listener::MyEventListener, event::RxInferenceEvent{ :after_iteration })\n    model, iteration = event\n    println(\"Iteration $(iteration) has been finished.\")\nend\n\nfunction Rocket.on_error!(listener::MyEventListener, err)\n    # ...\nend\n\nfunction Rocket.on_complete!(listener::MyEventListener)\n    # ...\nend\n\n\nand later on:\n\nengine = infer(events = Val((:after_iteration, )), ...)\n\nsubscription = subscribe!(engine.events, MyEventListener(...))\n\nSee also: infer, RxInferenceEngine\n\n\n\n\n\n","category":"type"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Let's build a simple example by implementing our own event listener that does not do anything complex but simply prints some debugging information.","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"using RxInfer, Test, Markdown\n# Update the documentation below if this test does not pass\n@test RxInfer.available_events(RxInfer.streaming_inference) === Val((\n    :before_start,\n    :after_start,\n    :before_stop,\n    :after_stop,\n    :on_new_data,\n    :before_iteration,\n    :before_auto_update,\n    :after_auto_update,\n    :before_data_update,\n    :after_data_update,\n    :after_iteration,\n    :before_history_save,\n    :after_history_save,\n    :on_tick,\n    :on_error,\n    :on_complete\n))\nnothing","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"struct MyEventListener <: Rocket.Actor{RxInferenceEvent}\n    # ... extra fields\nend","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"The available events are","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":":before_start","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Emits right before starting the engine with the RxInfer.start function. The data is (engine::RxInferenceEngine, )","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"function Rocket.on_next!(listener::MyEventListener, event::RxInferenceEvent{ :before_start })\n    (engine, ) = event\n    @test engine isa RxInferenceEngine #hide\n    println(\"The engine is about to start.\")\nend","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":":after_start","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Emits right after starting the engine with the RxInfer.start function. The data is (engine::RxInferenceEngine, )","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"function Rocket.on_next!(listener::MyEventListener, event::RxInferenceEvent{ :after_start })\n    (engine, ) = event\n    @test engine isa RxInferenceEngine #hide\n    println(\"The engine has been started.\")\nend","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":":before_stop","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Emits right before stopping the engine with the RxInfer.stop function. The data is (engine::RxInferenceEngine, )","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"function Rocket.on_next!(listener::MyEventListener, event::RxInferenceEvent{ :before_stop })\n    (engine, ) = event\n    @test engine isa RxInferenceEngine #hide\n    println(\"The engine is about to be stopped.\")\nend","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":":after_stop","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Emits right after stopping the engine with the RxInfer.stop function. The data is (engine::RxInferenceEngine, )","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"function Rocket.on_next!(listener::MyEventListener, event::RxInferenceEvent{ :after_stop })\n    (engine, ) = event\n    @test engine isa RxInferenceEngine #hide\n    println(\"The engine has been stopped.\")\nend","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":":on_new_data","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Emits right before processing new data point. The data is (model::ProbabilisticModel, data)","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"function Rocket.on_next!(listener::MyEventListener, event::RxInferenceEvent{ :on_new_data })\n    (model, data) = event\n    @test model isa ProbabilisticModel #hide\n    @test data isa NamedTuple #hide\n    @test haskey(data, :y) #hide\n    @test iszero(data[:y]) || isone(data[:y]) #hide\n    println(\"The new data point has been received: \", data)\nend","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":":before_iteration","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Emits right before starting new variational iteration. The data is (model::ProbabilisticModel, iteration::Int)","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"function Rocket.on_next!(listener::MyEventListener, event::RxInferenceEvent{ :before_iteration })\n    (model, iteration) = event\n    @test model isa ProbabilisticModel #hide\n    @test iteration isa Int #hide\n    println(\"Starting new variational iteration #\", iteration)\nend","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":":before_auto_update","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Emits right before executing the @autoupdates. The data is (model::ProbabilisticModel, iteration::Int, autoupdates)","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"function Rocket.on_next!(listener::MyEventListener, event::RxInferenceEvent{ :before_auto_update })\n    (model, iteration, autoupdates) = event\n    @test model isa ProbabilisticModel #hide\n    @test iteration isa Int #hide\n    @test autoupdates isa Tuple{RxInfer.RxInferenceAutoUpdate} #hide\n    println(\"Before processing autoupdates\")\nend","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":":after_auto_update","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Emits right after executing the @autoupdates. The data is (model::ProbabilisticModel, iteration::Int, autoupdates)","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"function Rocket.on_next!(listener::MyEventListener, event::RxInferenceEvent{ :after_auto_update })\n    (model, iteration, autoupdates) = event\n    @test model isa ProbabilisticModel #hide\n    @test iteration isa Int #hide\n    @test autoupdates isa Tuple{RxInfer.RxInferenceAutoUpdate} #hide\n    println(\"After processing autoupdates\")\nend","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":":before_data_update","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Emits right before feeding the model with the new data. The data is (model::ProbabilisticModel, iteration::Int, data)","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"function Rocket.on_next!(listener::MyEventListener, event::RxInferenceEvent{ :before_data_update })\n    (model, iteration, data) = event\n    @test model isa ProbabilisticModel #hide\n    @test iteration isa Int #hide\n    println(\"Before processing new data \", data)\nend","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":":after_data_update","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Emits right after feeding the model with the new data. The data is (model::ProbabilisticModel, iteration::Int, data)","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"function Rocket.on_next!(listener::MyEventListener, event::RxInferenceEvent{ :after_data_update })\n    (model, iteration, data) = event\n    @test model isa ProbabilisticModel #hide\n    @test iteration isa Int #hide\n    println(\"After processing new data \", data)\nend","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":":after_iteration","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Emits right after finishing a variational iteration. The data is (model::ProbabilisticModel, iteration::Int)","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"function Rocket.on_next!(listener::MyEventListener, event::RxInferenceEvent{ :after_iteration })\n    (model, iteration) = event\n    @test model isa ProbabilisticModel #hide\n    @test iteration isa Int #hide\n    println(\"Finishing the variational iteration #\", iteration)\nend","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":":before_history_save","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Emits right before saving the history (if requested). The data is (model::ProbabilisticModel, )","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"function Rocket.on_next!(listener::MyEventListener, event::RxInferenceEvent{ :before_history_save })\n    (model, ) = event\n    @test model isa ProbabilisticModel #hide\n    println(\"Before saving the history\")\nend","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":":after_history_save","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Emits right after saving the history (if requested). The data is (model::ProbabilisticModel, )","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"function Rocket.on_next!(listener::MyEventListener, event::RxInferenceEvent{ :after_history_save })\n    (model, ) = event\n    @test model isa ProbabilisticModel #hide\n    println(\"After saving the history\")\nend","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":":on_tick","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Emits right after finishing processing the new observations and completing the inference step. The data is (model::ProbabilisticModel, )","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"function Rocket.on_next!(listener::MyEventListener, event::RxInferenceEvent{ :on_tick })\n    (model, ) = event\n    @test model isa ProbabilisticModel #hide\n    println(\"Finishing the inference for the new observations\")\nend","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":":on_error","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Emits if an error occurs in the inference engine. The data is (model::ProbabilisticModel, err::Any)","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"function Rocket.on_next!(listener::MyEventListener, event::RxInferenceEvent{ :on_error })\n    (model, err) = event\n    @test model isa ProbabilisticModel #hide\n    println(\"An error occured during the inference procedure: \", err)\nend","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":":on_complete","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Emits when the datastream completes. The data is (model::ProbabilisticModel, )","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"function Rocket.on_next!(listener::MyEventListener, event::RxInferenceEvent{ :on_complete })\n    (model, ) = event\n    @test model isa ProbabilisticModel #hide\n    println(\"The data stream completed. The inference has been finished.\")\nend","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Let's use our event listener with the infer function:","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"engine = infer(\n    model          = beta_bernoulli_online(),\n    datastream     = observations,\n    autoupdates    = beta_bernoulli_autoupdates,\n    initialization = @initialization(q(θ) = Beta(1, 1)),\n    keephistory    = 5,\n    iterations     = 2,\n    autostart      = false,\n    free_energy    = true,\n    events         = Val((\n        :before_start,\n        :after_start,\n        :before_stop,\n        :after_stop,\n        :on_new_data,\n        :before_iteration,\n        :before_auto_update,\n        :after_auto_update,\n        :before_data_update,\n        :after_data_update,\n        :after_iteration,\n        :before_history_save,\n        :after_history_save,\n        :on_tick,\n        :on_error,\n        :on_complete\n    ))\n)","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"After we have created the engine, we can subscribe on events and RxInfer.start the engine:","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"events_subscription = subscribe!(engine.events, MyEventListener())\n\nRxInfer.start(engine)\nnothing #hide","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"The event loop stays idle without new observation and runs again when a new observation becomes available:","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"next!(datastream, rand(rng, distribution))","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Let's complete the datastream ","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"complete!(datastream)","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"In this case, it is not necessary to RxInfer.stop the engine, because  it will be stopped automatically.","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"@test_logs (:warn, r\"The engine has been completed.*\") RxInfer.stop(engine) #hide\nRxInfer.stop(engine)\nnothing #hide","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"note: Note\nThe :before_stop and :after_stop events are not emmited in case of the datastream completion. Use the :on_complete instead.","category":"page"},{"location":"manuals/inference/streamlined/#manual-online-inference-data","page":"Streamline inference","title":"Using data keyword argument with streaming inference","text":"","category":"section"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"The streaming version does support static datasets as well.  Internally, it converts it to a datastream, that emits all observations in a sequntial order without any delay. As an example:","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"staticdata = rand(rng, distribution, 1_000)","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"Use the data keyword argument instead of the datastream to pass the static data.","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"engine = infer(\n    model          = beta_bernoulli_online(),\n    data           = (y = staticdata, ),\n    autoupdates    = beta_bernoulli_autoupdates,\n    initialization = @initialization(q(θ) = Beta(1, 1)),\n    keephistory    = 1000,\n    autostart      = true,\n    free_energy    = true,\n)","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"engine.history[:θ]","category":"page"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"@gif for posterior in engine.history[:θ]\n    rθ = range(0, 1, length = 1000)\n    pθ = plot(rθ, (x) -> pdf(posterior, x), fillalpha=0.3, fillrange = 0, label=\"P(θ|y)\", c=3)\n    pθ = vline!(pθ, [ hidden_θ ], label = \"Real value of θ\")\n\n    plot(pθ)\nend","category":"page"},{"location":"manuals/inference/streamlined/#manual-online-inference-where-to-go","page":"Streamline inference","title":"Where to go next?","text":"","category":"section"},{"location":"manuals/inference/streamlined/","page":"Streamline inference","title":"Streamline inference","text":"This guide covered some fundamental usages of the infer function in the context of streamline inference,  but did not cover all the available keyword arguments of the function. Read more explanation about the other keyword arguments  in the Overview section or check out the Static Inference section. Also check out more complex examples.","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/#examples-active-inference-mountain-car","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"","category":"section"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"import Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"using RxInfer, Plots","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"A group of friends is going to a camping site that is located on the biggest mountain in the Netherlands. They use an electric car for the trip. When they are almost there, the car's battery is almost empty and is therefore limiting the engine force. Unfortunately, they are in the middle of a valley and don't have enough power to reach the camping site. Night is falling and they still need to reach the top of the mountain. As rescuers, let us develop an Active Inference (AI) agent that can get them up the hill with the limited engine power.","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/#The-environmental-process-of-the-mountain","page":"Active Inference Mountain car","title":"The environmental process of the mountain","text":"","category":"section"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"Firstly, we specify the environmental process according to Ueltzhoeffer (2017) \"Deep active inference\". This process shows how the environment evolves after interacting with the agent.","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"Particularly, let's denote z_t = (phi_t dotphi_t) as the environmental state depending on the position phi_t and velocity dotphi_t of the car; a_t as the action of the environment on the car. Then the evolution of the state is described as follows  ","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"beginaligned \ndotphi_t = dotphi_t-1 + F_g(phi_t-1) + F_f(dotphi_t-1) + F_a(a_t)\nphi_t = phi_t-1 + dotphi_t \nendaligned","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"where F_g(phi_t-1) is the gravitational force of the hill landscape that depends on the car's position","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"F_g(phi) = begincases\n        -005(2phi + 1)    mathrmif  phi  0 \n        -005 left(1 + 5phi^2)^-frac12 + phi^2 (1 + 5phi^2)^-frac32 + frac116phi^4 right   mathrmotherwise\nendcases","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"F_f(dotphi)","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"is the friction on the car defined through the car's velocity F_f(dotphi)  = -01  dotphi and F_a(a) is the engine force F_a(a) = 004 tanh(a) Since the car is on low battery, we use the tanh(cdot) function to limit the engine force to the interval [-0.04, 0.04].","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"In the cell below, the create_physics function defines forces F_g F_f F_a; and the create_world function defines the environmental process of the mountain.","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"import HypergeometricFunctions: _₂F₁\n\nfunction create_physics(; engine_force_limit = 0.04, friction_coefficient = 0.1)\n    # Engine force as function of action\n    Fa = (a::Real) -> engine_force_limit * tanh(a) \n\n    # Friction force as function of velocity\n    Ff = (y_dot::Real) -> -friction_coefficient * y_dot \n    \n    # Gravitational force (horizontal component) as function of position\n    Fg = (y::Real) -> begin\n        if y < 0\n            0.05*(-2*y - 1)\n        else\n            0.05*(-(1 + 5*y^2)^(-0.5) - (y^2)*(1 + 5*y^2)^(-3/2) - (y^4)/16)\n        end\n    end\n    \n    # The height of the landscape as a function of the horizontal coordinate\n    height = (x::Float64) -> begin\n        if x < 0\n            h = x^2 + x\n        else\n            h = x * _₂F₁(0.5,0.5,1.5, -5*x^2) + x^3 * _₂F₁(1.5, 1.5, 2.5, -5*x^2) / 3 + x^5 / 80\n        end\n        return 0.05*h\n    end\n\n    return (Fa, Ff, Fg,height)\nend;\n\nfunction create_world(; Fg, Ff, Fa, initial_position = -0.5, initial_velocity = 0.0)\n\n    y_t_min = initial_position\n    y_dot_t_min = initial_velocity\n    \n    y_t = y_t_min\n    y_dot_t = y_dot_t_min\n    \n    execute = (a_t::Float64) -> begin\n        # Compute next state\n        y_dot_t = y_dot_t_min + Fg(y_t_min) + Ff(y_dot_t_min) + Fa(a_t)\n        y_t = y_t_min + y_dot_t\n    \n        # Reset state for next step\n        y_t_min = y_t\n        y_dot_t_min = y_dot_t\n    end\n    \n    observe = () -> begin \n        return [y_t, y_dot_t]\n    end\n        \n    return (execute, observe)\nend","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"create_world (generic function with 1 method)","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"Let's visualize the mountain landscape and the situation of the car. ","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"engine_force_limit   = 0.04\nfriction_coefficient = 0.1\n\nFa, Ff, Fg, height = create_physics(\n    engine_force_limit = engine_force_limit,\n    friction_coefficient = friction_coefficient\n);\ninitial_position = -0.5\ninitial_velocity = 0.0\n\nx_target = [0.5, 0.0] \n\nvalley_x = range(-2, 2, length=400)\nvalley_y = [ height(xs) for xs in valley_x ]\nplot(valley_x, valley_y, title = \"Mountain valley\", label = \"Landscape\", color = \"black\")\nscatter!([ initial_position ], [ height(initial_position) ], label=\"initial car position\")   \nscatter!([x_target[1]], [height(x_target[1])], label=\"camping site\")","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/#Naive-approach","page":"Active Inference Mountain car","title":"Naive approach","text":"","category":"section"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"Well, let's see how our friends were struggling with the low-battery car when they tried to get it to the camping site before we come to help. They basically used the brute-force method, i.e. just pushing the gas pedal for full power.","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"N_naive  = 100 # Total simulation time\npi_naive = 100.0 * ones(N_naive) # Naive policy for right full-power only\n\n# Let there be a world\n(execute_naive, observe_naive) = create_world(; \n    Fg = Fg, Ff = Ff, Fa = Fa, \n    initial_position = initial_position, \n    initial_velocity = initial_velocity\n);\n\ny_naive = Vector{Vector{Float64}}(undef, N_naive)\nfor t = 1:N_naive\n    execute_naive(pi_naive[t]) # Execute environmental process\n    y_naive[t] = observe_naive() # Observe external states\nend\n\nanimation_naive = @animate for i in 1:N_naive\n    plot(valley_x, valley_y, title = \"Naive policy\", label = \"Landscape\", color = \"black\", size = (800, 400))\n    scatter!([y_naive[i][1]], [height(y_naive[i][1])], label=\"car\")\n    scatter!([x_target[1]], [height(x_target[1])], label=\"goal\")   \nend\n\n# The animation is saved and displayed as markdown picture for the automatic HTML generation\ngif(animation_naive, \"../pics/ai-mountain-car-naive.gif\", fps = 24, show_msg = false);","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"They failed as expected since the car doesn't have enough power. This helps to understand that the brute-force approach is not the most efficient one in this case and hopefully a bit of swinging is necessary to achieve the goal.","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/#Active-inference-approach","page":"Active Inference Mountain car","title":"Active inference approach","text":"","category":"section"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"Now let's help them solve the problem with an active inference approach. Particularly, we create an agent that predicts the future car position as well as the best possible actions in a probabilistic manner.","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"We start by specifying a probabilistic model for the agent that describes the agent's internal beliefs over the external dynamics of the environment. The generative model is defined as follows","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"beginaligned\np_t(xsu) propto p(s_t-1) prod_k=t^t+T p(x_k mid s_k)  p(s_k mid s_k-1u_k)  p(u_k)  p(x_k) nonumber\nendaligned","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"where the factors are defined as","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"p(x_k) = mathcalN(x_k mid x_goalV_goal)  quad (mathrmtarget)","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"p(s_k mid s_k-1u_k) = mathcalN(s_k mid tildeg(s_k-1)+h(u_k)gamma^-1)  quad (mathrmstate  transition)","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"p(x_k mid s_k) = mathcalN(x_k mid s_ktheta) quad (mathrmobservation)","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"p(u_k) = mathcalN(u_k mid m_uV_u) quad (mathrmcontrol)","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"p(s_t-1) = mathcalN(s_t-1 mid m_t-1V_t-1) quad (mathrmprevious  state)","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"where ","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"x\ndenotes observations of the agent after interacting with the environment; \ns_t = (s_tdots_t)\nis the state of the car embodying its position and velocity; \nu_t\ndenotes the control state of the agent; \nh(cdot)\nis the tanh(cdot) function modeling engine control; \ntildeg(cdot)\nexecutes a linear approximation of equations (1) and (2): ","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"beginaligned \ndots_t = dots_t-1 + F_g(s_t-1) + F_f(dots_t-1)\ns_t = s_t-1 + dots_t\nendaligned","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"In the cell below, the @model macro and the meta blocks are used to define the probabilistic model and the approximation methods for the nonlinear state-transition functions, respectively. In addition, the beliefs over the future states (up to T steps ahead) of the agent is included.","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"@model function mountain_car(m_u, V_u, m_x, V_x, m_s_t_min, V_s_t_min, T, Fg, Fa, Ff, engine_force_limit)\n    \n    # Transition function modeling transition due to gravity and friction\n    g = (s_t_min::AbstractVector) -> begin \n        s_t = similar(s_t_min) # Next state\n        s_t[2] = s_t_min[2] + Fg(s_t_min[1]) + Ff(s_t_min[2]) # Update velocity\n        s_t[1] = s_t_min[1] + s_t[2] # Update position\n        return s_t\n    end\n    \n    # Function for modeling engine control\n    h = (u::AbstractVector) -> [0.0, Fa(u[1])] \n    \n    # Inverse engine force, from change in state to corresponding engine force\n    h_inv = (delta_s_dot::AbstractVector) -> [atanh(clamp(delta_s_dot[2], -engine_force_limit+1e-3, engine_force_limit-1e-3)/engine_force_limit)] \n    \n    # Internal model perameters\n    Gamma = 1e4*diageye(2) # Transition precision\n    Theta = 1e-4*diageye(2) # Observation variance\n\n    s_t_min ~ MvNormal(mean = m_s_t_min, cov = V_s_t_min)\n    s_k_min = s_t_min\n\n    local s\n    \n    for k in 1:T\n        u[k] ~ MvNormal(mean = m_u[k], cov = V_u[k])\n        u_h_k[k] ~ h(u[k]) where { meta = DeltaMeta(method = Linearization(), inverse = h_inv) }\n        s_g_k[k] ~ g(s_k_min) where { meta = DeltaMeta(method = Linearization()) }\n        u_s_sum[k] ~ s_g_k[k] + u_h_k[k]\n        s[k] ~ MvNormal(mean = u_s_sum[k], precision = Gamma)\n        x[k] ~ MvNormal(mean = s[k], cov = Theta)\n        x[k] ~ MvNormal(mean = m_x[k], cov = V_x[k]) # goal\n        s_k_min = s[k]\n    end\n    \n    return (s, )\nend","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"After specifying the generative model, let's create an Active Inference(AI) agent for the car.  Technically, the agent goes through three phases: Act-Execute-Observe, Infer and Slide.","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"Act-Execute-Observe:   In this phase, the agent performs an action onto the environment at time t and gets T observations in exchange. These observations are basically the prediction of the agent on how the environment evolves over the next T time step. \nInfer:  After receiving observations, the agent starts updating its internal probabilistic model by doing inference. Particularly, it finds the posterior distributions over the state s_t and control u_t, i.e. p(s_tmid x_t) and p(u_tmid x_t).\nSlide:  After updating its internal belief, the agent moves to the next time step and uses the inferred action u_t in the previous time step to interact with the environment.  ","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"In the cell below, we create the agent through the create_agent function, which includes compute, act, slide and future functions:","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"The act function selects the next action based on the inferred policy. On the other hand, the future function predicts the next T positions based on the current action. These two function implement the Act-Execute-Observe phase.\nThe compute function infers the policy (which is a set of actions for the next T time steps) and the agent's state using the agent internal model. This function implements the Infer phase. We call it compute to avoid the clash with the infer function of RxInfer.jl.\nThe slide function implements the Slide phase, which moves the agent internal model to the next time step.","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"# We are going to use some private functionality from ReactiveMP, \n# in the future we should expose a proper API for this\nimport RxInfer.ReactiveMP: getrecent, messageout\n\nfunction create_agent(;T = 20, Fg, Fa, Ff, engine_force_limit, x_target, initial_position, initial_velocity)\n    Epsilon = fill(huge, 1, 1)                # Control prior variance\n    m_u = Vector{Float64}[ [ 0.0] for k=1:T ] # Set control priors\n    V_u = Matrix{Float64}[ Epsilon for k=1:T ]\n\n    Sigma    = 1e-4*diageye(2) # Goal prior variance\n    m_x      = [zeros(2) for k=1:T]\n    V_x      = [huge*diageye(2) for k=1:T]\n    V_x[end] = Sigma # Set prior to reach goal at t=T\n\n    # Set initial brain state prior\n    m_s_t_min = [initial_position, initial_velocity] \n    V_s_t_min = tiny * diageye(2)\n    \n    # Set current inference results\n    result = nothing\n\n    # The `infer` function is the heart of the agent\n    # It calls the `RxInfer.inference` function to perform Bayesian inference by message passing\n    compute = (upsilon_t::Float64, y_hat_t::Vector{Float64}) -> begin\n        m_u[1] = [ upsilon_t ] # Register action with the generative model\n        V_u[1] = fill(tiny, 1, 1) # Clamp control prior to performed action\n\n        m_x[1] = y_hat_t # Register observation with the generative model\n        V_x[1] = tiny*diageye(2) # Clamp goal prior to observation\n\n        data = Dict(:m_u       => m_u, \n                    :V_u       => V_u, \n                    :m_x       => m_x, \n                    :V_x       => V_x,\n                    :m_s_t_min => m_s_t_min,\n                    :V_s_t_min => V_s_t_min)\n        \n        model  = mountain_car(T = T, Fg = Fg, Fa = Fa, Ff = Ff, engine_force_limit = engine_force_limit) \n        result = infer(model = model, data = data)\n    end\n    \n    # The `act` function returns the inferred best possible action\n    act = () -> begin\n        if result !== nothing\n            return mode(result.posteriors[:u][2])[1]\n        else\n            return 0.0 # Without inference result we return some 'random' action\n        end\n    end\n    \n    # The `future` function returns the inferred future states\n    future = () -> begin \n        if result !== nothing \n            return getindex.(mode.(result.posteriors[:s]), 1)\n        else\n            return zeros(T)\n        end\n    end\n\n    # The `slide` function modifies the `(m_s_t_min, V_s_t_min)` for the next step\n    # and shifts (or slides) the array of future goals `(m_x, V_x)` and inferred actions `(m_u, V_u)`\n    slide = () -> begin\n\n        model  = RxInfer.getmodel(result.model)\n        (s, )  = RxInfer.getreturnval(model)\n        varref = RxInfer.getvarref(model, s) \n        var    = RxInfer.getvariable(varref)\n        \n        slide_msg_idx = 3 # This index is model dependend\n        (m_s_t_min, V_s_t_min) = mean_cov(getrecent(messageout(var[2], slide_msg_idx)))\n\n        m_u = circshift(m_u, -1)\n        m_u[end] = [0.0]\n        V_u = circshift(V_u, -1)\n        V_u[end] = Epsilon\n\n        m_x = circshift(m_x, -1)\n        m_x[end] = x_target\n        V_x = circshift(V_x, -1)\n        V_x[end] = Sigma\n    end\n\n    return (compute, act, slide, future)    \nend","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"create_agent (generic function with 1 method)","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"Now it's time to see if we can help our friends arrive at the camping site by midnight?","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"(execute_ai, observe_ai) = create_world(\n    Fg = Fg, Ff = Ff, Fa = Fa, \n    initial_position = initial_position, \n    initial_velocity = initial_velocity\n) # Let there be a world\n\nT_ai = 50\n\n(compute_ai, act_ai, slide_ai, future_ai) = create_agent(; # Let there be an agent\n    T  = T_ai, \n    Fa = Fa,\n    Fg = Fg, \n    Ff = Ff, \n    engine_force_limit = engine_force_limit,\n    x_target = x_target,\n    initial_position = initial_position,\n    initial_velocity = initial_velocity\n) \n\nN_ai = 100\n\n# Step through experimental protocol\nagent_a = Vector{Float64}(undef, N_ai) # Actions\nagent_f = Vector{Vector{Float64}}(undef, N_ai) # Predicted future\nagent_x = Vector{Vector{Float64}}(undef, N_ai) # Observations\n\nfor t=1:N_ai\n    agent_a[t] = act_ai()               # Invoke an action from the agent\n    agent_f[t] = future_ai()            # Fetch the predicted future states\n    execute_ai(agent_a[t])              # The action influences hidden external states\n    agent_x[t] = observe_ai()           # Observe the current environmental outcome (update p)\n    compute_ai(agent_a[t], agent_x[t]) # Infer beliefs from current model state (update q)\n    slide_ai()                          # Prepare for next iteration\nend\n\nanimation_ai = @animate for i in 1:N_ai\n    # pls - plot landscape\n    pls = plot(valley_x, valley_y, title = \"Active inference results\", label = \"Landscape\", color = \"black\")\n    pls = scatter!(pls, [agent_x[i][1]], [height(agent_x[i][1])], label=\"car\")\n    pls = scatter!(pls, [x_target[1]], [height(x_target[1])], label=\"goal\")   \n    pls = scatter!(pls, agent_f[i], height.(agent_f[i]), label = \"Predicted future\", alpha = map(i -> 0.5 / i, 1:T_ai))\n    \n    # pef - plot engine force\n    pef = plot(Fa.(agent_a[1:i]), title = \"Engine force (agents actions)\", xlim = (0, N_ai), ylim = (-0.05, 0.05))\n    \n    plot(pls, pef, size = (800, 400))\nend\n    \n# The animation is saved and displayed as markdown picture for the automatic HTML generation\ngif(animation_ai, \"../pics/ai-mountain-car-ai.gif\", fps = 24, show_msg = false);","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"Voila! The car now is able to reach the camping site with a smart strategy.","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"The left figure shows the agent reached its goal by swinging and the right one shows the corresponding engine force. As we can see, at the beginning the agent tried to reach the goal directly (with full engine force) but after some trials it realized that's not possible. Since the agent looks ahead for 50 time steps, it has enough time to explore other policies, helping it learn to move back to get more momentum to reach the goal.","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"Now our friends can enjoy their trip at the camping site!. ","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/#Reference","page":"Active Inference Mountain car","title":"Reference","text":"","category":"section"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"We refer reader to the Thijs van de Laar (2019) \"Simulating active inference processes by message passing\" original paper with more in-depth overview and explanation of the active inference agent implementation by message passing. The original environment/task description is from Ueltzhoeffer (2017) \"Deep active inference\".","category":"page"},{"location":"library/model-construction/#lib-model-construction","page":"Model construction","title":"Model construction in RxInfer","text":"","category":"section"},{"location":"library/model-construction/","page":"Model construction","title":"Model construction","text":"Model creation in RxInfer largely depends on GraphPPL package. RxInfer re-exports the @model macro from GraphPPL and defines extra plugins and data structures on top of the default functionality.","category":"page"},{"location":"library/model-construction/","page":"Model construction","title":"Model construction","text":"note: Note\nThe model creation and construction were largely refactored in GraphPPL v4.  Read Migration Guide for more details.","category":"page"},{"location":"library/model-construction/","page":"Model construction","title":"Model construction","text":"Also read the Model Specification guide.","category":"page"},{"location":"library/model-construction/#lib-model-construction-model-macro","page":"Model construction","title":"@model macro","text":"","category":"section"},{"location":"library/model-construction/","page":"Model construction","title":"Model construction","text":"RxInfer operates with so-called graphical probabilistic models, more specifically factor graphs. Working with graphs directly is, however, tedious and error-prone, especially for large models. To simplify the process, RxInfer exports the @model macro, which translates a textual description of a probabilistic model into a corresponding factor graph representation.","category":"page"},{"location":"library/model-construction/","page":"Model construction","title":"Model construction","text":"RxInfer.@model","category":"page"},{"location":"library/model-construction/#RxInfer.@model","page":"Model construction","title":"RxInfer.@model","text":"@model function model_name(model_arguments...)\n    # model description\nend\n\n@model macro generates a function that returns an equivalent graph-representation of the given probabilistic model description. See the documentation to GraphPPL.@model for more information.\n\nSupported aliases in the model specification specifically for RxInfer.jl and ReactiveMP.jl\n\na || b: alias for ReactiveMP.OR(a, b) node (operator precedence between ||, &&, -> and ! is the same as in Julia).\na && b: alias for ReactiveMP.AND(a, b) node (operator precedence ||, &&, -> and ! is the same as in Julia).\na -> b: alias for ReactiveMP.IMPLY(a, b) node (operator precedence ||, &&, -> and ! is the same as in Julia).\n¬a and !a: alias for ReactiveMP.NOT(a) node (Unicode \\neg, operator precedence ||, &&, -> and ! is the same as in Julia).\n\n\n\n\n\n","category":"macro"},{"location":"library/model-construction/","page":"Model construction","title":"Model construction","text":"Note, that GraphPPL also implements @model macro, but does not export it by default. This was a deliberate choice to allow inference backends (such as RxInfer) to implement custom functionality on top of the default GraphPPL.@model macro. This is done with a custom  backend for GraphPPL.@model macro. Read more about backends in the corresponding section of GraphPPL documentation.","category":"page"},{"location":"library/model-construction/","page":"Model construction","title":"Model construction","text":"RxInfer.ReactiveMPGraphPPLBackend","category":"page"},{"location":"library/model-construction/#RxInfer.ReactiveMPGraphPPLBackend","page":"Model construction","title":"RxInfer.ReactiveMPGraphPPLBackend","text":"A backend for GraphPPL that uses ReactiveMP for inference.\n\n\n\n\n\n","category":"type"},{"location":"library/model-construction/#lib-model-construction-conditioning","page":"Model construction","title":"Conditioning on data","text":"","category":"section"},{"location":"library/model-construction/","page":"Model construction","title":"Model construction","text":"After model creation RxInfer uses RxInfer.condition_on function to condition on data.  As an alias it is also possible to use the | operator for the same purpose, but with a nicer syntax.","category":"page"},{"location":"library/model-construction/","page":"Model construction","title":"Model construction","text":"RxInfer.condition_on\nBase.:(|)(generator::RxInfer.ModelGenerator, data)\nRxInfer.ConditionedModelGenerator","category":"page"},{"location":"library/model-construction/#RxInfer.condition_on","page":"Model construction","title":"RxInfer.condition_on","text":"condition_on(generator::ModelGenerator; kwargs...)\n\nA function that creates a ConditionedModelGenerator object from GraphPPL.ModelGenerator. The | operator can be used as a shorthand for this function.\n\njulia> using RxInfer\n\njulia> @model function beta_bernoulli(y, a, b)\n           θ ~ Beta(a, b)\n           y .~ Bernoulli(θ)\n       end\n\njulia> conditioned_model = beta_bernoulli(a = 1.0, b = 2.0) | (y = [ 1.0, 0.0, 1.0 ], )\nbeta_bernoulli(a = 1.0, b = 2.0) conditioned on: \n  y = [1.0, 0.0, 1.0]\n\njulia> RxInfer.create_model(conditioned_model) isa RxInfer.ProbabilisticModel\ntrue\n\n\n\n\n\n","category":"function"},{"location":"library/model-construction/#Base.:|-Tuple{GraphPPL.ModelGenerator, Any}","page":"Model construction","title":"Base.:|","text":"An alias for RxInfer.condition_on.\n\n\n\n\n\n","category":"method"},{"location":"library/model-construction/#RxInfer.ConditionedModelGenerator","page":"Model construction","title":"RxInfer.ConditionedModelGenerator","text":"ConditionedModelGenerator(generator, conditioned_on)\n\nAccepts a model generator and data to condition on.  The generator must be GraphPPL.ModelGenerator object. The conditioned_on must be named tuple or a dictionary with keys corresponding to the names of the input arguments in the model.\n\n\n\n\n\n","category":"type"},{"location":"library/model-construction/","page":"Model construction","title":"Model construction","text":"Sometimes it might be useful to condition on data, which is not available at model creation time.  This might be especially useful in reactive inference setting, where data, e.g. might be available later on from some asynchronous sensor input. For this reason, RxInfer implements a special deferred data handler, that does mark model argument as data, but does not specify any particular value for this data nor its shape.","category":"page"},{"location":"library/model-construction/","page":"Model construction","title":"Model construction","text":"RxInfer.DefferedDataHandler","category":"page"},{"location":"library/model-construction/#RxInfer.DefferedDataHandler","page":"Model construction","title":"RxInfer.DefferedDataHandler","text":"An object that is used to condition on unknown data. That may be necessary to create a model from a ModelGenerator object for which data is not known at the time of the model creation. \n\n\n\n\n\n","category":"type"},{"location":"library/model-construction/","page":"Model construction","title":"Model construction","text":"After the model has been conditioned it can be materialized with the RxInfer.create_model function. This function takes the RxInfer.ConditionedModelGenerator object and materializes it into a RxInfer.ProbabilisticModel.","category":"page"},{"location":"library/model-construction/","page":"Model construction","title":"Model construction","text":"RxInfer.create_model(generator::RxInfer.ConditionedModelGenerator)\nRxInfer.ProbabilisticModel\nRxInfer.getmodel(model::RxInfer.ProbabilisticModel)\nRxInfer.getreturnval(model::RxInfer.ProbabilisticModel)\nRxInfer.getvardict(model::RxInfer.ProbabilisticModel)\nRxInfer.getrandomvars(model::RxInfer.ProbabilisticModel)\nRxInfer.getdatavars(model::RxInfer.ProbabilisticModel)\nRxInfer.getconstantvars(model::RxInfer.ProbabilisticModel)\nRxInfer.getfactornodes(model::RxInfer.ProbabilisticModel)","category":"page"},{"location":"library/model-construction/#GraphPPL.create_model-Tuple{RxInfer.ConditionedModelGenerator}","page":"Model construction","title":"GraphPPL.create_model","text":"create_model(generator::ConditionedModelGenerator)\n\nMaterializes the model specification conditioned on some data into a corresponding factor graph representation. Returns ProbabilisticModel.\n\n\n\n\n\n","category":"method"},{"location":"library/model-construction/#RxInfer.ProbabilisticModel","page":"Model construction","title":"RxInfer.ProbabilisticModel","text":"A structure that holds the factor graph representation of a probabilistic model.\n\n\n\n\n\n","category":"type"},{"location":"library/model-construction/#GraphPPL.getmodel-Tuple{ProbabilisticModel}","page":"Model construction","title":"GraphPPL.getmodel","text":"Returns the underlying factor graph model.\n\n\n\n\n\n","category":"method"},{"location":"library/model-construction/#RxInfer.getreturnval-Tuple{ProbabilisticModel}","page":"Model construction","title":"RxInfer.getreturnval","text":"Returns the value from the return ... operator inside the model specification.\n\n\n\n\n\n","category":"method"},{"location":"library/model-construction/#RxInfer.getvardict-Tuple{ProbabilisticModel}","page":"Model construction","title":"RxInfer.getvardict","text":"Returns the (nested) dictionary of random variables from the model specification.\n\n\n\n\n\n","category":"method"},{"location":"library/model-construction/#RxInfer.getrandomvars-Tuple{ProbabilisticModel}","page":"Model construction","title":"RxInfer.getrandomvars","text":"Returns the random variables from the model specification.\n\n\n\n\n\n","category":"method"},{"location":"library/model-construction/#RxInfer.getdatavars-Tuple{ProbabilisticModel}","page":"Model construction","title":"RxInfer.getdatavars","text":"Returns the data variables from the model specification.\n\n\n\n\n\n","category":"method"},{"location":"library/model-construction/#RxInfer.getconstantvars-Tuple{ProbabilisticModel}","page":"Model construction","title":"RxInfer.getconstantvars","text":"Returns the constant variables from the model specification.\n\n\n\n\n\n","category":"method"},{"location":"library/model-construction/#RxInfer.getfactornodes-Tuple{ProbabilisticModel}","page":"Model construction","title":"RxInfer.getfactornodes","text":"Returns the factor nodes from the model specification.\n\n\n\n\n\n","category":"method"},{"location":"library/model-construction/#lib-model-construction-pipelines","page":"Model construction","title":"Additional GraphPPL pipeline stages","text":"","category":"section"},{"location":"library/model-construction/","page":"Model construction","title":"Model construction","text":"RxInfer implements several additional pipeline stages for default parsing stages in GraphPPL. A notable distinction of the RxInfer model specification language is the fact that RxInfer \"folds\"  some mathematical expressions and adds extra brackets to ensure the correct number of arguments for factor nodes. For example an expression x ~ x1 + x2 + x3 + x4 becomes x ~ ((x1 + x2) + x3) + x4 to ensure that the + function has exactly two arguments.","category":"page"},{"location":"library/model-construction/","page":"Model construction","title":"Model construction","text":"RxInfer.error_datavar_constvar_randomvar\nRxInfer.compose_simple_operators_with_brackets\nRxInfer.inject_tilderhs_aliases\nRxInfer.ReactiveMPNodeAliases","category":"page"},{"location":"library/model-construction/#RxInfer.error_datavar_constvar_randomvar","page":"Model construction","title":"RxInfer.error_datavar_constvar_randomvar","text":"warn_datavar_constvar_randomvar(expr::Expr)\n\nAn additional pipeline stage for the @model macro from GraphPPL.  Notify the user that the datavar, constvar and randomvar syntax has been removed and is not be supported in the current version.\n\n\n\n\n\n","category":"function"},{"location":"library/model-construction/#RxInfer.compose_simple_operators_with_brackets","page":"Model construction","title":"RxInfer.compose_simple_operators_with_brackets","text":"compose_simple_operators_with_brackets(expr::Expr)\n\nAn additional pipeline stage for the @model macro from GraphPPL.  This pipeline converts simple multi-argument operators to their corresponding bracketed expression.  E.g. the expression x ~ x1 + x2 + x3 + x4 becomes x ~ ((x1 + x2) + x3) + x4). The operators to compose are + and *.\n\n\n\n\n\n","category":"function"},{"location":"library/model-construction/#RxInfer.inject_tilderhs_aliases","page":"Model construction","title":"RxInfer.inject_tilderhs_aliases","text":"inject_tilderhs_aliases(e::Expr)\n\nA pipeline stage for the @model macro from GraphPPL. This pipeline applies the aliases defined in ReactiveMPNodeAliases to the expression.\n\n\n\n\n\n","category":"function"},{"location":"library/model-construction/#RxInfer.ReactiveMPNodeAliases","page":"Model construction","title":"RxInfer.ReactiveMPNodeAliases","text":"Syntaxic sugar for ReactiveMP nodes. Replaces a || b with ReactiveMP.OR(a, b), a && b with ReactiveMP.AND(a, b), a -> b with ReactiveMP.IMPLY(a, b) and ¬a with ReactiveMP.NOT(a).\n\n\n\n\n\n","category":"constant"},{"location":"library/model-construction/#lib-model-constriction-internal-variable","page":"Model construction","title":"Getting access to an internal variable data structures","text":"","category":"section"},{"location":"library/model-construction/","page":"Model construction","title":"Model construction","text":"To get an access to an internal ReactiveMP data structure of a variable in RxInfer model, it is possible to return  a so called label of the variable from the model macro, and access it later on as the following:","category":"page"},{"location":"library/model-construction/","page":"Model construction","title":"Model construction","text":"using RxInfer\nusing Test #hide\n\n@model function beta_bernoulli(y)\n    θ ~ Beta(1, 1)\n    y ~ Bernoulli(θ)\n    return θ\nend\n\nresult = infer(\n    model = beta_bernoulli(),\n    data  = (y = 0.0, )\n)","category":"page"},{"location":"library/model-construction/","page":"Model construction","title":"Model construction","text":"graph     = RxInfer.getmodel(result.model)\nreturnval = RxInfer.getreturnval(graph)\nθ         = returnval\nvariable  = RxInfer.getvariable(RxInfer.getvarref(graph, θ))\n@test variable isa ReactiveMP.RandomVariable #hide\nReactiveMP.israndom(variable)","category":"page"},{"location":"manuals/migration-guide-v2-v3/#Migration-Guide-from-version-2.x-to-3.x","page":"Migration from v2 to v3","title":"Migration Guide from version 2.x to 3.x","text":"","category":"section"},{"location":"manuals/migration-guide-v2-v3/","page":"Migration from v2 to v3","title":"Migration from v2 to v3","text":"This guide is intended to help you migrate your project from version 2.x to 3.x of RxInfer. The main difference between these two versions is the redefinition of the model specification language. A detailed explanation of the new model definition language can be found in the GraphPPL documentation. Here, we will give an overview of the most important changes and introduce RxInfer specific changes.","category":"page"},{"location":"manuals/migration-guide-v2-v3/#Model-Definition","page":"Migration from v2 to v3","title":"Model Definition","text":"","category":"section"},{"location":"manuals/migration-guide-v2-v3/","page":"Migration from v2 to v3","title":"Migration from v2 to v3","text":"The model definition in the @model macro has changed significantly. This change also has implications for the infer function. Since all interfaces to a model are now passed as arguments to the @model macro, the infer function needs additional information on model construction. Therefore we only support keyword arguments on model construction. An example of the new model definition is shown below:","category":"page"},{"location":"manuals/migration-guide-v2-v3/","page":"Migration from v2 to v3","title":"Migration from v2 to v3","text":"using RxInfer\n\n@model function coin_toss(prior, y)\n    θ ~ prior\n    y .~ Bernoulli(θ)\nend\n\n# Here, we pass a prior as a parameter to the model, and the data `y` is passed as data. Since we have to distinguish between what should be used as which argument, we have to pass the data as a keyword argument.\ninfer(model = coin_toss(prior=Beta(1, 1)), \n        data=(y=[1, 0, 1],) \n)","category":"page"},{"location":"manuals/migration-guide-v2-v3/#Initialization","page":"Migration from v2 to v3","title":"Initialization","text":"","category":"section"},{"location":"manuals/migration-guide-v2-v3/","page":"Migration from v2 to v3","title":"Migration from v2 to v3","text":"Initialization of messages and marginals to kickstart the inference procedure was previously done with the initmessages and initmarginals keyword. With the introduction of a nested model specificiation in the @model macro, we now need a more specific way to initialize messages and marginals. This is done with the new @initialization macro. The syntax for the @initialization macro is similar to the @constraints and @meta macro. An example is shown below:","category":"page"},{"location":"manuals/migration-guide-v2-v3/","page":"Migration from v2 to v3","title":"Migration from v2 to v3","text":"@model function submodel() end #hide\n\n@initialization begin\n    # Initialize the marginal for the variable x\n    q(x) = vague(NormalMeanVariance)\n\n    # Initialize the message for the variable z\n    μ(z) = vague(NormalMeanVariance)\n\n    # Specify the initialization for a submodel of type `submodel`\n    for init in submodel\n        q(some_var) = vague(NormalMeanVariance)\n    end\n\n    # Specify the initialization for a submodel of type `submodel` with a specific index\n    for init in (submodel, 1)\n        q(some_var) = vague(NormalMeanVariance)\n    end\nend","category":"page"},{"location":"manuals/migration-guide-v2-v3/","page":"Migration from v2 to v3","title":"Migration from v2 to v3","text":"Similar to the @constraints macro, the @initialization macro also supports function definitions:","category":"page"},{"location":"manuals/migration-guide-v2-v3/","page":"Migration from v2 to v3","title":"Migration from v2 to v3","text":"@initialization function my_init()\n    # Initialize the marginal for the variable x\n    q(x) = vague(NormalMeanVariance)\n\n    # Initialize the message for the variable z\n    μ(z) = vague(NormalMeanVariance)\n\n    # Specify the initialization for a submodel of type `submodel`\n    for init in submodel\n        q(some_var) = vague(NormalMeanVariance)\n    end\n\n    # Specify the initialization for a submodel of type `submodel` with a specific index\n    for init in (submodel, 1)\n        q(some_var) = vague(NormalMeanVariance)\n    end\nend","category":"page"},{"location":"manuals/migration-guide-v2-v3/","page":"Migration from v2 to v3","title":"Migration from v2 to v3","text":"The result of the initialization macro can be passed to the inference function with keyword argument initialization.","category":"page"},{"location":"manuals/migration-guide-v2-v3/#Deprecated-syntax","page":"Migration from v2 to v3","title":"Deprecated syntax","text":"","category":"section"},{"location":"manuals/migration-guide-v2-v3/","page":"Migration from v2 to v3","title":"Migration from v2 to v3","text":"The following syntax is deprecated and will be removed in future versions of RxInfer:","category":"page"},{"location":"manuals/migration-guide-v2-v3/","page":"Migration from v2 to v3","title":"Migration from v2 to v3","text":"initmessages and initmarginals keyword arguments\nrandomvar and datavar syntax in the @model macro","category":"page"},{"location":"library/bethe-free-energy/#lib-bethe-free-energy","page":"Bethe Free Energy","title":"Bethe Free Energy implementation in RxInfer","text":"","category":"section"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"The following text introduces the Bethe Free Energy. We start be defining a factorized model and move from the Variational Free Energy to a definition of the Bethe Free Energy.","category":"page"},{"location":"library/bethe-free-energy/#lib-bethe-factorized-model","page":"Bethe Free Energy","title":"Factorized model","text":"","category":"section"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"Before we can define a model, we must identify all variables that are relevant to the problem at hand. We distinguish between variables that can be directly observed, y = (y_1 dots y_j dots y_m) and variables that can not be observed directly, also known as latent variables, x = (x_1 dots x_i dots x_n) We then define a model that factorizes over consituent smaller factors (functions), as f(yx) = prod_a f_a(y_ax_a)","category":"page"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"Individual factors may represent stochastic functions, such as conditional or prior distributions, but also potential functions or deterministic relationships. A factor may depend on multiple observed and/or latent variables (or none).","category":"page"},{"location":"library/bethe-free-energy/#lib-bethe-vfe","page":"Bethe Free Energy","title":"Variational Free Energy","text":"","category":"section"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"The Variational Free Energy (VFE) then defines a functional objective that includes the model and a variational distribution over the latent variables, Fq(haty) = mathbbE_q(x)leftlog fracq(x)f(y=haty x) right A functional defines a function of a function that returns a scalar. Here, the VFE is a function of the variational distribution (as indicated by square brackets) and returns a number.","category":"page"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"The VFE is also a function of the observed data, as indicated by round brackets, where the data are substituted in the factorized model.","category":"page"},{"location":"library/bethe-free-energy/#lib-bethe-variational-inference","page":"Bethe Free Energy","title":"Variational inference","text":"","category":"section"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"The goal of variational inference is to find a variational distibution that minimizes the VFE, q^*(x) = argmin_qinmathcalQ Fq(haty) This objective can be optimized (under specific constraints) with the use of variational calculus. Constraints are implied by the domain over which the variational distribution is optimized, and can be enforced by Lagrange multipliers.","category":"page"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"For the VFE, constraints enforce e.g. the normalization of the variational distribution. The variational distribution that minimizes the VFE then approximates the true (but often unobtainable) posterior distribution.","category":"page"},{"location":"library/bethe-free-energy/#lib-bethe-approximation","page":"Bethe Free Energy","title":"Bethe approximation","text":"","category":"section"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"Optimization of the VFE is still a daunting task, because the variational distribution is a joint distribution over possibly many latent variables. Instead of optimizing the joint variational distribution directly, a factorized variational distribution is often chosen. The factorized variational distribution is then optimized for its constituent factors.","category":"page"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"A popular choice of factorization is the Bethe approximation, which is constructed from the factorization of the model itself, q(x) triangleq fracprod_a q_a(x_a)prod_i q_i(x_i)^d_i - 1 The numerator iterates over the factors in the model, and carves the joint variational distribution in smaller variational distributions that are more manageable to optimize.","category":"page"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"The denominator of the Bethe approximation iterates over all individual latent variables and discounts them. The discounting factor is chosen as the degree of the variable minus one, where the degree counts the number of factors in which the variable appears.","category":"page"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"The Bethe approximation thus constrains the variational distribution to a factorized form. However, the true posterior distribution might not factorize in this way, e.g. if the grapical representation of the model contains cycles. In these cases the Bethe approximation trades the exact solution for computational tractability.","category":"page"},{"location":"library/bethe-free-energy/#lib-bethe-bfe","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"","category":"section"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"The Bethe Free Energy (BFE) substitutes the Bethe approximation in the VFE, which then fragments over factors and variables, as F_Bq(haty) = sum_a U_aq_a(haty_a) - sum_a Hq_a + sum_i (d_i - 1) Hq_i The first term of the BFE specifies an average energy,  U_aq_a(haty_a) = -mathbbE_q_a(x_a)leftlog f_a(y_a=haty_a x_a)right which internalizes the factors of the  model. The last two terms specify entropies.","category":"page"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"Crucially, the BFE can be iteratively optimized for each individual variational distribution in turn. Optimization of the BFE is thus more manageable than direct optimization of the VFE.","category":"page"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"For iterative optimization of the BFE, the variational distributions must first be initialized. The infer function uses the initialization keyword argument to initialize the variational distributions of the BFE.","category":"page"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"For disambiguation, note that the initialization of the variational distribution is a different design consideration than the choice of priors. A prior specifies a factor in the model definition, while initialization concerns factors in the variational distribution.","category":"page"},{"location":"library/bethe-free-energy/#lib-bethe-further-reading","page":"Bethe Free Energy","title":"Further reading","text":"","category":"section"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"Pearl (1986) on the original foundations of Bayesian networks and belief propagation;\nYedidia et al. (2005) on the connections between belief propagation and regional approximations to the VFE;\nDauwels (2007) on variational message passing on Forney-style factor graphs (FFGs);\nSenoz et al. (2021) on constraint manipulation and message passing on FFGs.","category":"page"},{"location":"library/bethe-free-energy/#Implementation-details","page":"Bethe Free Energy","title":"Implementation details","text":"","category":"section"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"RxInfer implements Bethe Free Energy optimization in an implicit way via the mesasge passing technique. That means that the inference engine does not compute BFE values explicitly,  unless specified explicitly. The infer function has free_energy flag, which indicates whether BFE values must be computed explicitly or not. Note, however, that due to the reactive nature of the message passing implementation in RxInfer the computed BFE value may not represent its actual state. This may happen when updates for certain posteriors arriving more often than updates for other posteriors and usually tend to happen in models with loops in its structure. To circumvent this, instead of checking if BFE value is being minimized it is advised to check if it converges.","category":"page"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"RxInfer.BetheFreeEnergy\nRxInfer.BetheFreeEnergyDefaultMarginalSkipStrategy\nRxInfer.BetheFreeEnergyDefaultScheduler\nRxInfer.ReactiveMPFreeEnergyPlugin","category":"page"},{"location":"library/bethe-free-energy/#RxInfer.BetheFreeEnergy","page":"Bethe Free Energy","title":"RxInfer.BetheFreeEnergy","text":"BetheFreeEnergy(skip_strategy, scheduler)\n\nImplements a reactive stream for Bethe Free Energy values.  Must be used in combination with the score function of ReactiveMP.jl. \n\nArguments\n\n::Type{T}: a type of the counting real number, e.g. Float64. Set to Real by default, otherwise the inference procedure is not automatically differentiable.\nskip_strategy: a strategy that defines which posterior marginals to skip, e.g. SkipInitial().\nscheduler: a scheduler for the underlying stream, e.g. AsapScheduler().\n\n\n\n\n\n","category":"type"},{"location":"library/bethe-free-energy/#RxInfer.BetheFreeEnergyDefaultMarginalSkipStrategy","page":"Bethe Free Energy","title":"RxInfer.BetheFreeEnergyDefaultMarginalSkipStrategy","text":"Default marginal skip strategy for the Bethe Free Energy objective. \n\n\n\n\n\n","category":"constant"},{"location":"library/bethe-free-energy/#RxInfer.BetheFreeEnergyDefaultScheduler","page":"Bethe Free Energy","title":"RxInfer.BetheFreeEnergyDefaultScheduler","text":"Default scheduler for the Bethe Free Energy objective.\n\n\n\n\n\n","category":"constant"},{"location":"library/bethe-free-energy/#RxInfer.ReactiveMPFreeEnergyPlugin","page":"Bethe Free Energy","title":"RxInfer.ReactiveMPFreeEnergyPlugin","text":"A plugin for GraphPPL graph engine that adds the Bethe Free Energy objective computation to the nodes of the model.\n\n\n\n\n\n","category":"type"},{"location":"library/bethe-free-energy/#Extra-diagnostic-checks","page":"Bethe Free Energy","title":"Extra diagnostic checks","text":"","category":"section"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"RxInfer verifies intermediate computations of BFE on each iteration. By default, RxInfer will throw an exception, if local factor node or variable node computations result in either NaN or Inf. Note, that the verification happens only if the computation of BFE has been requested explicitly.","category":"page"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"RxInfer.apply_diagnostic_check\nRxInfer.ObjectiveDiagnosticCheckNaNs\nRxInfer.ObjectiveDiagnosticCheckInfs\nRxInfer.DefaultObjectiveDiagnosticChecks","category":"page"},{"location":"library/bethe-free-energy/#RxInfer.apply_diagnostic_check","page":"Bethe Free Energy","title":"RxInfer.apply_diagnostic_check","text":"apply_diagnostic_check(check, stream)\n\nThis function applies a check to the stream. Does nothing if check is of type Nothing. \n\n\n\n\n\n","category":"function"},{"location":"library/bethe-free-energy/#RxInfer.ObjectiveDiagnosticCheckNaNs","page":"Bethe Free Energy","title":"RxInfer.ObjectiveDiagnosticCheckNaNs","text":"ObjectiveDiagnosticCheckNaNs\n\nIf enabled checks that both variable and factor bound score functions in the objective computation do not return NaNs.  Throws an error if finds NaN. \n\n\n\n\n\n","category":"type"},{"location":"library/bethe-free-energy/#RxInfer.ObjectiveDiagnosticCheckInfs","page":"Bethe Free Energy","title":"RxInfer.ObjectiveDiagnosticCheckInfs","text":"ObjectiveDiagnosticCheckInfs\n\nIf enabled checks that both variable and factor bound score functions in the objective computation do not return Infs.  Throws an error if finds Inf. \n\n\n\n\n\n","category":"type"},{"location":"library/bethe-free-energy/#RxInfer.DefaultObjectiveDiagnosticChecks","page":"Bethe Free Energy","title":"RxInfer.DefaultObjectiveDiagnosticChecks","text":"const DefaultObjectiveDiagnosticChecks = (ObjectiveDiagnosticCheckNaNs(), ObjectiveDiagnosticCheckInfs())\n\nA constant that defines the default objective diagnostic checks.\n\n\n\n\n\n","category":"constant"},{"location":"manuals/inference/delta-node/#delta-node-manual","page":"Deterministic nodes","title":"Deterministic nodes","text":"","category":"section"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"RxInfer.jl offers a comprehensive set of stochastic nodes, primarily emphasizing distributions from the exponential family and related compositions, such as Gaussian with controlled variance (GCV) or autoregressive (AR) nodes. The DeltaNode stands out in this package, representing a deterministic transformation of either a single random variable or a group of them. This guide provides insights into the DeltaNode and its functionalities.","category":"page"},{"location":"manuals/inference/delta-node/#Features-and-Supported-Inference-Scenarios","page":"Deterministic nodes","title":"Features and Supported Inference Scenarios","text":"","category":"section"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"The delta node supports several approximation methods for probabilistic inference. The desired approximation method depends on the nodes connected to the delta node. We differentiate the following deterministic transformation scenarios:","category":"page"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"Gaussian Nodes: For delta nodes linked to strictly multivariate or univariate Gaussian distributions, the recommended methods are Linearization or Unscented transforms.\nExponential Family Nodes: For the delta node connected to nodes from the exponential family, the CVI (Conjugate Variational Inference) is the method of choice.\nStacking Delta Nodes: For scenarios where delta nodes are stacked, either Linearization or Unscented transforms are suitable.","category":"page"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"The table below summarizes the features of the delta node in RxInfer.jl, categorized by the approximation method:","category":"page"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"Methods Gaussian Nodes Exponential Family Nodes Stacking Delta Nodes\nLinearization ✓ ✗ ✓\nUnscented ✓ ✗ ✓\nCVI ✓ ✓ ✗","category":"page"},{"location":"manuals/inference/delta-node/#Gaussian-Case","page":"Deterministic nodes","title":"Gaussian Case","text":"","category":"section"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"In the context of Gaussian distributions, we recommend either the Linearization or Unscented method for delta node approximation. The Linearization method provides a first-order approximation, while the Unscented method delivers a more precise second-order approximation. It's worth noting that while the Unscented method is more accurate, it may require hyperparameters tuning.","category":"page"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"For clarity, consider the following example:","category":"page"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"using RxInfer\n\n@model function delta_node_example(z)\n    x ~ Normal(mean=0.0, var=1.0)\n    y := tanh(x)\n    z ~ Normal(mean=y, var=1.0)\nend","category":"page"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"To perform inference on this model, designate the approximation method for the delta node (here, the tanh function) using the @meta specification:","category":"page"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"delta_meta = @meta begin \n    tanh() -> Linearization()\nend","category":"page"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"or","category":"page"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"delta_meta = @meta begin \n    tanh() -> Unscented()\nend","category":"page"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"For a deeper understanding of the Unscented method and its parameters, consult the docstrings.","category":"page"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"Given the invertibility of tanh, indicating its inverse function can optimize the inference procedure:","category":"page"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"delta_meta = @meta begin \n    tanh() -> DeltaMeta(method = Linearization(), inverse = atanh)\nend","category":"page"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"To execute the inference procedure:","category":"page"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"infer(model = delta_node_example(), meta=delta_meta, data = (z = 1.0,))","category":"page"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"This methodology is consistent even when the delta node is associated with multiple nodes. For instance:","category":"page"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"f(x, g) = x*tanh(g)","category":"page"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"@model function delta_node_example(z)\n    x ~ Normal(mean=1.0, var=1.0)\n    g ~ Normal(mean=1.0, var=1.0)\n    y := f(x, g)\n    z ~ Normal(mean=y, var=0.1)\nend","category":"page"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"The corresponding meta specification is:","category":"page"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"delta_meta = @meta begin \n    f() -> DeltaMeta(method = Linearization())\nend","category":"page"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"or simply","category":"page"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"delta_meta = @meta begin \n    f() -> Linearization()\nend","category":"page"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"If specific functions outline the backward relation of variables within the f function, you can provide a tuple of inverse functions in the order of the variables:","category":"page"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"f_back_x(out, g) = out/tanh(g)\nf_back_g(out, x) = atanh(out/x)","category":"page"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"delta_meta = @meta begin \n    f() -> DeltaMeta(method = Linearization(), inverse=(f_back_x, f_back_g))\nend","category":"page"},{"location":"manuals/inference/delta-node/#Exponential-Family-Case","page":"Deterministic nodes","title":"Exponential Family Case","text":"","category":"section"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"When the delta node is associated with nodes from the exponential family (excluding Gaussians), the Linearization and Unscented methods are not applicable. In such cases, the CVI (Conjugate Variational Inference) is available. Here's a modified example:","category":"page"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"using RxInfer\n\n@model function delta_node_example1(z)\n    x ~ Gamma(shape=1.0, rate=1.0)\n    y := tanh(x)\n    z ~ Bernoulli(y)\nend","category":"page"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"The corresponding meta specification can be represented as:","category":"page"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"using StableRNGs\nusing Optimisers\n\ndelta_meta = @meta begin \n    tanh() -> DeltaMeta(method = CVI(StableRNG(42), 100, 100, Optimisers.Descent(0.01)))\nend","category":"page"},{"location":"manuals/inference/delta-node/","page":"Deterministic nodes","title":"Deterministic nodes","text":"Consult the ProdCVI docstrings for a detailed explanation of these parameters.","category":"page"}]
}

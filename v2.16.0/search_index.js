var documenterSearchIndex = {"docs":
[{"location":"manuals/delta-node/#delta-node-manual","page":"Delta node","title":"Delta node manual","text":"","category":"section"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"RxInfer.jl offers a comprehensive set of stochastic nodes, with a primary emphasis on distributions from the exponential family and its associated compositions, such as Gaussian with controlled variance (GCV) or autoregressive (AR) nodes. The DeltaNode stands out in this package, representing a deterministic transformation of either a single random variable or a group of them. This guide provides insights into the DeltaNode and its functionalities.","category":"page"},{"location":"manuals/delta-node/#Features-and-Supported-Inference-Scenarios","page":"Delta node","title":"Features and Supported Inference Scenarios","text":"","category":"section"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"The delta node has several approximation methods for performing probabilistic inference. The desired approximation method depends on the nodes connected to the delta node. We differentiate the following deterministic transformation scenarios:","category":"page"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"Gaussian Nodes: For delta nodes linked to strictly multivariate or univariate Gaussian distributions, the recommended methods are Linearization or Unscented transforms.\nExponential Family Nodes: For the delta node connected to nodes from the exponential family, the CVI (Conjugate Variational Inference) is the method of choice.\nStacking Delta Nodes: For scenarios where delta nodes are stacked, either Linearization or Unscented transforms are suitable.","category":"page"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"The table below summarizes the features of the delta node in RxInfer.jl, categorized by the approximation method:","category":"page"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"Methods Gaussian Nodes Exponential Family Nodes Stacking Delta Nodes\nLinearization ✓ ✗ ✓\nUnscented ✓ ✗ ✓\nCVI ✓ ✓ ✗","category":"page"},{"location":"manuals/delta-node/#Gaussian-Case","page":"Delta node","title":"Gaussian Case","text":"","category":"section"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"In the context of Gaussian distributions, we recommend either the Linearization or Unscented method for delta node approximation. The Linearization method provides a first-order approximation, while the Unscented method delivers a more precise second-order approximation. It's worth noting that while the Unscented method is more accurate, it may require hyper-parameters tuning.","category":"page"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"For clarity, consider the following example:","category":"page"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"using RxInfer\n\n@model function delta_node_example()\n    z = datavar(Float64)\n    x ~ Normal(mean=0.0, var=1.0)\n    y ~ tanh(x)\n    z ~ Normal(mean=y, var=1.0)\nend","category":"page"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"To perform inference on this model, designate the approximation method for the delta node (here, the tanh function) using the @meta specification:","category":"page"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"delta_meta = @meta begin \n    tanh() -> Linearization()\nend","category":"page"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"or","category":"page"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"delta_meta = @meta begin \n    tanh() -> Unscented()\nend","category":"page"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"For a deeper understanding of the Unscented method and its parameters, consult the docstrings.","category":"page"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"Given the invertibility of tanh, indicating its inverse function can optimize the inference procedure:","category":"page"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"delta_meta = @meta begin \n    tanh() -> DeltaMeta(method = Linearization(), inverse = atanh)\nend","category":"page"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"To execute the inference procedure:","category":"page"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"infer(model = delta_node_example(), meta=delta_meta, data = (z = 1.0,))","category":"page"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"This methodology is consistent even when the delta node is associated with multiple nodes. For instance:","category":"page"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"f(x, g) = x*tanh(g)","category":"page"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"@model function delta_node_example()\n    z = datavar(Float64)\n    x ~ Normal(mean=1.0, var=1.0)\n    g ~ Normal(mean=1.0, var=1.0)\n    y ~ f(x, g)\n    z ~ Normal(mean=y, var=0.1)\nend","category":"page"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"The corresponding meta specification is:","category":"page"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"delta_meta = @meta begin \n    f() -> DeltaMeta(method = Linearization())\nend","category":"page"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"or simply","category":"page"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"delta_meta = @meta begin \n    f() -> Linearization()\nend","category":"page"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"If specific functions outline the backward relation of variables within the f function, you can provide a tuple of inverse functions in the order of the variables:","category":"page"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"delta_meta = @meta begin \n    f() -> DeltaMeta(method = Linearization(), inverse=(f_back_x, f_back_g))\nend","category":"page"},{"location":"manuals/delta-node/#Exponential-Family-Case","page":"Delta node","title":"Exponential Family Case","text":"","category":"section"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"When the delta node is associated with nodes from the exponential family (excluding Gaussians), the Linearization and Unscented methods are not applicable. In such cases, the CVI (Conjugate Variational Inference) is available. Here's a modified example:","category":"page"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"using RxInfer\n\n@model function delta_node_example1()\n    z = datavar(Float64)\n    x ~ Gamma(shape=1.0, rate=1.0)\n    y ~ tanh(x)\n    z ~ Bernoulli(y)\nend","category":"page"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"The corresponding meta specification can be represented as:","category":"page"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"using StableRNGs\nusing Optimisers\n\ndelta_meta = @meta begin \n    tanh() -> DeltaMeta(method = CVI(StableRNG(42), 100, 100, Optimisers.Descent(0.01)))\nend","category":"page"},{"location":"manuals/delta-node/","page":"Delta node","title":"Delta node","text":"Consult the ProdCVI docstrings for a detailed explanation of these parameters.","category":"page"},{"location":"manuals/inference/postprocess/#user-guide-inference-postprocess","page":"Inference results postprocessing","title":"Inference results postprocessing","text":"","category":"section"},{"location":"manuals/inference/postprocess/","page":"Inference results postprocessing","title":"Inference results postprocessing","text":"infer allow users to postprocess the inference result with the postprocess = ... keyword argument. The inference engine  operates on wrapper types to distinguish between marginals and messages. By default  these wrapper types are removed from the inference results if no addons option is present. Together with the enabled addons, however, the wrapper types are preserved in the  inference result output value. Use the options below to change this behaviour:","category":"page"},{"location":"manuals/inference/postprocess/","page":"Inference results postprocessing","title":"Inference results postprocessing","text":"DefaultPostprocess\nUnpackMarginalPostprocess\nNoopPostprocess","category":"page"},{"location":"manuals/inference/postprocess/#RxInfer.DefaultPostprocess","page":"Inference results postprocessing","title":"RxInfer.DefaultPostprocess","text":"DefaultPostprocess picks the most suitable postprocessing step automatically\n\n\n\n\n\n","category":"type"},{"location":"manuals/inference/postprocess/#RxInfer.UnpackMarginalPostprocess","page":"Inference results postprocessing","title":"RxInfer.UnpackMarginalPostprocess","text":"This postprocessing step removes the Marginal wrapper type from the result\n\n\n\n\n\n","category":"type"},{"location":"manuals/inference/postprocess/#RxInfer.NoopPostprocess","page":"Inference results postprocessing","title":"RxInfer.NoopPostprocess","text":"This postprocessing step does nothing\n\n\n\n\n\n","category":"type"},{"location":"manuals/model-specification/#user-guide-model-specification","page":"Model specification","title":"Model Specification","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"The RxInfer.jl package exports the @model macro for model specification. This @model macro accepts the model specification itself in a form of regular Julia function. For example: ","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"@model function model_name(model_arguments...; model_keyword_arguments...)\n    # model specification here\n    return ...\nend","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Model options, model_arguments and model_keyword_arguments are optional and may be omitted:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"@model function model_name()\n    # model specification here\n    return ...\nend","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"The @model macro returns a regular Julia function (in this example model_name()) which can be executed as usual. It returns a so-called model generator object, e.g:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"@model function my_model(model_arguments...)\n    # model specification here\n    # ...\n    return x, y\nend","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"generator = my_model(model_arguments...)","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"In order to create an instance of the model object we should use the create_model function:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"model, (x, y) = create_model(generator)","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"It is not necessary to return anything from the model, in that case RxInfer.jl will automatically inject return nothing to the end of the model function.","category":"page"},{"location":"manuals/model-specification/#A-full-example-before-diving-in","page":"Model specification","title":"A full example before diving in","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Before presenting the details of the model specification syntax, an example of a probabilistic model is given. Here is an example of a simple state space model with latent random variables x and noisy observations y:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"@model function state_space_model(n_observations, noise_variance)\n\n    c = constvar(1.0)\n    x = randomvar(n_observations)\n    y = datavar(Float64, n_observations)\n\n    x[1] ~ NormalMeanVariance(0.0, 100.0)\n\n    for i in 2:n_observations\n       x[i] ~ x[i - 1] + c\n       y[i] ~ NormalMeanVariance(x[i], noise_var)\n    end\n\n    return x, y\nend","category":"page"},{"location":"manuals/model-specification/#Graph-variables-creation","page":"Model specification","title":"Graph variables creation","text":"","category":"section"},{"location":"manuals/model-specification/#user-guide-model-specification-constant-variables","page":"Model specification","title":"Constants","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Even though any runtime constant passed to a model as a model argument will be automatically converted to a fixed constant, sometimes it might be useful to create constants by hand (e.g. to avoid copying large matrices across the model and to avoid extensive memory allocations).","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"You can create a constant within a model specification macro with constvar() function. For example:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"@model function model_name(...)\n    ...\n    c = constvar(1.0)\n\n    for i in 2:n\n        x[i] ~ x[i - 1] + c # Reuse the same reference to a constant 1.0\n    end\n    ...\nend","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"note: Note\nconstvar() function is supposed to be used only within the @model macro.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Additionally you can specify an extra ::ConstVariable type for some of the model arguments. In this case macro automatically converts them to a single constant using constvar() function. E.g.:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"@model function model_name(nsamples::Int, c::ConstVariable)\n    ...\n    # no need to call for a constvar() here\n    for i in 2:n\n        x[i] ~ x[i - 1] + c # Reuse the same reference to a constant `c`\n    end\n    ...\nend","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"note: Note\n::ConstVariable annotation does not play role in Julia's multiple dispatch. RxInfer.jl removes this annotation and replaces it with ::Any.","category":"page"},{"location":"manuals/model-specification/#user-guide-model-specification-data-variables","page":"Model specification","title":"Data variables","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"It is important to have a mechanism to pass data values to the model. You can create data inputs with datavar() function. As a first argument it accepts a type specification and optional dimensionality (as additional arguments or as a tuple). User can treat datavar()s in the model as both clamped values for priors and observations.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Examples: ","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"@model function model_name(...)\n    ...\n    y = datavar(Float64) # Creates a single data input with `y` as identificator\n    y = datavar(Float64, n) # Returns a vector of  `y_i` data input objects with length `n`\n    y = datavar(Float64, n, m) # Returns a matrix of `y_i_j` data input objects with size `(n, m)`\n    y = datavar(Float64, (n, m)) # It is also possible to use a tuple for dimensionality\n    ...\nend","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"note: Note\ndatavar() function is supposed to be used only within the @model macro.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"datavar() call within @model macro supports where { options... } block for extra options specification, e.g:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"@model function model_name(...)\n    ...\n    y = datavar(Float64, n) where { allow_missing = true }\n    ...\nend","category":"page"},{"location":"manuals/model-specification/#Data-variables-available-options","page":"Model specification","title":"Data variables available options","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"allow_missing = true/false: Specifies if it is possible to pass missing object as an observation. Note however that by default the ReactiveMP inference engine does not expose any message computation rules that involve missings.","category":"page"},{"location":"manuals/model-specification/#user-guide-model-specification-random-variables","page":"Model specification","title":"Random variables","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"There are several ways to create random variables. The first one is an explicit call to randomvar() function. By default it doesn't accept any argument, creates a single random variable in the model and returns it. It is also possible to pass dimensionality arguments to randomvar() function in the same way as for the datavar() function.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Examples: ","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"@model function model_name(...)\n    ...\n    x = randomvar() # Returns a single random variable which can be used later in the model\n    x = randomvar(n) # Returns an vector of random variables with length `n`\n    x = randomvar(n, m) # Returns a matrix of random variables with size `(n, m)`\n    x = randomvar((n, m)) # It is also possible to use a tuple for dimensionality\n    ...\nend","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"note: Note\nrandomvar() function is supposed to be used only within the @model macro.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"randomvar() call within @model macro supports where { options... } block for extra options specification, e.g:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"@model function model_name(...)\n    ...\n    y = randomvar() where { prod_constraint = ProdGeneric() }\n    ...\nend","category":"page"},{"location":"manuals/model-specification/#Random-variables-available-options","page":"Model specification","title":"Random variables available options","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"prod_constraint\nprod_strategy\nmarginal_form_constraint\nmarginal_form_check_strategy\nmessages_form_constraint\nmessages_form_check_strategy\npipeline","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"The second way to create a random variable is to create a node with the ~ operator. If the random variable has not yet been created before this call, it will be created automatically during the creation of the node. Read more about the ~ operator below.","category":"page"},{"location":"manuals/model-specification/#Node-creation","page":"Model specification","title":"Node creation","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Factor nodes are used to define a relationship between random variables and/or constants and data inputs. A factor node defines a probability distribution over selected random variables. ","category":"page"},{"location":"manuals/model-specification/#Distributions.jl-compatibility","page":"Model specification","title":"Distributions.jl compatibility","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"For some factor nodes we rely on the syntax from Distributions.jl to make it easy to adopt RxInfer.jl for these users. These nodes include for example the Beta and Wishart distributions. These nodes can be created using the ~ syntax with the arguments as specified in Distributions.jl. Unfortunately, we RxInfer.jl is not yet compatible with all possible distributions to be used as factor nodes. If you feel that you would like to see another node implemented, please file an issue.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"note: Note\nTo quickly check the list of all available factor nodes that can be used in the model specification language call ?make_node or Base.doc(make_node).","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Specifically for the Gaussian/Normal case we have custom implementations that yield a higher computational efficiency and improved stability in comparison to Distributions.jl as these are optimized for sampling operations. Our aliases for these distributions therefore do not correspond to the implementations from Distributions.jl. However, our model specification language is compatible with syntax from Distributions.jl for normal distributions, which will be automatically converted. RxInfer has its own implementation because of the following 3 reasons:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Distributions.jl constructs normal distributions by saving the corresponding covariance matrices in a PDMat object from PDMats.jl. This construction always computes the Cholesky decompositions of the covariance matrices, which is very convenient for sampling-based procedures. However, in RxInfer.jl we mostly base our computations on analytical expressions which do not always need to compute the Cholesky decomposition. In order to reduce the overhead that Distributions.jl introduces, we therefore have custom implementations.\nDepending on the update rules, we might favor different parameterizations of the normal distributions. ReactiveMP.jl has quite a variety in parameterizations that allow us to efficient computations where we convert between parameterizations as little as possible.\nIn certain situations we value stability a lot, especially when inverting matrices. PDMats.jl, and hence Distributions.jl, is not capable to fulfill all needs that we have here. Therefore we use PositiveFactorizations.jl to cope with the corner-cases.","category":"page"},{"location":"manuals/model-specification/#Tilde-syntax","page":"Model specification","title":"Tilde syntax","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"We model a random variable by a probability distribution using the ~ operator. For example, to create a random variable y which is modeled by a Normal distribution, where its mean and variance are controlled by the random variables m and v respectively, we define","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"@model function model_name(...)\n    ...\n    m = randomvar()\n    v = randomvar()\n    y ~ NormalMeanVariance(m, v) # Creates a `y` random variable automatically\n    ...\nend","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Another example, but using a deterministic relation between random variables:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"@model function model_name(...)\n    ...\n    a = randomvar()\n    b = randomvar()\n    c ~ a + b\n    ...\nend","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"note: Note\nThe RxInfer.jl package uses the ~ operator for modelling both stochastic and deterministic relationships between random variables.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"The @model macro automatically resolves any inner function calls into anonymous extra nodes in case this inner function call is a non-linear transformation. It will also create needed anonymous random variables. But it is important to note that the inference backend will try to optimize inner non-linear deterministic function calls in the case where all arguments are constants or data inputs. For example:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"noise ~ NormalMeanVariance(mean, inv(precision)) # Will create a non-linear `inv` node in case if `precision` is a random variable. Won't create an additional non-linear node in case if `precision` is a constant or data input.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"It is possible to use any functional expression within the ~ operator arguments list. The only one exception is the ref expression (e.g x[i]). All reference expressions within the ~ operator arguments list are left untouched during model parsing. This means that the model parser will not create unnecessary nodes when only simple indexing is involved.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"note: Note\nIt is forbidden to use random variable within square brackets in the model specification.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"y ~ NormalMeanVariance(x[i - 1], variance) # While in principle `i - 1` is an inner function call (`-(i, 1)`) model parser will leave it untouched and won't create any anonymous nodes for `ref` expressions.\n\ny ~ NormalMeanVariance(A * x[i - 1], variance) # This example will create a `*` anonymous node (in case if x[i - 1] is a random variable) and leave `x[i - 1]` untouched.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"It is also possible to return a node reference from the ~ operator. Use the following syntax:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"node, y ~ NormalMeanVariance(mean, var)","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Having a node reference can be useful in case the user wants to return it from a model and to use it later on to specify initial joint marginal distributions.","category":"page"},{"location":"manuals/model-specification/#Broadcasting-syntax","page":"Model specification","title":"Broadcasting syntax","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"note: Note\nBroadcasting syntax requires at least v2.1.0 of GraphPPL.jl ","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"GraphPPL support broadcasting for ~ operator in the exact same way as Julia itself. A user is free to write an expression of the following form:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"y = datavar(Float64, n)\ny .~ NormalMeanVariance(0.0, 1.0) # <- i.i.d observations","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"More complex expression are also allowed:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"m ~ NormalMeanPrecision(0.0, 0.0001)\nt ~ Gamma(1.0, 1.0)\n\ny = randomvar(Float64, n)\ny .~ NormalMeanPrecision(m, t)","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"A = constvar(...)\nx = randomvar(n)\ny = datavar(Vector{Float64}, n)\n\nw         ~ Wishart(3, diageye(2))\nx[1]      ~ MvNormalMeanPrecision(zeros(2), diageye(2))\nx[2:end] .~ A .* x[1:end-1] # <- State-space model with transition matrix A\ny        .~ MvNormalMeanPrecision(x, w) # <- Observations with unknown precision matrix","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Note, however, that all variables that take part in the broadcasting operation must be defined before either with randomvar or datavar. The exception here is constants that are automatically converted to their constvar equivalent. If you want to prevent broadcasting for some constant (e.g. if you want to add a vector to a multivariate Gaussian distribution) use explicit constvar call:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"# Suppose `x` is a 2-dimensional Gaussian distribution\nz .~ x .+ constvar([ 1, 1 ])\n# Which is equivalent to \nfor i in 1:n\n   z[i] ~ x[i] + constvar([ 1, 1 ])\nend","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Without explicit constvar Julia's broadcasting machinery would instead attempt to unroll for loop in the following way:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"# Without explicit `constvar`\nz .~ x .+ [ 1, 1 ]\n# Which is equivalent to \narray = [1, 1]\nfor i in 1:n\n   z[i] ~ x[i] + array[i] # This is wrong if `x[i]` is supposed to be a multivariate Gaussian \nend","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Read more about how broadcasting machinery works in Julia in the official documentation.","category":"page"},{"location":"manuals/model-specification/#Node-creation-options","page":"Model specification","title":"Node creation options","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"To pass optional arguments to the node creation constructor the user can use the where { options...  } options specification syntax.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Example:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"y ~ NormalMeanVariance(y_mean, y_var) where { q = q(y_mean)q(y_var)q(y) } # mean-field factorisation over q","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"A list of the available options specific to the ReactiveMP inference engine is presented below.","category":"page"},{"location":"manuals/model-specification/#Factorisation-constraint-option","page":"Model specification","title":"Factorisation constraint option","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"See also Constraints Specification section.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Users can specify a factorisation constraint over the approximate posterior q for variational inference. The general syntax for factorisation constraints over q is the following:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"variable ~ Node(node_arguments...) where { q = RecognitionFactorisationConstraint }","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"where RecognitionFactorisationConstraint can be the following","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"MeanField()","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Automatically specifies a mean-field factorisation","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Example:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"y ~ NormalMeanVariance(y_mean, y_var) where { q = MeanField() }","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"FullFactorisation()","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Automatically specifies a full factorisation","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Example:","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"y ~ NormalMeanVariance(y_mean, y_var) where { q = FullFactorisation() }","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"q(μ)q(v)q(out) or q(μ) * q(v) * q(out)","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"A user can specify any factorisation he wants as the multiplication of q(interface_names...) factors. As interface names the user can use the interface names of an actual node (read node's documentation), its aliases (if available) or actual random variable names present in the ~ operator expression.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Examples: ","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"# Using interface names of a `NormalMeanVariance` node for factorisation constraint. \n# Call `?NormalMeanVariance` to know more about interface names for some node\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(μ)q(v)q(out) }\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(μ, v)q(out) }\n\n# Using interface names aliases of a `NormalMeanVariance` node for factorisation constraint. \n# Call `?NormalMeanVariance` to know more about interface names aliases for some node\n# In general aliases correspond to the function names for distribution parameters\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(mean)q(var)q(out) }\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(mean, var)q(out) }\n\n# Using random variables names from `~` operator expression\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(y_mean)q(y_var)q(y) }\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(y_mean, y_var)q(y) }\n\n# All methods can be combined easily\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(μ)q(y_var)q(out) }\ny ~ NormalMeanVariance(y_mean, y_var) where { q = q(y_mean, v)q(y) }","category":"page"},{"location":"manuals/model-specification/#Metadata-option","page":"Model specification","title":"Metadata option","text":"","category":"section"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"Is is possible to pass any extra metadata to a factor node with the meta option. Metadata can be later accessed in message computation rules. See also Meta specification section.","category":"page"},{"location":"manuals/model-specification/","page":"Model specification","title":"Model specification","text":"z ~ f(x, y) where { meta = ... }","category":"page"},{"location":"examples/overview/#examples-overview","page":"Overview","title":"Examples overview","text":"","category":"section"},{"location":"examples/overview/","page":"Overview","title":"Overview","text":"This section contains a set of examples for Bayesian Inference with RxInfer package in various probabilistic models.","category":"page"},{"location":"examples/overview/","page":"Overview","title":"Overview","text":"note: Note\nAll examples have been pre-generated automatically from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/overview/","page":"Overview","title":"Overview","text":"Basic examples: Basic examples contain \"Hello World!\" of Bayesian inference in RxInfer.\nAdvanced examples: Advanced examples contain more complex inference problems.\nProblem specific: Problem specific examples contain specialized models and inference for various domains.","category":"page"},{"location":"manuals/inference/manual/#user-guide-manual-inference","page":"Manual inference specification","title":"Manual inference","text":"","category":"section"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"For advanced use cases it is advised to use manual inference specification. ","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"Manual inference specification with RxInfer usually consists of the same simple building blocks and designed in such a way to support both static and real-time infinite datasets:","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"Create a model with @model macro and get a references to random variables and data inputs\nSubscribe to random variable posterior marginal updates \nSubscribe to Bethe Free Energy updates (optional)\nFeed model with observations \nUnsubscribe from posterior marginal updates (optional)","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"It is worth to note that Step 5 is optional and in case where observations come from an infinite real-time data stream (e.g. from an external source or the internet) it may be justified to never unsubscribe and perform real-time Bayesian inference in a reactive manner as soon as data arrives.","category":"page"},{"location":"manuals/inference/manual/#user-guide-manual-inference-model-creation","page":"Manual inference specification","title":"Model creation","text":"","category":"section"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"During model specification stage user decides on variables of interest in a model and returns (optionally) them using a return ... statement. As an example consider that we have a simple hierarchical model in which the mean of a Normal distribution is represented by another Normal distribution whose mean is modelled by another Normal distribution.","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"using RxInfer, Distributions, Random\n\n@model function my_model()\n    m2 ~ NormalMeanVariance(0.0, 1.0)\n    m1 ~ NormalMeanVariance(m2, 1.0)\n\n    y = datavar(Float64)\n    y ~ NormalMeanVariance(m1, 1.0)\n\n    # Return variables of interests, optional\n    return m1, y\nend","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"And later on we may create our model with the create_model function and obtain references for variables of interests:","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"model, (m1, y) = create_model(my_model())\nnothing #hide","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"Alternatively, it is possible to query any variable using squared brackets on model object:","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"model[:m1] # m1\nmodel[:y]  # y\nnothing #hide","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"@model macro also return a reference for a factor graph as its first return value. Factor graph object (named model in previous example) contains all information about all factor nodes in a model as well as random variables and data inputs.","category":"page"},{"location":"manuals/inference/manual/#user-guide-manual-inference-marginal-updates","page":"Manual inference specification","title":"Posterior marginal updates","text":"","category":"section"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"The RxInfer inference engine has a reactive API and operates in terms of Observables and Actors. For detailed information about these concepts we refer to Rocket.jl documentation.","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"We use getmarginal function from ReactiveMP to get a posterior marginal updates observable:","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"m1_posterior_updates = getmarginal(m1)\nnothing #hide","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"After that we can subscribe on new updates and perform some actions based on new values:","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"m1_posterior_subscription = subscribe!(m1_posterior_updates, (new_posterior) -> begin\n    println(\"New posterior for m1: \", new_posterior)\nend)\nnothing #hide","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"Sometimes it is useful to return an array of random variables from model specification, in this case we may use getmarginals() function that transform an array of observables to an observable of arrays.","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"@model function my_model()\n    ...\n    m_n = randomvar(n)\n    ...\n    return m_n, ...\nend\n\nmodel, (m_n, ...) = create_model(my_model())\n\nm_n_updates = getmarginals(m_n)","category":"page"},{"location":"manuals/inference/manual/#user-guide-manual-inference-observations","page":"Manual inference specification","title":"Feeding observations","text":"","category":"section"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"By default (without any extra factorisation constraints) model specification implies Belief Propagation message passing update rules. In case of BP algorithm RxInfer package computes an exact Bayesian posteriors with a single message passing iteration. To enforce Belief Propagation message passing update rule for some specific factor node user may use where { q = FullFactorisation() } option. Read more in Model Specification section. To perform a message passing iteration we need to pass some data to all our data inputs that were created with datavar function during model specification.","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"To feed an observation for a specific data input we use update! function:","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"update!(y, 0.0)\nnothing #hide","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"As you can see after we passed a single value to our data input we got a posterior marginal update from our subscription and printed it with println function. In case of BP if observations do not change it should not affect posterior marginal results:","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"update!(y, 0.0) # Observation didn't change, should result in the same posterior\nnothing #hide","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"If y is an array of data inputs it is possible to pass an array of observation to update! function:","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"for i in 1:length(data)\n    update!(y[i], data[i])\nend\n# is an equivalent of\nupdate!(y, data)","category":"page"},{"location":"manuals/inference/manual/#user-guide-manual-inference-vmp","page":"Manual inference specification","title":"Variational Message Passing","text":"","category":"section"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"Variational message passing (VMP) algorithms are generated much in the same way as the belief propagation algorithm we saw in the previous section. There is a major difference though: for VMP algorithm generation we need to define the factorization properties of our approximate distribution. A common approach is to assume that all random variables of the model factorize with respect to each other. This is known as the mean field assumption. In RxInfer, the specification of such factorization properties is defined during model specification stage using the where { q = ... } syntax or with the @constraints macro (see Constraints specification section for more info about the @constraints macro). Let's take a look at a simple example to see how it is used. In this model we want to learn the mean and precision of a Normal distribution, where the former is modelled with a Normal distribution and the latter with a Gamma.","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"using RxInfer, Distributions, Random","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"real_mean      = -4.0\nreal_precision = 0.2\nrng            = MersenneTwister(1234)\n\nn    = 100\ndata = rand(rng, Normal(real_mean, sqrt(inv(real_precision))), n)\nnothing #hide","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"@model function normal_estimation(n)\n    m ~ NormalMeanVariance(0.0, 10.0)\n    w ~ Gamma(0.1, 10.0)\n\n    y = datavar(Float64, n)\n\n    for i in 1:n\n        y[i] ~ NormalMeanPrecision(m, w) where { q = MeanField() }\n    end\n\n    return m, w, y\nend","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"We create our model as usual, however in order to start VMP inference procedure we need to set initial posterior marginals for all random variables in the model:","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"model, (m, w, y) = create_model(normal_estimation(n))\n\n# We use vague initial marginals\nsetmarginal!(m, vague(NormalMeanVariance)) \nsetmarginal!(w, vague(Gamma))\nnothing #hide","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"To perform a single VMP iteration it is enough to feed all data inputs with some values. To perform multiple VMP iterations we should feed our all data inputs with the same values multiple times:","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"m_marginals = []\nw_marginals = []\n\nsubscriptions = subscribe!([\n    (getmarginal(m), (marginal) -> push!(m_marginals, marginal)),\n    (getmarginal(w), (marginal) -> push!(w_marginals, marginal)),\n])\n\nvmp_iterations = 10\n\nfor _ in 1:vmp_iterations\n    update!(y, data)\nend\n\nunsubscribe!(subscriptions)\nnothing #hide","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"As we process more VMP iterations, our beliefs about the possible values of m and w converge and become more confident.","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"using Plots\n\np1    = plot(title = \"'Mean' posterior marginals\")\ngrid1 = -6.0:0.01:4.0\n\nfor iter in [ 1, 2, 10 ]\n\n    estimated = Normal(mean(m_marginals[iter]), std(m_marginals[iter]))\n    e_pdf     = (x) -> pdf(estimated, x)\n\n    plot!(p1, grid1, e_pdf, fill = true, opacity = 0.3, label = \"Estimated mean after $iter VMP iterations\")\n\nend\n\nplot!(p1, [ real_mean ], seriestype = :vline, label = \"Real mean\", color = :red4, opacity = 0.7)","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"p2    = plot(title = \"'Precision' posterior marginals\")\ngrid2 = 0.01:0.001:0.35\n\nfor iter in [ 2, 3, 10 ]\n\n    estimated = Gamma(shape(w_marginals[iter]), scale(w_marginals[iter]))\n    e_pdf     = (x) -> pdf(estimated, x)\n\n    plot!(p2, grid2, e_pdf, fill = true, opacity = 0.3, label = \"Estimated precision after $iter VMP iterations\")\n\nend\n\nplot!(p2, [ real_precision ], seriestype = :vline, label = \"Real precision\", color = :red4, opacity = 0.7)","category":"page"},{"location":"manuals/inference/manual/#user-guide-manual-inference-vmp-bfe","page":"Manual inference specification","title":"Computing Bethe Free Energy","text":"","category":"section"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"VMP inference boils down to finding the member of a family of tractable probability distributions that is closest in KL divergence to an intractable posterior distribution. This is achieved by minimizing a quantity known as Variational Free Energy. RxInfer uses Bethe Free Energy approximation to the real Variational Free Energy. Free energy is particularly useful to test for convergence of the VMP iterative procedure.","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"The RxInfer package exports score function for an observable of free energy values:","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"fe_observable = score(model, BetheFreeEnergy())\nnothing #hide","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"# Reset posterior marginals for `m` and `w`\nsetmarginal!(m, vague(NormalMeanVariance))\nsetmarginal!(w, vague(Gamma))\n\nfe_values = []\n\nfe_subscription = subscribe!(fe_observable, (v) -> push!(fe_values, v))\n\nvmp_iterations = 10\n\nfor _ in 1:vmp_iterations\n    update!(y, data)\nend\n\nunsubscribe!(fe_subscription)","category":"page"},{"location":"manuals/inference/manual/","page":"Manual inference specification","title":"Manual inference specification","text":"plot(fe_values, label = \"Bethe Free Energy\", xlabel = \"Iteration #\")","category":"page"},{"location":"contributing/new-package/#Contributing-to-the-dependencies","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"","category":"section"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"Julia programming language makes it extremely easy to create, develop and register new packages in the ecosystem.","category":"page"},{"location":"contributing/new-package/#The-benefits-of-small-packages-for-the-ecosystem","page":"Contributing to the dependencies","title":"The benefits of small packages for the ecosystem","text":"","category":"section"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"In the world of software development, there's often a choice to be made between creating a single monolithic package or breaking your codebase into smaller, more focused packages. While both approaches have their merits, opting for smaller packages can offer several significant benefits:","category":"page"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"Modularity: Smaller packages focus on specific tasks, making them easier to maintain and debug.\nCollaboration: Teams can work on different packages concurrently, speeding up development.\nVersion Control: Precise versioning and fewer dependencies lead to leaner projects.\nPerformance: Smaller packages can result in faster precompilation (in Julia) and more efficient testing.\nFlexibility: Developers can select and customize packages for their needs.\nCommunity: Smaller packages attract contributors, fostering collaboration and faster feedback.","category":"page"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"In summary, while monolithic packages have their place, opting for smaller, focused packages can bring numerous advantages in terms of modularity, collaboration, version control, flexibility, and community engagement.","category":"page"},{"location":"contributing/new-package/#Use-[PkgTemplates](https://github.com/JuliaCI/PkgTemplates.jl)","page":"Contributing to the dependencies","title":"Use PkgTemplates","text":"","category":"section"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"PkgTemplates.jl is a Julia package to create new Julia packages in an easy, repeatable, and customizable way. You can use the following template to generate a new package:","category":"page"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"julia> using PkgTemplates\n\njulia> USER = \"your github user name\" # Use `reactivebayes` if developing within the ReactiveBayes organisation\n\njulia> template = Template(\n    user = USER, \n    plugins = [\n        CompatHelper(), \n        ProjectFile(), \n        SrcDir(), \n        Git(), \n        License(), \n        Readme(), \n        Tests(), \n        GitHubActions(), \n        Codecov(), \n        Documenter{GitHubActions}(), \n        Formatter(style=\"blue\"), \n        BlueStyleBadge(), \n        PkgEvalBadge()\n])\n\njulia> template(\"MyNewCoolPackage\")","category":"page"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"This template generates a standard Julia package complete with streamlined documentation, tests, code coverage, and Blue style formatting. Refer to the PkgTemplates documentation if you wish to customize certain steps in the process.","category":"page"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"Adjust the minimum supported version of Julia","category":"page"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"After auto-generation, the minimum supported Julia version will be set to 1.0.0. You can modify this in the Project.toml file, for example:","category":"page"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"[compat]\njulia = \"1.9.2\"","category":"page"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"Try to be conservative and set as low version of Julia as possible.","category":"page"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"Adjust the authors of the package","category":"page"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"The authors field is present in the Project.toml, e.g ","category":"page"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"authors = [\"John Wick <john.wick@continental.com>\", ...]","category":"page"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"Add requires dependencies and their [compat] bounds","category":"page"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"To add new dependencies to your newly created package, start Julia in the package's folder and activate the project using one of the following methods:","category":"page"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"julia --project=.","category":"page"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"or ","category":"page"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"julia","category":"page"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"julia> ] activate .","category":"page"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"Then, add dependencies like this:","category":"page"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"julia> ] add SomeCoolDependency, SomeOtherCoolDependency","category":"page"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"For each new dependency, it's essential to specify the minimum compatible version in the [compat]section of the Project.toml file, otherwise the official Julia registry will not register your new package. Add the [compat] entries like this:","category":"page"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"[compat]\njulia = \"1.9\"\nSomeCoolDependency = \"0.19.2\"\nSomeOtherCoolDependency = \"1.3.12\"","category":"page"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"For more details on compat bounds, check the official Julia documentation.","category":"page"},{"location":"contributing/new-package/#Adjust-README.md","page":"Contributing to the dependencies","title":"Adjust README.md","text":"","category":"section"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"The README.md file is the front door to your project, offering a concise introduction and guidance for users and contributors. It's a critical piece of documentation that sets the tone for your project's accessibility and success. A well-crafted README.md provides essential information, such as installation instructions, usage examples, and project goals, making it easier for others to understand, engage with, and contribute to your work. So, remember, taking the time to write a clear and informative README.md.","category":"page"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"note: Note\nSome badges in the auto-generated README.md will be broken unless you register your package in the official Julia registry.","category":"page"},{"location":"contributing/new-package/#Write-code-and-tests","page":"Contributing to the dependencies","title":"Write code and tests","text":"","category":"section"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"The provided template generates a package with testing and test coverage enabled. Ensure to test all new functionality in the test/runtests.jl file.","category":"page"},{"location":"contributing/new-package/#Simplify-Testing-with-ReTestItems","page":"Contributing to the dependencies","title":"Simplify Testing with ReTestItems","text":"","category":"section"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"You can streamline testing by using the ReTestItems package, which support VSCode UI for running tests. Refer to the ReTestItems documentation for more information.","category":"page"},{"location":"contributing/new-package/#Write-code-and-the-documentation","page":"Contributing to the dependencies","title":"Write code and the documentation","text":"","category":"section"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"Julia, adding documentation is straightforward with the Documenter.jl package. Add docstrings to newly created functions and update the docs/index.md  ` file.  To build the documentation locally, use this command (ensure you initialize and instantiate the docs environment first):","category":"page"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"julia --project=docs docs/make.jl","category":"page"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"For customization, refer to the Documenter.jl documentation. Also, check out the contributing guide.","category":"page"},{"location":"contributing/new-package/#Hosting-Documentation-with-GitHub-Actions","page":"Contributing to the dependencies","title":"Hosting Documentation with GitHub Actions","text":"","category":"section"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"The template provided generates a package with automatic documentation hosting through GitHub Actions. To make this process work, you'll need to generate a DOCUMENTER_KEY using DocumenterTools.jl and add it to your package's repository settings. You can find detailed instructions on how to do this here.","category":"page"},{"location":"contributing/new-package/#Enable-GitHub-Pages-in-Repository-Settings","page":"Contributing to the dependencies","title":"Enable GitHub Pages in Repository Settings","text":"","category":"section"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"The final step for setting up documentation hosting is to enable GitHub Pages in your package's repository settings. To do this:","category":"page"},{"location":"contributing/new-package/","page":"Contributing to the dependencies","title":"Contributing to the dependencies","text":"Navigate to the GitHub Pages settings of your repository.\nChoose the Deploy from a branch option.\nSelect the gh-pages branch.","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/#examples-assessing-people’s-skills","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"","category":"section"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"The goal of this demo is to demonstrate the use of the @node and @rule macros, which allow the user to define custom factor nodes and associated update rules respectively. We will introduce these macros in the context of a root cause analysis on a student's test results. This demo is inspired by Chapter 2 of \"Model-Based Machine Learning\" by Winn et al.","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/#Problem-Statement","page":"Assessing People’s Skills","title":"Problem Statement","text":"","category":"section"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"We consider a student who takes a test that consists of three questions. Answering each question correctly requires a combination of skill and attitude. More precisely, has the student studied for the test, and have they partied the night before?","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"We model the result for question i as a continuous variable r_iin01, and skill/attitude as a binary variable s_i in 0 1, where s_1 represents whether the student has partied, and s_2 and s_3 represent whether the student has studied the chapters for the corresponding questions.","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"We assume the following logic:","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"If the student is alert (has not partied), then they will score on the first question;\nIf the student is alert or has studied chapter two, then they will score on question two;\nIf the student can answer question two and has studied chapter three, then they will score on question three.","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/#Generative-Model-Definition","page":"Assessing People’s Skills","title":"Generative Model Definition","text":"","category":"section"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"To model the probability for correct answers, we assume a latent state variable t_i in 01. The dependencies between the variables can then be modeled by the following Bayesian network:","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"(s_1)   (s_2)   (s_3)\n  |       |       |\n  v       v       v\n(t_1)-->(t_2)-->(t_3)\n  |       |       |\n  v       v       v\n(r_1)   (r_2)   (r_3)","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"As prior beliefs, we assume that a student is equally likely to study/party or not: s_i sim Ber(05) for all i. Next, we model the domain logic as beginaligned   t_1 = s_1\n  t_2 = t_1  s_2\n  t_3 = t_2  s_3 endaligned For the scoring results we might not have a specific forward model in mind. However, we can define a backward mapping, from continuous results to discrete latent variables, as  t_i sim Ber(s_i) for all i.","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/#Custom-Nodes-and-Rules","page":"Assessing People’s Skills","title":"Custom Nodes and Rules","text":"","category":"section"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"The backward mapping from results to latents is quite specific to our application. Moreover, it does not define a proper generative forward model. In order to still define a full generative model for our application, we can define a custom Score node and define an update rule that implements the backward mapping from scores to latents as a message.","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"In RxInfer, the @node macro defines a factor node. This macro accepts the new node type, an indicator for a stochastic or deterministic relationship, and a list of interfaces.","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"using RxInfer, Random\n\n# Create Score node\nstruct Score end\n\n@node Score Stochastic [out, in]","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"We can now define the backward mapping as a sum-product message through the @rule macro. This macro accepts the node type, the (outbound) interface on which the message is sent, any relevant constraints, and the message/distribution types on the remaining (inbound) interfaces.","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"# Adding update rule for the Score node\n@rule Score(:in, Marginalisation) (q_out::PointMass,) = begin     \n    return Bernoulli(mean(q_out))\nend","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/#Generative-Model-Specification","page":"Assessing People’s Skills","title":"Generative Model Specification","text":"","category":"section"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"We can now build the full generative model.","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"# GraphPPL.jl exports the `@model` macro for model specification\n# It accepts a regular Julia function and builds an FFG under the hood\n@model function skill_model()\n    s = randomvar(3)\n    t = randomvar(3)\n    r = datavar(Float64, 3)\n\n    # Priors\n    for i=1:3\n        s[i] ~ Bernoulli(0.5)\n    end\n\n    # Domain logic\n    t[1] ~ ¬s[1]\n    t[2] ~ t[1] || s[2]\n    t[3] ~ t[2] && s[3]\n    \n    # Results\n    for i=1:3\n        r[i] ~ Score(t[i])\n    end\nend","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/#Inference-Specification","page":"Assessing People’s Skills","title":"Inference Specification","text":"","category":"section"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"Let us assume that a student scored very low on all questions and set up and execute an inference algorithm.","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"test_results = [0.1, 0.1, 0.1]\ninference_result = infer(\n    model = skill_model(),\n    data  = (r = test_results, )\n)","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"Inference results:\n  Posteriors       | available for (s, t)","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/#Results","page":"Assessing People’s Skills","title":"Results","text":"","category":"section"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"# Inspect the results\nmap(params, inference_result.posteriors[:s])","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"3-element Vector{Tuple{Float64}}:\n (0.9872448979591837,)\n (0.06377551020408162,)\n (0.4719387755102041,)","category":"page"},{"location":"examples/advanced_examples/Assessing People Skills/","page":"Assessing People’s Skills","title":"Assessing People’s Skills","text":"These results suggest that this particular student was very likely out on the town last night.","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/#examples-predicting-bike-rental-demand","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"","category":"section"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"using RxInfer","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/#Preamble:-Enabling-Predictions","page":"Predicting Bike Rental Demand","title":"Preamble: Enabling Predictions","text":"","category":"section"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"RxInfer.jl facilitates predictions in two primary ways. ","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Implicit Prediction: By adding missing instances directly into the data, which are then treated as regular observations during inference. This method typically does not require model alterations.\nExplicit Prediction: By defining a separate data variable in the model. This approach doesn't necessitate passing missing instances as the data variable but does require specifying the predictvar argument in the inference function.","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/#Example","page":"Predicting Bike Rental Demand","title":"Example","text":"","category":"section"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Consider the following model:","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"@model function example_model()\n    y = datavar(Float64) where {allow_missing = true}\n\n    h ~ NormalMeanPrecision(0, 1.0)\n    x ~ NormalMeanPrecision(h, 1.0)\n    y ~ NormalMeanPrecision(x, 10.0)\nend","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# Implicit Prediction\nresult = infer(model = example_model(), data = (y = missing,))","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Inference results:\n  Posteriors       | available for (h, x)\n  Predictions      | available for (y)","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# Explicit Prediction\nresult = infer(model = example_model(), predictvars = (y = KeepLast(),))","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Inference results:\n  Posteriors       | available for (h, x)\n  Predictions      | available for (y)","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Both approaches yield the same results, but the choice depends on personal preferences and the model's structure. In scenarios with a clear distinction between observed and predicted variables, the explicit method is preferable. However, our subsequent example will not differentiate between observations and predictions, as it utilizes a state space representation.","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"using CSV, DataFrames, Plots","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/#Objective","page":"Predicting Bike Rental Demand","title":"Objective","text":"","category":"section"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"This example aims to simultaneously learn the dynamics of the feature space and predict hourly bike rental demand through reactive message passing, a signature approach of RxInfer.jl.","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/#Dataset-Source","page":"Predicting Bike Rental Demand","title":"Dataset Source","text":"","category":"section"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Data for this example study is sourced from the Kaggle Bike Count Prediction Dataset. For the purpose of this example, the original dataset from Kaggle has been adapted by removing categorical variables such as season, holiday, and working day. Additionally we take only 500 entries. This modification allows us to focus on modeling the continuous variables without additional complexities of handling categorical data. Nevertheless, this extension is feasible within RxInfer.jl.","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# Load the data\ndf = CSV.read(\"../data/bike_count/modified_bicycle.csv\", DataFrame)\ndf[1:10, :]","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"10×6 DataFrame\n Row │ datetime            temp     atemp    humidity  windspeed  count\n     │ String31            Float64  Float64  Float64   Float64    Int64\n─────┼──────────────────────────────────────────────────────────────────\n   1 │ 2011-01-01 0:00:00     9.84   14.395      81.0     0.0        16\n   2 │ 2011-01-01 1:00:00     9.02   13.635      80.0     0.0        40\n   3 │ 2011-01-01 2:00:00     9.02   13.635      80.0     0.0        32\n   4 │ 2011-01-01 3:00:00     9.84   14.395      75.0     0.0        13\n   5 │ 2011-01-01 4:00:00     9.84   14.395      75.0     0.0         1\n   6 │ 2011-01-01 5:00:00     9.84   12.88       75.0     6.0032      1\n   7 │ 2011-01-01 6:00:00     9.02   13.635      80.0     0.0         2\n   8 │ 2011-01-01 7:00:00     8.2    12.88       86.0     0.0         3\n   9 │ 2011-01-01 8:00:00     9.84   14.395      75.0     0.0         8\n  10 │ 2011-01-01 9:00:00    13.12   17.425      76.0     0.0        14","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# we reserve few samples for prediction\nn_future = 24\n\nX = Union{Vector{Float64}, Missing}[[row[i] for i in 2:(ncol(df))-1] for row in eachrow(df)][1:end-n_future]\ny = Union{Float64, Missing}[df[:, \"count\"]...][1:end-n_future]\n\nstate_dim = length(X[1]); # dimensionality of feature space","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/#Generative-Model-with-Priors","page":"Predicting Bike Rental Demand","title":"Generative Model with Priors","text":"","category":"section"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"We present a generative model that delineates the latent dynamics of feature evolution, represented by mathbfh_t, and their link to the bike rental counts, mathbfy_t.","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/#Equations-and-Priors","page":"Predicting Bike Rental Demand","title":"Equations and Priors","text":"","category":"section"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Feature Dynamics with Prior:\nPrior: mathbfa sim mathcalN(mathbf0 mathbfI)\nDynamics: mathbfh_t sim mathcalN(mathbfA h_t-1 mathbfQ)\nmathbfA\nis the transition matrix, reshaped from the prior vector mathbfa, and mathbfQ represents process noise.\nNoisy Observations:\nmathbfx_t sim mathcalN(mathbfh_t mathbfP)\nRepresents the observed noisy state of the features.\nCount Prediction with Prior:\nPrior: boldsymboltheta sim mathcalN(mathbf0 mathbfI)\nPrediction: y_t sim mathcalN(textsoftplus(boldsymboltheta^topmathbfh_t) sigma^2)\nModels the bike rental count as influenced by a non-linear transformation of the hidden state.","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/#Interpretation","page":"Predicting Bike Rental Demand","title":"Interpretation","text":"","category":"section"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"This framework aims to simultaneously infer the transition matrix mathbfA and the regression parameters boldsymboltheta, providing a comprehensive view of the feature space dynamics and the count prediction.\nBy employing Gaussian priors on both mathbfa and boldsymboltheta, we incorporate beliefs about their distributions.\nThe inference process aims to discover these underlying dynamics, enabling predictions of both features mathbfx_t and counts y_t.","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# We augument the dataset with missing entries for 24 hours ahead\nappend!(X, repeat([missing], n_future))\nappend!(y, repeat([missing], n_future));","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# Function to perform the state transition in the model.\n# It reshapes vector `a` into a matrix and multiplies it with vector `x` to simulate the transition.\nfunction transition(a, x)\n    nm, n = length(a), length(x)\n    m = nm ÷ n  # Calculate the number of rows for reshaping 'a' into a matrix\n    A = reshape(a, (m, n))  \n    return A * x\nend","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"transition (generic function with 1 method)","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# The dotsoftplus function combines a dot product and softplus transformation.\n# While useful for ensuring positivity, it may not be the optimal choice for all scenarios,\n# especially if the data suggests other forms of relationships or distributions.\nfunction dotsoftplus(a, x)\n    log(1 + exp(dot(a, x)))\nend","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"dotsoftplus (generic function with 1 method)","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# model definction\n@model function bicycle_ssm(n, h0, θ0, a0, Q, P)\n\n    # `h` is a sequence of hidden states\n    h = randomvar(n)\n\n    # `x` is a sequence of observed states (features)\n    x = datavar(Vector{Float64}, n) where {allow_missing = true}\n    # `y` is a sequence of \"count\" bicycles\n    y = datavar(Float64, n) where {allow_missing = true}\n\n    a ~ MvNormal(μ=a0, Σ=diageye(length(a0)))\n\n    θ ~ MvNormal(μ=mean(θ0), Σ=cov(θ0))\n\n    h_prior ~ MvNormal(μ=mean(h0), Σ=cov(h0))\n    h_prev = h_prior\n    \n    for i in 1:n\n        \n        h[i] ~ MvNormal(μ=transition(a, h_prev), Σ=Q)\n        \n        x[i] ~ MvNormal(μ=h[i], Σ=P)\n        \n        y[i] ~ Normal(μ=dotsoftplus(θ, h[i]), σ²=1.0)\n        \n        h_prev = h[i]\n    end\n\nend","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# In this example, we opt for a basic Linearization approach for the transition and dotsoftplus functions.\n# However, alternative methods like Unscented or CVI approximations can also be considered.\ntransition_meta = @meta begin \n    transition() -> Linearization()\n    dotsoftplus() -> Linearization()\nend","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Meta specification:\n  transition() -> Linearization()\n  dotsoftplus() -> Linearization()\nOptions:\n  warn = true","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# We define rather uninformative priors \nprior_θ = MvNormalMeanCovariance(ones(state_dim), diageye(state_dim))\nprior_h = MvNormalMeanCovariance(zeros(state_dim), 1e2diageye(state_dim))\nprior_a = MvNormalMeanCovariance(zeros(state_dim^2), diageye(state_dim^2))","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"MvNormalMeanCovariance(\nμ: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0\n.0, 0.0]\nΣ: [1.0 0.0 … 0.0 0.0; 0.0 1.0 … 0.0 0.0; … ; 0.0 0.0 … 1.0 0.0; 0.0 0.0 … \n0.0 1.0]\n)","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# the deterministic relationsships (transition) and (dotsoftplus) will induce loops in the graph representation of our model, this necessiates the initialization of the messages\nimessages = ( h = prior_h, a = prior_a, θ = prior_θ,)\n\nbicycle_model = bicycle_ssm(length(y), prior_h, prior_θ, prior_a, diageye(state_dim), diageye(state_dim))\n\nresult = infer(\n    model = bicycle_model,\n    data  = (y = y, x=X), \n    options = (limit_stack_depth = 500, ), \n    returnvars = KeepLast(),\n    initmessages = imessages,\n    meta = transition_meta,\n    iterations = 20,\n    showprogress=true,\n)","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"Inference results:\n  Posteriors       | available for (a, h, h_prior, θ)\n  Predictions      | available for (y, x)","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"# For a sake of this example, we extract only predictions\nmean_y, cov_y = mean.(result.predictions[:y]), cov.(result.predictions[:y])\nmean_x, cov_x = mean.(result.predictions[:x]), var.(result.predictions[:x])\n\nmean_x1, cov_x1 = getindex.(mean_x, 1), getindex.(cov_x, 1)\nmean_x2, cov_x2 = getindex.(mean_x, 2), getindex.(cov_x, 2)\nmean_x3, cov_x3 = getindex.(mean_x, 3), getindex.(cov_x, 3)\nmean_x4, cov_x4 = getindex.(mean_x, 4), getindex.(cov_x, 4);","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"slice = (300, length(y))\ndata = df[:, \"count\"][length(y)-n_future:length(y)]\n\np = scatter(y, \n            color=:darkblue, \n            markerstrokewidth=0,\n            label=\"Observed Count\", \n            alpha=0.6)\n\n# Plotting the mean prediction with variance ribbon\nplot!(mean_y, ribbon=sqrt.(cov_y), \n      color=:orange, \n      fillalpha=0.3,\n      label=\"Predicted Mean ± Std Dev\")\n\n# Adding a vertical line to indicate the start of the future prediction\nvline!([length(y)-n_future], \n       label=\"Prediction Start\", \n       linestyle=:dash, \n       linecolor=:green)\n\n# Future (unobserved) data\nplot!(length(y)-n_future:length(y), data, label=\"Future Count\")\n\n# Adjusting the limits\nxlims!(slice)\n\n# Enhancing the plot with titles and labels\ntitle!(\"Bike Rental Demand Prediction\")\nxlabel!(\"Time\")\nylabel!(\"Bike Count\")\n\n# Adjust the legend\nlegend=:topright\n\n# Show the final plot\ndisplay(p)","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"using Plots\n\n# Define a color palette\npalette = cgrad(:viridis)\n\n# Plot the hidden states with observations\np1 = plot(mean_x1, ribbon=sqrt.(cov_x1), color=palette[1], label=\"Hidden State 1\", legend=:topleft)\nplot!(df[!, :temp], color=:grey, label=\"Temperature\")\nvline!([length(y)-n_future], linestyle=:dash, color=:red, label=\"Prediction Start\")\nxlabel!(\"Time\")\nylabel!(\"Value\")\ntitle!(\"Temperature vs Hidden State 1\")\n\np2 = plot(mean_x2, ribbon=sqrt.(cov_x2), color=palette[2], label=\"Hidden State 2\", legend=:topleft)\nplot!(df[!, :atemp], color=:grey, label=\"Feels-Like Temp\")\nvline!([length(y)-n_future], linestyle=:dash, color=:red, label=\"\")\nxlabel!(\"Time\")\nylabel!(\"Value\")\ntitle!(\"Feels-Like Temp vs Hidden State 2\")\n\np3 = plot(mean_x3, ribbon=sqrt.(cov_x3), color=palette[3], label=\"Hidden State 3\", legend=:topleft)\nplot!(df[!, :humidity], color=:grey, label=\"Humidity\")\nvline!([length(y)-n_future], linestyle=:dash, color=:red, label=\"Prediction Start\")\nxlabel!(\"Time\")\nylabel!(\"Value\")\ntitle!(\"Humidity vs Hidden State 3\")\n\np4 = plot(mean_x4, ribbon=sqrt.(cov_x4), color=palette[4], label=\"Hidden State 4\", legend=:topleft)\nplot!(df[!, :windspeed], color=:grey, label=\"Windspeed\")\nvline!([length(y)-n_future], linestyle=:dash, color=:red, label=\"Prediction Start\")\nxlabel!(\"Time\")\nylabel!(\"Value\")\ntitle!(\"Windspeed vs Hidden State 4\")\n\nfor p in [p1, p2, p3, p4]\n    xlims!(p, first(slice), last(slice))\nend\n\nplot(p1, p2, p3, p4, layout=(2, 2), size=(800, 400))","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/#Conclusion","page":"Predicting Bike Rental Demand","title":"Conclusion","text":"","category":"section"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"While the predictions provided by our current model may not align closely with the real-world data, it's crucial to acknowledge the assumptions and simplifications that were made which could have influenced the outcomes. This model serves as a conceptual framework, demonstrating that it's possible to infer states, parameters, and predictions concurrently with a focus on the predictive aspect of the analysis.","category":"page"},{"location":"examples/basic_examples/Predicting Bike Rental Demand/","page":"Predicting Bike Rental Demand","title":"Predicting Bike Rental Demand","text":"The real power of modeling lies in iteration and refinement. We encourage the community to build upon this foundation. Through collaborative effort and shared insights, we can enhance the predictive accuracy of such models, making them not only illustrative but also practical for real-world applications.","category":"page"},{"location":"manuals/comparison/#comparison","page":"RxInfer.jl vs. Others","title":"Comparison to other packages","text":"","category":"section"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"Nowadays there's plenty of probabilistic programming languages and packages available. While all of them are based on Bayesian Inference, their methodologies vary. This section compares RxInfer.jl against other renowned probabilistic programming languages and packages. The goal is to enlighten potential users about the nuances and guide them in choosing the package that best suits their requirements.","category":"page"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"DISCLAIMER: ","category":"page"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"This comparison isn't exhaustive and mirrors the author's hands-on experience with the packages. Some might have undergone more rigorous testing than others. If you're an author of one of these packages and believe the comparison doesn't do justice, please reach out, and we'll be more than willing to rectify.\nThe comparison is more qualitative than quantitative, considering the intricacies of upkeeping benchmarking code for perpetually evolving packages.","category":"page"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"Toolbox Universality Efficiency Expressiveness Debugging & Visualization Modularity Inference Engine Language Community & Ecosystem\nRxInfer.jl ~ ✓ ✓ ~ ✗ Message-passing Julia ✗\nForneyLab.jl ✗ ~ ✗ ~ ✗ Message-passing Julia ✗\nInfer.net ~ ✓ ✗ ✓ ✗ Message-passing C# ✗\nPGMax ✗ ✓ ✗ ✓ ✗ Message-passing Python ✗\nTuring.jl ✓ ✗ ✓ ~ ✗ Sampling Julia ✓\nPyMC ✓ ✗ ✓ ✓ ✗ Sampling Python ✓\nNumPyro ✓ ✓ ~ ✓ ✗ Sampling Python ✓\nTensorFlow Probability ✓ ✗ ~ ✓ ✗ Sampling Python ✓\nStan ✓ ✗ ✓ ✓ ✗ Sampling Stan ✓","category":"page"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"(Date of creation: 20/10/2023)","category":"page"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"Legend","category":"page"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"✓ : Full capability or feature is present.\n~ : Partial capability or feature is present.\n✗ : No capability or feature.","category":"page"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"Notes:","category":"page"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"Universality: Denotes the capability to depict a vast array of probabilistic models.\nEfficiency: Highlights computational competence. A \"–\" in this context suggests perceived slowness.\nExpressiveness: Assesses the ability to concisely formulate intricate probabilistic models.\nDebugging & Visualization: Evaluates the suite of tools for model debugging and visualization.\nModularity: Reflects the potential to create models by integrating smaller models.\nInference Engines: Pinpoints the primary inference strategy employed by the toolbox.\nLanguage: Identifies the programming language integral to the toolbox.\nCommunity & Ecosystem: Signifies the vibrancy of the ecosystem, inclusive of tools, libraries, and community backing.","category":"page"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"","category":"page"},{"location":"manuals/comparison/#RxInfer.jl-breakdown","page":"RxInfer.jl vs. Others","title":"RxInfer.jl breakdown","text":"","category":"section"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"Universality: RxInfer.jl shines in formulating intricate models derived from the exponential family distributions. The package encompasses not only commonly used distributions such as Gaussian or Bernoulli, but also specialized stochastic nodes that represents prevalent probabilistic models like Autoregressive models, Gamma Mixture models, among others. Furthermore, RxInfer.jl proficiently manages deterministic transformations of variables from the exponential family, see Delta node. Nevertheless, for models outside the exponential family, RxInfer.jl might not be the good choice. Such models would require the creation of novel nodes and corresponding rules, as illustrated in this section.\nEfficiency: RxInfer.jl distinguishes itself with its inference engine rooted in reactive message passing. This approach is supremely efficient, facilitating real-time propagation of updates across the system, supporting parallelization, interruptibility, and more. However, the current version of RxInfer.jl hasn't harnessed all these potentials.\nModularity: Broadly, the toolboxes in the table aren't modular in the truest sense. They don't offer the fusion of models by integrating smaller models. While RxInfer.jl currently doesn't support this, a solution is on the horizon:","category":"page"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"@model function inner_inner(τ, y)\n    y ~ Normal(τ[1], τ[2])\nend\n\n@model function inner(θ, α)\n    β ~ Normal(0, 1)\n    α ~ Gamma(β, 1)\n    α ~ inner_inner(τ = θ)\nend\n\n@model function outer()\n    local w\n    for i = 1:5\n        w[i] ~ inner(θ = Gamma(1, 1))\n    end\n    y ~ inner(θ = w[2:3])\nend","category":"page"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"Expressiveness: RxInfer.jl empowers users to elegantly and concisely craft models, closely mirroring probabilistic notation, thanks to Julia's macro capabilities. To illustrate this, let's consider the following model:","category":"page"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"beginaligned\n x  sim mathrmNormal(00 10)\n w  sim mathrmInverseGamma(10 10)\n y  sim mathrmNormal(x w)\nendaligned","category":"page"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"The model then is expressed in RxInfer.jl as follows:","category":"page"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"using RxInfer\n\n@model function example_model()\n    x ~ Normal(mean=0.0, var=1.0)\n    w ~ InverseGamma(α = 1, θ = 1)\n    y ~ Normal(mean = x, var = w)\nend","category":"page"},{"location":"manuals/comparison/","page":"RxInfer.jl vs. Others","title":"RxInfer.jl vs. Others","text":"Debugging & Visualization: While RxInfer.jl struggles with Julia's early-stage debugging system, it does provide a mechanism to debug the inference procedure, even though not as seamlessly as some other packages.","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/#examples-infinite-data-stream","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"","category":"section"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"This example shows the capabilities of RxInfer to perform Bayesian inference on real-time signals. As usual, first, we start with importing necessary packages:","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"using RxInfer, Plots, Random, StableRNGs","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"For demonstration purposes we will create a synthetic environment that has a hidden underlying signal, which we cannot observer directly. Instead, we will observe a noised realisation of this hidden signal:","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"mutable struct Environment\n    rng                   :: AbstractRNG\n    current_state         :: Float64\n    observation_precision :: Float64\n    history               :: Vector{Float64}\n    observations          :: Vector{Float64}\n    \n    Environment(current_state, observation_precision; seed = 123) = begin \n         return new(StableRNG(seed), current_state, observation_precision, [], [])\n    end\nend\n\nfunction getnext!(environment::Environment)\n    environment.current_state = environment.current_state + 1.0\n    nextstate  = 10sin(0.1 * environment.current_state)\n    observation = rand(NormalMeanPrecision(nextstate, environment.observation_precision))\n    push!(environment.history, nextstate)\n    push!(environment.observations, observation)\n    return observation\nend\n\nfunction gethistory(environment::Environment)\n    return environment.history\nend\n\nfunction getobservations(environment::Environment)\n    return environment.observations\nend","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"getobservations (generic function with 1 method)","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/#Model-specification","page":"Infinite Data Stream","title":"Model specification","text":"","category":"section"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"We assume that we don't know the shape of our signal in advance. So we try to fit a simple gaussian random walk with unknown observation noise:","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"@model function kalman_filter()\n    \n    # Prior for the previous state\n    x_prev_mean = datavar(Float64)\n    x_prev_var  = datavar(Float64)\n    \n    x_prev ~ Normal(mean = x_prev_mean, variance = x_prev_var)\n    \n    # Prior for the observation noise\n    τ_shape = datavar(Float64)\n    τ_rate  = datavar(Float64)\n    \n    τ ~ Gamma(shape = τ_shape, rate = τ_rate)\n    \n    # Random walk with fixed precision\n    x_current ~ Normal(mean = x_prev, precision = 1.0)\n    \n    # Noisy observation\n    y = datavar(Float64)\n    y ~ Normal(mean = x_current, precision = τ)\n    \nend\n\n# We assume the following factorisation between variables \n# in the variational distribution\n@constraints function filter_constraints()\n    q(x_prev, x_current, τ) = q(x_prev, x_current)q(τ)\nend","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"filter_constraints (generic function with 1 method)","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/#Prepare-environment","page":"Infinite Data Stream","title":"Prepare environment","text":"","category":"section"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"initial_state         = 0.0\nobservation_precision = 0.1","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"0.1","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"After we have created the environment we can observe how our signal behaves:","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"testenvironment = Environment(initial_state, observation_precision);\n\nanimation = @animate for i in 1:100\n    getnext!(testenvironment)\n    \n    history = gethistory(testenvironment)\n    observations = getobservations(testenvironment)\n    \n    p = plot(size = (1000, 300))\n    \n    p = plot!(p, 1:i, history[1:i], label = \"Hidden signal\")\n    p = scatter!(p, 1:i, observations[1:i], ms = 4, alpha = 0.7, label = \"Observation\")\nend\n\ngif(animation, \"../pics/infinite-data-stream.gif\", fps = 24, show_msg = false);","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/#Filtering-on-static-dataset","page":"Infinite Data Stream","title":"Filtering on static dataset","text":"","category":"section"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"RxInfer is flexible and allows for running inference both on real-time and static datasets. In the next section we show how to perform the filtering procedure on a static dataset. We also will verify our inference procedure by checking on the Bethe Free Energy values:","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"n                  = 300\nstatic_environment = Environment(initial_state, observation_precision);\n\nfor i in 1:n\n    getnext!(static_environment)\nend\n\nstatic_history      = gethistory(static_environment)\nstatic_observations = getobservations(static_environment);\nstatic_datastream   = from(static_observations) |> map(NamedTuple{(:y,), Tuple{Float64}}, (d) -> (y = d, ));","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"function run_static(environment, datastream)\n    \n    # `@autoupdates` structure specifies how to update our priors based on new posteriors\n    # For example, every time we have updated a posterior over `x_current` we update our priors\n    # over `x_prev`\n    autoupdates = @autoupdates begin \n        x_prev_mean, x_prev_var = mean_var(q(x_current))\n        τ_shape = shape(q(τ))\n        τ_rate = rate(q(τ))\n    end\n    \n    engine = infer(\n        model         = kalman_filter(),\n        constraints   = filter_constraints(),\n        datastream    = datastream,\n        autoupdates   = autoupdates,\n        returnvars    = (:x_current, ),\n        keephistory   = 10_000,\n        historyvars   = (x_current = KeepLast(), τ = KeepLast()),\n        initmarginals = (x_current = NormalMeanVariance(0.0, 1e3), τ = GammaShapeRate(1.0, 1.0)),\n        iterations    = 10,\n        free_energy   = true,\n        autostart     = true,\n    )\n    \n    return engine\nend","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"run_static (generic function with 1 method)","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"result = run_static(static_environment, static_datastream);","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"static_inference = @animate for i in 1:n\n    estimated = result.history[:x_current]\n    p = plot(1:i, mean.(estimated[1:i]), ribbon = var.(estimated[1:n]), label = \"Estimation\")\n    p = plot!(static_history[1:i], label = \"Real states\")    \n    p = scatter!(static_observations[1:i], ms = 2, label = \"Observations\")\n    p = plot(p, size = (1000, 300), legend = :bottomright)\nend\n\ngif(static_inference, \"../pics/infinite-data-stream-inference.gif\", fps = 24, show_msg = false);","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"plot(result.free_energy_history, label = \"Bethe Free Energy (averaged)\")","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/#Filtering-on-realtime-dataset","page":"Infinite Data Stream","title":"Filtering on realtime dataset","text":"","category":"section"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"Next lets create a \"real\" infinite stream. We use timer() observable from Rocket.jlto emulate real-world scenario. In our example we are going to generate a new data point every ~41ms (24 data points per second). For demonstration purposes we force stop after n data points, but there is no principled limitation to run inference indefinite:","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"function run_and_plot(environment, datastream)\n    \n    # `@autoupdates` structure specifies how to update our priors based on new posteriors\n    # For example, every time we have updated a posterior over `x_current` we update our priors\n    # over `x_prev`\n    autoupdates = @autoupdates begin \n        x_prev_mean, x_prev_var = mean_var(q(x_current))\n        τ_shape = shape(q(τ))\n        τ_rate = rate(q(τ))\n    end\n    \n    posteriors = []\n    \n    plotfn = (q_current) -> begin \n        IJulia.clear_output(true)\n        \n        push!(posteriors, q_current)\n\n        p = plot(mean.(posteriors), ribbon = var.(posteriors), label = \"Estimation\")\n        p = plot!(gethistory(environment), label = \"Real states\")    \n        p = scatter!(getobservations(environment), ms = 2, label = \"Observations\")\n        p = plot(p, size = (1000, 300), legend = :bottomright)\n\n        display(p)\n    end\n    \n    engine = infer(\n        model         = kalman_filter(),\n        constraints   = filter_constraints(),\n        datastream    = datastream,\n        autoupdates   = autoupdates,\n        returnvars    = (:x_current, ),\n        initmarginals = (x_current = NormalMeanVariance(0.0, 1e3), τ = GammaShapeRate(1.0, 1.0)),\n        iterations    = 10,\n        autostart     = false,\n    )\n    \n    qsubscription = subscribe!(engine.posteriors[:x_current], plotfn)\n    \n    RxInfer.start(engine)\n    \n    return engine\nend","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"run_and_plot (generic function with 1 method)","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"# This example runs in our documentation pipeline, which does not support \"real-time\" execution context\n# We skip this code if run not in Jupyter notebook (see below an example with gif)\nengine = nothing \nif isdefined(Main, :IJulia)\n    timegen      = 41 # 41 ms\n    environment  = Environment(initial_state, observation_precision);\n    observations = timer(timegen, timegen) |> map(Float64, (_) -> getnext!(environment)) |> take(n) # `take!` automatically stops after `n` observations\n    datastream   = observations |> map(NamedTuple{(:y,), Tuple{Float64}}, (d) -> (y = d, ));\n    engine = run_and_plot(environment, datastream)\nend;","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"The plot above is fully interactive and we can stop and unsubscribe from our datastream before it ends:","category":"page"},{"location":"examples/advanced_examples/Infinite Data Stream/","page":"Infinite Data Stream","title":"Infinite Data Stream","text":"if !isnothing(engine) && isdefined(Main, :IJulia)\n    RxInfer.stop(engine)\n    IJulia.clear_output(true)\nend;","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/#examples-universal-mixtures","page":"Universal Mixtures","title":"Universal Mixtures","text":"","category":"section"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"using RxInfer, Distributions, Random, Plots","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"John and Jane are having a coin toss competition. Before they start, they both have the feeling that something is not right. The coin is unbalanced and favors one side over the other. However, both John and Jane do not know which side is being favored. John thinks that the coin favors heads and Jane thinks tails. Coincidentally, both John and Jane have a strong mathematics background and are aware of the appropriate likelihood function for this experiment","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(y_i mid theta) = mathrmBer(y_i mid theta)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"where y_i in 01 are the outcomes of the coin tosses, i.e. heads or tails, and where theta is the coin parameter. They express their gut feeling about the fairness of the coin in terms of a prior distribution over the coin parameter theta, which represents the probability of the coin landing on heads. Based on their gut feeling and the support of thetain01 they come up with the prior beliefs:","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(theta mid textJohn) = mathrmBeta(theta mid 7 2)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(theta mid textJane) = mathrmBeta(theta mid 2 7)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"rθ = range(0, 1, length = 1000)\np = plot(title = \"prior beliefs\")\nplot!(rθ, (x) -> pdf(Beta(7.0, 2.0), x), fillalpha=0.3, fillrange = 0, label=\"P(θ) John\", c=1)\nplot!(rθ, (x) -> pdf(Beta(2.0, 7.0), x), fillalpha=0.3, fillrange = 0, label=\"p(θ) Jane\", c=3,)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"John and Jane really want to clear the odds and decide to perform a lengthy experiment. They toss the unbalanced coin N = 10 times, because their favorite TV show is cancelled anyway and therefore they have plenty of time. ","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"true_coin = Bernoulli(0.25)\nnr_throws = 10\ndataset = Int.(rand(MersenneTwister(42), true_coin, nr_throws))\nnr_heads, nr_tails = sum(dataset), nr_throws-sum(dataset)\nprintln(\"experimental outcome: \\n - heads: \", nr_heads, \"\\n - tails: \", nr_tails);","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"experimental outcome: \n - heads: 3\n - tails: 7","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"For computing the posterior beliefs p(theta mid y) about the parameter theta, they will perform probabilistic inference in the model based on Bayes' rule. Luckily everything is tractable and therefore they can resort to exact inference. They decide to outsource these tedious computations using RxInfer.jl and specify the following models:","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/#John's-model:","page":"Universal Mixtures","title":"John's model:","text":"","category":"section"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(y theta mid textJohn) = p(theta mid textJohn) prod_i=1^N p(y_i mid theta)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"@model function beta_model_john(nr_throws)\n\n    # specify experimental outcomes\n    y = datavar(Int64, nr_throws)\n\n    # specify John's prior model over θ\n    θ ~ Beta(7.0, 2.0)\n\n    # create likelihood models\n    for i in 1:nr_throws\n        y[i] ~ Bernoulli(θ)\n    end\n\n    return y, θ\n    \nend","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/#Jane's-model:","page":"Universal Mixtures","title":"Jane's model:","text":"","category":"section"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(y theta mid textJane) = p(theta mid textJane) prod_i=1^N p(y_i mid theta)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"@model function beta_model_jane(nr_throws)\n\n    # specify experimental outcomes\n    y = datavar(Int64, nr_throws)\n\n    # specify Jane's prior model over θ\n    θ ~ Beta(2.0, 7.0)\n\n    # create likelihood models\n    for i in 1:nr_throws\n        y[i] ~ Bernoulli(θ)\n    end\n\n    return y, θ\n    \nend","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"Now it is time to figure out whose prior belief was the best and who was actually right. They perform probabilistic inference automatically and compute the Bethe free energy to compare eachothers models. For acyclic models, the Bethe free energy mathrmF_B bounds the model evidence p(y) as ","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"mathrmF_Bpq geq - ln p(y)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"result_john = infer(\n    model = beta_model_john(nr_throws), \n    data  = (y = dataset, ),\n    free_energy = true,\n)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"Inference results:\n  Posteriors       | available for (θ)\n  Free Energy:     | Real[8.96366]","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"result_jane = infer(\n    model = beta_model_jane(nr_throws), \n    data  = (y = dataset, ),\n    free_energy = true\n)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"Inference results:\n  Posteriors       | available for (θ)\n  Free Energy:     | Real[6.63988]","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"From these results, they agree that Jane her gut feeling was right all along, as her Bethe free energy is lower and therefore her model evidence is higher. Nonetheless, after the 10 throws, they now have a better idea about the underlying theta parameter. They formulate this through the posterior distributions p(theta mid y textJohn) and p(theta mid y textJane):","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"rθ = range(0, 1, length = 1000)\np = plot(title = \"posterior beliefs\")\nplot!(rθ, (x) -> pdf(result_john.posteriors[:θ], x), fillalpha=0.3, fillrange = 0, label=\"P(θ|y) John\", c=1)\nplot!(rθ, (x) -> pdf(result_jane.posteriors[:θ], x), fillalpha=0.3, fillrange = 0, label=\"p(θ|y) Jane\", c=3,)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"What John and Jane did not know, was that Mary, their neighbour, was overhearing their conversation. She was also curious, but could not see the coin. She did not really know how to formulate a prior distribution over theta, so instead she combined both John and Jane their prior beliefs. She had the feeling that John his assessment was more correct, as he was often going to the casino. As a result, she mixed the prior beliefs of John and Jane with proportions 0.7 and 0.3, respectively. Her model for the environment is specified as","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(y theta c mid textMary) = p(c mid textMary)  p(theta mid textJohn)^c p(theta mid textJane)^1-c prod_i=1^N p(y_i mid theta)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"where c describes the probability of John being correct as ","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(c mid textMary) = mathrmBer(c mid 07)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"The predictive distribution p(theta mid textMary) for theta (similar to the prior beliefs of John and Jane) she obtained from the marginalisation over c as","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"p(theta mid textMary) = sum_cin01 p(cmidtextMary) p(theta mid textJohn)^c p(theta mid textJane)^1-c = 07 cdot p(theta mid textJohn) + 03 cdot p(theta mid textJane)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"rθ = range(0, 1, length = 1000)\np = plot(title = \"prior belief\")\nplot!(rθ, (x) -> pdf(MixtureDistribution([Beta(2.0, 7.0), Beta(7.0, 2.0)], [ 0.3, 0.7 ]), x), fillalpha=0.3, fillrange = 0, label=\"P(θ) Mary\", c=1)\nplot!(rθ, (x) -> 0.7*pdf(Beta(7.0, 2.0), x), c=3, label=\"\")\nplot!(rθ, (x) -> 0.3*pdf(Beta(2.0, 7.0), x), c=3, label=\"\")","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"She was also interested in the results and used the new Mixture node and addons in ReactiveMP.jl. She specified her model as follows and performed inference in this model:","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"@model function beta_model_mary(nr_throws)\n\n    # specify experimental outcomes\n    y = datavar(Int64, nr_throws)\n\n    # specify John's and Jane's prior models over θ\n    θ_jane ~ Beta(2.0, 7.0)\n    θ_john ~ Beta(7.0, 2.0)\n\n    # specify initial guess as to who is right\n    john_is_right ~ Bernoulli(0.7) \n\n    # specify mixture prior Distribution\n    θ ~ Mixture(john_is_right, (θ_jane, θ_john))\n\n    # create likelihood models\n    for i in 1:nr_throws\n        y[i] ~ Bernoulli(θ)\n    end\n\n    return y, θ\n    \nend","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"This Mixture node updates the belief over c on the performance of the individual models of both John and Jane using so-called scale factors, as introduced in Nguyen et al.. The specific update rules for this node can be found here.","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"result_mary = infer(\n    model = beta_model_mary(nr_throws), \n    data  = (y = dataset, ),\n    returnvars = (θ = KeepLast(), θ_john = KeepLast(), θ_jane = KeepLast(), john_is_right = KeepLast()),\n    addons = AddonLogScale(),\n    postprocess = UnpackMarginalPostprocess(),\n)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"Inference results:\n  Posteriors       | available for (john_is_right, θ_john, θ, θ_jane)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"Mary was happy, with her mixture prior, she beat John in terms of performance. However, it was not the best decision to think that John was right. In fact, after the experiment there was only a minor possibility remaining that John was right. Her posterior distribution over theta also changed, and as expected the estimate from Jane was more prominent.","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"rθ = range(0, 1, length = 1000)\np = plot(title = \"posterior belief\")\nplot!(rθ, (x) -> pdf(result_mary.posteriors[:θ], x), fillalpha=0.3, fillrange = 0, label=\"P(θ|y) Mary\", c=1)\nplot!(rθ, (x) -> result_mary.posteriors[:θ].weights[1] * pdf(component(result_mary.posteriors[:θ], 1), x), label=\"\", c=3)\nplot!(rθ, (x) -> result_mary.posteriors[:θ].weights[2] * pdf(component(result_mary.posteriors[:θ], 2), x), label=\"\", c=3)","category":"page"},{"location":"examples/problem_specific/Universal Mixtures/","page":"Universal Mixtures","title":"Universal Mixtures","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/overview/#examples-basic_examples-overview","page":"Overview","title":"Basic examples","text":"","category":"section"},{"location":"examples/basic_examples/overview/","page":"Overview","title":"Overview","text":"This section contains a set of examples for Bayesian Inference with RxInfer package in various probabilistic models.","category":"page"},{"location":"examples/basic_examples/overview/","page":"Overview","title":"Overview","text":"note: Note\nAll examples have been pre-generated automatically from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/basic_examples/overview/","page":"Overview","title":"Overview","text":"Basic examples contain \"Hello World!\" of Bayesian inference in RxInfer.","category":"page"},{"location":"examples/basic_examples/overview/","page":"Overview","title":"Overview","text":"Coin toss model (Beta-Bernoulli): An example of Bayesian inference in Beta-Bernoulli model with IID observations.\nBayesian Linear Regression Tutorial: An extensive tutorial on Bayesian linear regression with RxInfer with a lot of examples, including multivariate and hierarchical linear regression.\nKalman filtering and smoothing: In this demo, we are interested in Bayesian state estimation in different types of State-Space Models, including linear, nonlinear, and cases with missing observations\nPredicting Bike Rental Demand: An illustrative guide to implementing prediction mechanisms within RxInfer.jl, using bike rental demand forecasting as a contextual example.\nHow to train your Hidden Markov Model: An example of structured variational Bayesian inference in Hidden Markov Model with unknown transition and observational matrices.","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/#examples-global-parameter-optimisation","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"","category":"section"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"This notebook demonstrates how to optimize parameters in state space models using external optimization packages, such as Optim.jl and Flux.jl. We utilize RxInfer.jl, a powerful package for inference in probabilistic models.","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"By the end of this notebook, you will have practical knowledge of global parameter optimization in state space models. You will learn how to optimize parameters in both univariate and multivariate state space models, and harness the power of external optimization packages such as Optim.jl and Flux.jl.","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/#Univariate-State-Space-Model","page":"Global Parameter Optimisation","title":"Univariate State Space Model","text":"","category":"section"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Let us try use the following simple state space model:","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"beginaligned\n    x_t = x_t-1 + c \n    y_t sim mathcalNleft(x_t p right) \nendaligned","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"with prior x_0 sim mathcalN(m_x_0 v_x_0). Our goal is to optimize parameters c and m_x_0.","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"using RxInfer, BenchmarkTools, Random, LinearAlgebra, Plots","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"@model function smoothing(n, x0, c::ConstVariable, P::ConstVariable)\n    \n    x_prior ~ NormalMeanVariance(mean(x0), cov(x0)) \n\n    x = randomvar(n)\n    y = datavar(Float64, n)\n\n    x_prev = x_prior\n\n    for i in 1:n\n        x[i] ~ x_prev + c\n        y[i] ~ NormalMeanVariance(x[i], P)\n        \n        x_prev = x[i]\n    end\n\n    return x, y\nend","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"rng = MersenneTwister(42)\n\nP      = 1.0\nn      = 250\nc_real = -5.0\ndata   = c_real .+ collect(1:n) + rand(rng, Normal(0.0, sqrt(P)), n);","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# c[1] is C\n# c[2] is μ0\nfunction f(c)\n    x0_prior = NormalMeanVariance(c[2], 100.0)\n    result = infer(\n        model = smoothing(n, x0_prior, c[1], P), \n        data  = (y = data,), \n        free_energy = true\n    )\n    return result.free_energy[end]\nend","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"f (generic function with 1 method)","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"using Optim","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"res = optimize(f, ones(2), GradientDescent(), Optim.Options(g_tol = 1e-3, iterations = 100, store_trace = true, show_trace = true, show_every = 10))","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Iter     Function value   Gradient norm \n     0     3.651509e+02     1.001412e+03\n * time: 0.023398876190185547\n * Status: success\n\n * Candidate solution\n    Final objective value:     3.645772e+02\n\n * Found with\n    Algorithm:     Gradient Descent\n\n * Convergence measures\n    |x - x'|               = 1.18e-06 ≰ 0.0e+00\n    |x - x'|/|x'|          = 2.30e-07 ≰ 0.0e+00\n    |f(x) - f(x')|         = 9.13e-07 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 2.51e-09 ≰ 0.0e+00\n    |g(x)|                 = 2.56e-06 ≤ 1.0e-03\n\n * Work counters\n    Seconds run:   6  (vs limit Inf)\n    Iterations:    9\n    f(x) calls:    67\n    ∇f(x) calls:   67","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"res.minimizer # Real values are indeed (c = 1.0 and μ0 = -5.0)","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"2-element Vector{Float64}:\n  1.0007749243942134\n -5.14368311177876","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"println(\"Real value vs Optimized\")\nprintln(\"Real:      \", [ 1.0, c_real ])\nprintln(\"Optimized: \", res.minimizer)","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Real value vs Optimized\nReal:      [1.0, -5.0]\nOptimized: [1.0007749243942134, -5.14368311177876]","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/#Multivariate-state-space-model","page":"Global Parameter Optimisation","title":"Multivariate state space model","text":"","category":"section"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Let us consider the multivariate state space model:","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"beginaligned\n    mathbfx_t sim mathcalNleft(mathbfAx_t-1 mathbfQ right) \n    mathbfy_t sim mathcalNleft(mathbfx_t mathbfP right) \nendaligned","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"with prior ","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"beginaligned\nmathbfx_0 sim mathcalN(mathbfm_x_0 mathbfV_x_0)\nendaligned","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"and transition matrix ","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"beginaligned\nmathbfA = beginbmatrix costheta  -sintheta  sintheta  costheta endbmatrix\nendaligned","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Covariance matrices mathbfV_x_0, mathbfP and mathbfQ are known. Our goal is to optimize parameters mathbfm_x_0 and theta.","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"using RxInfer, BenchmarkTools, Random, LinearAlgebra, Plots","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"@model function rotate_ssm(n, θ, x0, Q::ConstVariable, P::ConstVariable)\n    \n    x = randomvar(n)\n    y = datavar(Vector{Float64}, n)\n    \n    x_prior ~ MvNormalMeanCovariance(mean(x0), cov(x0))\n    \n    x_prev = x_prior\n    \n    A = constvar([ cos(θ) -sin(θ); sin(θ) cos(θ) ])\n    \n    for i in 1:n\n        x[i] ~ MvNormalMeanCovariance(A * x_prev, Q)\n        y[i] ~ MvNormalMeanCovariance(x[i], P)\n        \n        x_prev = x[i]\n    end\n    \nend","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# Generate data\nfunction generate_rotate_ssm_data()\n    rng = MersenneTwister(1234)\n\n    θ = π / 8\n    A = [ cos(θ) -sin(θ); sin(θ) cos(θ) ]\n    Q = Matrix(Diagonal(1.0 * ones(2)))\n    P = Matrix(Diagonal(1.0 * ones(2)))\n\n    n = 300\n\n    x_prev = [ 10.0, -10.0 ]\n\n    x = Vector{Vector{Float64}}(undef, n)\n    y = Vector{Vector{Float64}}(undef, n)\n\n    for i in 1:n\n        \n        x[i] = rand(rng, MvNormal(A * x_prev, Q))\n        y[i] = rand(rng, MvNormal(x[i], Q))\n        \n        x_prev = x[i]\n    end\n\n    return θ, A, Q, P, n, x, y\nend","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"generate_rotate_ssm_data (generic function with 1 method)","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"θ, A, Q, P, n, x, y = generate_rotate_ssm_data();","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"px = plot()\n\npx = plot!(px, getindex.(x, 1), ribbon = diag(Q)[1] .|> sqrt, fillalpha = 0.2, label = \"real₁\")\npx = plot!(px, getindex.(x, 2), ribbon = diag(Q)[2] .|> sqrt, fillalpha = 0.2, label = \"real₂\")\n\nplot(px, size = (1200, 450))","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"function f(θ)\n    x0 = MvNormalMeanCovariance([ θ[2], θ[3] ], Matrix(Diagonal(0.01 * ones(2))))\n    result = infer(\n        model = rotate_ssm(n, θ[1], x0, Q, P), \n        data  = (y = y,), \n        free_energy = true\n    )\n    return result.free_energy[end]\nend","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"f (generic function with 1 method)","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"res = optimize(f, zeros(3), LBFGS(), Optim.Options(f_tol = 1e-14, g_tol = 1e-12, show_trace = true, show_every = 10))","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Iter     Function value   Gradient norm \n     0     2.192003e+04     9.032537e+04\n * time: 7.009506225585938e-5\n * Status: success\n\n * Candidate solution\n    Final objective value:     1.161372e+03\n\n * Found with\n    Algorithm:     L-BFGS\n\n * Convergence measures\n    |x - x'|               = 5.09e-09 ≰ 0.0e+00\n    |x - x'|/|x'|          = 4.48e-10 ≰ 0.0e+00\n    |f(x) - f(x')|         = 0.00e+00 ≤ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 0.00e+00 ≤ 1.0e-14\n    |g(x)|                 = 5.63e-07 ≰ 1.0e-12\n\n * Work counters\n    Seconds run:   18  (vs limit Inf)\n    Iterations:    9\n    f(x) calls:    58\n    ∇f(x) calls:   58","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"println(\"Real value vs Optimized\")\nprintln(\"Real:      \", θ)\nprintln(\"Optimized: \", res.minimizer[1])\n\n@show sin(θ), sin(res.minimizer[1])\n@show cos(θ), cos(res.minimizer[1])","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Real value vs Optimized\nReal:      0.39269908169872414\nOptimized: 0.3929332481380117\n(sin(θ), sin(res.minimizer[1])) = (0.3826834323650898, 0.3828997634515549)\n(cos(θ), cos(res.minimizer[1])) = (0.9238795325112867, 0.9237898955654058)\n(0.9238795325112867, 0.9237898955654058)","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"x0 = MvNormalMeanCovariance([ res.minimizer[2], res.minimizer[3] ], Matrix(Diagonal(100.0 * ones(2))))\n\nresult = infer(\n    model = rotate_ssm(n, res.minimizer[1], x0, Q, P), \n    data  = (y = y,), \n    free_energy = true\n)\n\nxmarginals = result.posteriors[:x]\n\npx = plot()\n\npx = plot!(px, getindex.(x, 1), ribbon = diag(Q)[1] .|> sqrt, fillalpha = 0.2, label = \"real₁\")\npx = plot!(px, getindex.(x, 2), ribbon = diag(Q)[2] .|> sqrt, fillalpha = 0.2, label = \"real₂\")\npx = plot!(px, getindex.(mean.(xmarginals), 1), ribbon = getindex.(var.(xmarginals), 1) .|> sqrt, fillalpha = 0.5, label = \"inf₁\")\npx = plot!(px, getindex.(mean.(xmarginals), 2), ribbon = getindex.(var.(xmarginals), 2) .|> sqrt, fillalpha = 0.5, label = \"inf₂\")\n\nplot(px, size = (1200, 450))","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/#Learning-Kalman-filter-with-LSTM-driven-dynamic","page":"Global Parameter Optimisation","title":"Learning Kalman filter with LSTM driven dynamic","text":"","category":"section"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"In this example, our focus is on Bayesian state estimation in a Nonlinear State-Space Model. Specifically, we will utilize the time series generated by the Lorenz system as an example. ","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Our objective is to compute the marginal posterior distribution of the latent (hidden) state x_k at each time step k, considering the history of measurements up to that time step:","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"\np(x_k  y_1k)\n\n\nThe above expression represents the probability distribution of the latent state x_k given the measurements y_1k up to time step k\n\njulia\nusing RxInfer BenchmarkTools Flux ReverseDiff Random Plots LinearAlgebra ProgressMeter JLD StableRNGs\n\n\n\n\n Generate data\n\njulia\n Lorenz system equations to be used to generate dataset\nBasekwdef mutable struct Lorenz\n    dtFloat64\n    σFloat64\n    ρFloat64\n    βFloat64\n    xFloat64\n    yFloat64\n    zFloat64\nend\n\nfunction step(lLorenz)\n    dx = lσ * (ly - lx)         lx += ldt * dx\n    dy = lx * (lρ - lz) - ly   ly += ldt * dy\n    dz = lx * ly - lβ * lz     lz += ldt * dz\nend\n\n\n\n\njulia\n Dataset\nrng = StableRNG(999)\n\nordered_dataset = \nordered_parameters = \nfor σ = 1115\n    for ρ = 2327\n        for β_nom = 69\n            attractor = Lorenz(002 σ ρ β_nom30 1 1 1)\n            noise_free_data = 10 10 10\n            for i=199\n                step(attractor)\n                push(noise_free_data attractorx attractory attractorz)\n            end\n            push(ordered_dataset noise_free_data)\n            push(ordered_parameters σ ρ β_nom30)\n        end\n    end\nend\n\nnew_order = collect(1100)\nshuffle(rngnew_order)\n\ndataset =   noisy dataset\nnoise_free_dataset =   noise free dataset\nlorenz_parameters = \n\nfor i in new_order\n    local data = \n    push(noise_free_dataset ordered_dataseti)\n    push(lorenz_parameters ordered_parametersi)\n    for nfd in ordered_dataseti\n        push(datanfd+randn(rng3))\n    end\n    push(dataset data)\nend\n\ntrainset = dataset160\nvalidset = dataset6180\ntestset = dataset81end\n\nnoise_free_trainset = noise_free_dataset160\nnoise_free_validset = noise_free_dataset6180\nnoise_free_testset = noise_free_dataset81end\n\n\n\n\n\n Data visualization\n\njulia\none_nonoise=noise_free_trainset1\none=trainset1\ngx gy gz = zeros(100) zeros(100) zeros(100)\nrx ry rz = zeros(100) zeros(100) zeros(100)\nfor i=1100\n    rxi ryi rzi = onei1 onei2 onei3\n    gxi gyi gzi = one_nonoisei1 one_nonoisei2 one_nonoisei3\nend\np1=plot(rxrylabel=Noise observations)\np1=plot(gxgylabel=True state)\nxlabel(x)\nylabel(y)\np2=plot(rxrzlabel=Noise observations)\np2=plot(gxgzlabel=True state)\nxlabel(x)\nylabel(z)\np3=plot(ryrzlabel=Noise observations)\np3=plot(gygzlabel=True state)\nxlabel(y)\nylabel(z)\nplot(p1 p2 p3 size = (800 200)layout=(13))\n\n\n(assetsexamplesGlobal Parameter Optimisation_22_1png)\n\n\n Inference\n\n\nWe use the following state-space model representation\n\nbeginaligned\nx_k sim p(x_k  x_k-1) \ny_k sim p(y_k  x_k)\nendaligned","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"where x_k sim p(x_k  x_k-1) represents the hidden dynamics of our system.  The hidden dynamics of the Lorenz system exhibit nonlinearities and hence cannot be solved in the closed form. One manner of solving this problem is by introducing a neural network to approximate the transition matrix of the Lorenz system. ","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"beginaligned\nA_k-1=NN(y_k-1) \np(x_k  x_k-1)=mathcalN(x_k  A_k-1x_k-1 Q) \np(y_k  x_k)=mathcalN(y_k  Bx_k R)\nendaligned","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"where NN is the neural network. The input is the observation y_k-1, and output is the trasition matrix A_k-1. B denote distortion or measurment matrix. Q and R are covariance matrices. Note that the hidden state x_k comprises three coordinates, i.e. x_k = (rx_k ry_k rz_k)","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"By employing this state-space model representation and utilizing the neural network approximation, we can estimate the hidden dynamics and perform inference in the Lorenz system.","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# Neural Network model\nmutable struct NN\n    InputLayer\n    OutputLater\n    g\n    params\n    function NN(W1,b1,W2_1,W2_2,b2,s2_1,W3,b3)\n        InputLayer = Dense(W1, b1, relu)\n        Lstm = LSTM(W2_1,W2_2,b2,s2_1)\n        OutputLayer = Dense(W3, b3)\n        g = Chain(InputLayer, OutputLayer);\n        new(InputLayer, OutputLayer, g, (W1,b1,W2_1,W2_2,b2,s2_1,W3,b3))\n    end\nend","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/#Model-specification","page":"Global Parameter Optimisation","title":"Model specification","text":"","category":"section"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Note that we treat the trasition matrix A_k-1 as time-varying.","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"#State Space Model\n@model function ssm(n, As, Q::ConstVariable, B::ConstVariable, R::ConstVariable)\n    x = randomvar(n)\n    y = datavar(Vector{Float64}, n)\n    \n    x_prior_mean = zeros(3)\n    x_prior_cov  = Matrix(Diagonal(ones(3)))\n    \n    x[1] ~ MvNormalMeanCovariance(x_prior_mean, x_prior_cov)\n    y[1] ~ MvNormalMeanCovariance(B * x[1], R) where { q = q(mean)q(out)q(cov) }\n    \n    for i in 2:n\n        x[i] ~ MvNormalMeanCovariance(As[i - 1] * x[i - 1], Q) where { q = q(mean, out)q(cov) }\n        y[i] ~ MvNormalMeanCovariance(B * x[i], R) where { q = q(mean)q(out)q(cov) }\n    end\n    \n    return x, y\nend","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"We set distortion matrix B and the covariance matrices Q and R as identity matrix.","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Q = Matrix(Diagonal(ones(3)))*2\nB = Matrix(Diagonal(ones(3)))\nR = Matrix(Diagonal(ones(3)))\n;","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"We use the inference function in the RxInfer.jl. Before that, we need to bulid a function to get the matrix A output by the neural network. And the A is treated as a datavar in the inference function.","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"function get_matrix_AS(data,W1,b1,W2_1,W2_2,b2,s2_1,W3,b3)\n    n = length(data)\n    neural = NN(W1,b1,W2_1,W2_2,b2,s2_1,W3,b3)\n    Flux.reset!(neural)\n    As  = map((d) -> Matrix(Diagonal(neural.g(d))), data[1:end-1])\n    return As\nend","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"get_matrix_AS (generic function with 1 method)","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"The weights of neural network NN are initialized as follows:","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# Initial model parameters\nW1, b1 = randn(5, 3)./100, randn(5)./100\nW2_1, W2_2, b2, s2_1, s2_2 = randn(5 * 4, 5) ./ 100, randn(5 * 4, 5) ./ 100, randn(5*4) ./ 100, zeros(5), zeros(5)\nW3, b3 = randn(3, 5) ./ 100, randn(3) ./ 100\n;","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Before network training, we show the inference results for the hidden states:","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# Performance on an instance from the testset before training\nindex = 1\ndata=testset[index]\nn=length(data)\nresult = infer(\n    model = ssm(n, get_matrix_AS(data,W1,b1,W2_1,W2_2,b2,s2_1,W3,b3),Q,B,R), \n    data  = (y = data, ), \n    returnvars = (x = KeepLast(), ),\n    free_energy = true\n)\nx_est=result.posteriors[:x]\nrx, ry, rz = zeros(100), zeros(100), zeros(100)\nrx_est_m, ry_est_m, rz_est_m = zeros(100), zeros(100), zeros(100)\nrx_est_var, ry_est_var, rz_est_var = zeros(100), zeros(100), zeros(100)\n\nfor i=1:100\n    rx[i], ry[i], rz[i] = testset[index][i][1], testset[index][i][2], testset[index][i][3]\n    rx_est_m[i], ry_est_m[i], rz_est_m[i] = mean(x_est[i])[1], mean(x_est[i])[2], mean(x_est[i])[3]\n    rx_est_var[i], ry_est_var[i], rz_est_var[i] = var(x_est[i])[1], var(x_est[i])[2], var(x_est[i])[3]\nend\n\np1 = plot(rx,label=\"Hidden state rx\")\np1 = plot!(rx_est_m,label=\"Inferred states\", ribbon=rx_est_var)\np1 = scatter!(first.(testset[index]), label=\"Observations\", markersize=1.0)\n\np2 = plot(ry,label=\"Hidden state ry\")\np2 = plot!(ry_est_m,label=\"Inferred states\", ribbon=ry_est_var)\np2 = scatter!(getindex.(testset[index], 2), label=\"Observations\", markersize=1.0)\n\np3 = plot(rz,label=\"Hidden state rz\")\np3 = plot!(rz_est_m,label=\"Inferred states\", ribbon=rz_est_var)\np3 = scatter!(last.(testset[index]), label=\"Observations\", markersize=1.0)\n\n\nplot(p1, p2, p3, size = (1000, 300))","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/#Training-network","page":"Global Parameter Optimisation","title":"Training network","text":"","category":"section"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"In this part, we use the Free Energy as the objective function to optimize the weights of network.","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# free energy objective to be optimized during training\nfunction fe_tot_est(W1,b1,W2_1,W2_2,b2,s2_1,W3,b3)\n    fe_ = 0\n    for train_instance in trainset\n        result = infer(\n            model = ssm(n, get_matrix_AS(train_instance,W1,b1,W2_1,W2_2,b2,s2_1,W3,b3),Q,B,R), \n            data  = (y = train_instance, ), \n            returnvars = (x = KeepLast(), ),\n            free_energy = true\n        )\n        fe_ += result.free_energy[end]\n    end\n    return fe_\nend","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"fe_tot_est (generic function with 1 method)","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/#Training","page":"Global Parameter Optimisation","title":"Training","text":"","category":"section"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# Training is a computationally expensive procedure, for the sake of an example we load pre-trained weights\n# Uncomment the following code to train the network manually\n# opt = Flux.Optimise.RMSProp(0.006, 0.95)\n# params = (W1,b1,W2_1,W2_2,b2,s2_1,W3,b3)\n# @showprogress for epoch in 1:800\n#     grads = ReverseDiff.gradient(fe_tot_est, params);\n#     for i=1:length(params)\n#         Flux.Optimise.update!(opt,params[i],grads[i])\n#     end\n# end","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/#Test","page":"Global Parameter Optimisation","title":"Test","text":"","category":"section"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"Import the weights of neural network that we have trained.","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"W1a, b1a, W2_1a, W2_2a, b2a, s2_1a, W3, b3a = load(\"../data/nn_prediction/weights.jld\")[\"data\"];","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# Performance on an instance from the testset after training\nindex = 1\ndata = testset[index]\nn = length(data)\nresult = infer(\n    model = ssm(n, get_matrix_AS(data,W1a,b1a,W2_1a,W2_2a,b2a,s2_1a,W3,b3a),Q,B,R), \n    data  = (y = data, ), \n    returnvars = (x = KeepLast(), ),\n    free_energy = true\n)\nx_est=result.posteriors[:x]\n\ngx, gy, gz = zeros(100), zeros(100), zeros(100)\nrx, ry, rz = zeros(100), zeros(100), zeros(100)\nrx_est_m, ry_est_m, rz_est_m = zeros(100), zeros(100), zeros(100)\nrx_est_var, ry_est_var, rz_est_var = zeros(100), zeros(100), zeros(100)\n\nfor i=1:100\n    gx[i], gy[i], gz[i] = noise_free_testset[index][i][1], noise_free_testset[index][i][2], noise_free_testset[index][i][3]\n    rx[i], ry[i], rz[i] = testset[index][i][1], testset[index][i][2], testset[index][i][3]\n    rx_est_m[i], ry_est_m[i], rz_est_m[i] = mean(x_est[i])[1], mean(x_est[i])[2], mean(x_est[i])[3]\n    rx_est_var[i], ry_est_var[i], rz_est_var[i] = var(x_est[i])[1], var(x_est[i])[2], var(x_est[i])[3]\nend\n\np1 = plot(rx,label=\"Hidden state rx\")\np1 = plot!(rx_est_m,label=\"Inferred states\", ribbon=rx_est_var)\np1 = scatter!(first.(testset[index]), label=\"Observations\", markersize=1.0)\n\np2 = plot(ry,label=\"Hidden state ry\")\np2 = plot!(ry_est_m,label=\"Inferred states\", ribbon=ry_est_var)\np2 = scatter!(getindex.(testset[index], 2), label=\"Observations\", markersize=1.0)\n\np3 = plot(rz,label=\"Hidden state rz\")\np3 = plot!(rz_est_m,label=\"Inferred states\", ribbon=rz_est_var)\np3 = scatter!(last.(testset[index]), label=\"Observations\", markersize=1.0)\n\nplot(p1, p2, p3, size = (1000, 300))","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/#Prediction","page":"Global Parameter Optimisation","title":"Prediction","text":"","category":"section"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"In the above instances, the observations during whole time are available. For prediction task, we can only access to the  observations untill k and estimate the future state at time k+1, k+2, dots,k+T.","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"We can still solve this problem by the trained neural network to approximate the transition matrix. And we can get the one-step prediction in the future. Then, the predicted results are feed into the neural network to generate the transition matrix for the next step, and roll into the future to get the multi-step prediction.","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"beginaligned\nA_k=NN(x_k) \np(x_k+1  x_k)=mathcalN(x_k+1  A_kx_k Q) \nendaligned","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"#Define the prediction function\nmultiplyGaussian(A,m,V) = (A * m, A * V * transpose(A))\nsumGaussians(m1,m2,V1,V2) = (m1 + m2, V1 + V2)\n\nfunction runForward(A,B,Q,R,mh_old,Vh_old)\n    mh_1, Vh_1 = multiplyGaussian(A,mh_old,Vh_old)\n    mh_pred, Vh_pred = sumGaussians(mh_1, zeros(length(mh_old)), Vh_1, Q)\nend\n\nfunction g_predict(mh_old,Vh_old,Q)\n    neural = NN(W1a,b1a,W2_1a,W2_2a,b2a,s2_1a,W3,b3a)\n    # Flux.reset!(neural)\n    As  = map((d) -> Matrix(Diagonal(neural.g(d))), [mh_old])\n    As = As[1]\n    return runForward(As,B,Q,R,mh_old,Vh_old), As\nend","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"g_predict (generic function with 1 method)","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"After k=75, the observations are not available, and we predict the future state from k=76 to the end","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"tt = 75\nmh = mean(x_est[tt])\nVh = cov(x_est[tt])\nmo_list, Vo_list, A_list = [], [], [] \ninv_Q = inv(Q)\nfor t=1:100-tt\n    (mo, Vo), A_t = g_predict(mh,Vh,inv_Q)\n    push!(mo_list, mo)\n    push!(Vo_list, Vh)\n    push!(A_list, A_t)\n    global mh = mo\n    global Vh = Vo\nend","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"# Prediction visualization\nrx, ry, rz = zeros(100), zeros(100), zeros(100)\nrx_est_m, ry_est_m, rz_est_m = zeros(100), zeros(100), zeros(100)\nrx_est_var, ry_est_var, rz_est_var = zeros(100), zeros(100), zeros(100)\nfor i=1:tt\n    rx[i], ry[i], rz[i] = testset[index][i][1], testset[index][i][2], testset[index][i][3]\n    rx_est_m[i], ry_est_m[i], rz_est_m[i] = mean(x_est[i])[1], mean(x_est[i])[2], mean(x_est[i])[3]\n    rx_est_var[i], ry_est_var[i], rz_est_var[i] = var(x_est[i])[1], var(x_est[i])[2], var(x_est[i])[3]\nend\nfor i=tt+1:100\n    ii=i-tt\n    rx[i], ry[i], rz[i] = testset[index][i][1], testset[index][i][2], testset[index][i][3]\n    rx_est_m[i], ry_est_m[i], rz_est_m[i] = mo_list[ii][1], mo_list[ii][2], mo_list[ii][3]\n    rx_est_var[i], ry_est_var[i], rz_est_var[i] = Vo_list[ii][1,1], Vo_list[ii][2,2], Vo_list[ii][3,3]\nend\np1 = plot(rx,label=\"Ground truth rx\")\np1 = plot!(rx_est_m,label=\"Inffered state rx\",ribbon=rx_est_var)\np1 = scatter!(first.(testset[index][1:tt]), label=\"Observations\", markersize=1.0)\n\np2 = plot(ry,label=\"Ground truth ry\")\np2 = plot!(ry_est_m,label=\"Inferred states\", ribbon=ry_est_var)\np2 = scatter!(getindex.(testset[index][1:tt], 2), label=\"Observations\", markersize=1.0)\n\np3 = plot(rz,label=\"Ground truth rz\")\np3 = plot!(rz_est_m,label=\"Inferred states\", ribbon=rz_est_var)\np3 = scatter!(last.(testset[index][1:tt]), label=\"Observations\", markersize=1.0)\n\n\nplot(p1, p2, p3, size = (1000, 300),legend=:bottomleft)","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Global Parameter Optimisation/","page":"Global Parameter Optimisation","title":"Global Parameter Optimisation","text":"","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/#examples-autoregressive-models","page":"Autoregressive Models","title":"Autoregressive Models","text":"","category":"section"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"In this example we are going to perform an automated Variational Bayesian Inference for a latent autoregressive model that can be represented as following:","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"beginaligned\np(gamma) = mathrmGamma(gammaa b)\np(mathbftheta) = mathcalN(mathbfthetamathbfmu Sigma)\np(x_tmathbfx_t-1t-k) = mathcalN(x_tmathbftheta^Tmathbfx_t-1t-k gamma^-1)\np(y_tx_t) = mathcalN(y_tx_t tau^-1)\nendaligned","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"where x_t is a current state of our system, mathbfx_t-1t-k is a sequence of k previous states, k is an order of autoregression process, mathbftheta is a vector of transition coefficients, gamma is a precision of state transition process, y_k is a noisy observation of x_k with precision tau.","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"For a more rigorous introduction to Bayesian inference in Autoregressive models we refer to Albert Podusenko, Message Passing-Based Inference for Time-Varying Autoregressive Models.","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"We start with importing all needed packages:","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"using RxInfer, Distributions, LinearAlgebra, Random, Plots, BenchmarkTools, Parameters","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Let's generate some synthetic dataset, we use a predefined sets of coeffcients for k = 1, 3 and 5 respectively:","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# The following coefficients correspond to stable poles\ncoefs_ar_1 = [-0.27002517200218096]\ncoefs_ar_2 = [0.4511170798064709, -0.05740081602446657]\ncoefs_ar_5 = [0.10699399235785655, -0.5237303489793305, 0.3068897071844715, -0.17232255282458891, 0.13323964347539288];","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"function generate_ar_data(rng, n, θ, γ, τ)\n    order        = length(θ)\n    states       = Vector{Vector{Float64}}(undef, n + 3order)\n    observations = Vector{Float64}(undef, n + 3order)\n    \n    γ_std = sqrt(inv(γ))\n    τ_std = sqrt(inv(τ))\n    \n    states[1] = randn(rng, order)\n    \n    for i in 2:(n + 3order)\n        states[i]       = vcat(rand(rng, Normal(dot(θ, states[i - 1]), γ_std)), states[i-1][1:end-1])\n        observations[i] = rand(rng, Normal(states[i][1], τ_std))\n    end\n    \n    return states[1+3order:end], observations[1+3order:end]\nend","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"generate_ar_data (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# Seed for reproducibility\nseed = 123\nrng  = MersenneTwister(seed)\n\n# Number of observations in synthetic dataset\nn = 500\n\n# AR process parameters\nreal_γ = 1.0\nreal_τ = 0.5\nreal_θ = coefs_ar_5\n\nstates, observations = generate_ar_data(rng, n, real_θ, real_γ, real_τ);","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Let's plot our synthetic dataset:","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"plot(first.(states), label = \"Hidden states\")\nscatter!(observations, label = \"Observations\")","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Next step is to specify probabilistic model, inference constraints and run inference procedure with RxInfer. We will specify two different models for Multivariate AR with order k > 1 and for Univariate AR (reduces to simple State-Space-Model) with order k = 1.","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"@model function lar_model(T::Type, n, order, c, τ)\n    \n     \n    # We create a sequence of random variables for hidden states\n    x = randomvar(n)\n    # As well a sequence of observartions\n    y = datavar(Float64, n)\n    \n    ct = constvar(c)\n    # We assume observation noise to be known\n    cτ = constvar(τ)\n    \n    γ  = randomvar()\n    θ  = randomvar()\n    x0 = randomvar()\n    \n    # Prior for first state\n    if T === Multivariate\n        γ  ~ Gamma(α = 1.0, β = 1.0)\n        θ  ~ MvNormal(μ = zeros(order), Λ = diageye(order))\n        x0 ~ MvNormal(μ = zeros(order), Λ = diageye(order))\n    else\n        γ  ~ Gamma(α = 1.0, β = 1.0)\n        θ  ~ Normal(μ = 0.0, γ = 1.0)\n        x0 ~ Normal(μ = 0.0, γ = 1.0)\n    end\n    \n    x_prev = x0\n    \n    for i in 1:n\n        \n        x[i] ~ AR(x_prev, θ, γ) \n        \n        if T === Multivariate\n            y[i] ~ Normal(μ = dot(ct, x[i]), γ = cτ)\n        else\n            y[i] ~ Normal(μ = ct * x[i], γ = cτ)\n        end\n        \n        x_prev = x[i]\n    end\n\nend","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"@constraints function ar_constraints() \n    q(x0, x, θ, γ) = q(x0, x)q(θ)q(γ)\nend","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"ar_constraints (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"@meta function ar_meta(artype, order, stype)\n    AR() -> ARMeta(artype, order, stype)\nend","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"ar_meta (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"morder  = 5\nmartype = Multivariate\nmc      = ReactiveMP.ar_unit(martype, morder)\nmconstraints = ar_constraints()\nmmeta        = ar_meta(martype, morder, ARsafe())\n\nmoptions = (limit_stack_depth = 100, )\n\nmmodel         = lar_model(martype, length(observations), morder, mc, real_τ)\nmdata          = (y = observations, )\nminitmarginals = (γ = GammaShapeRate(1.0, 1.0), θ = MvNormalMeanPrecision(zeros(morder), diageye(morder)))\nmreturnvars    = (x = KeepLast(), γ = KeepEach(), θ = KeepEach())\n\n# First execution is slow due to Julia's initial compilation \n# Subsequent runs will be faster (benchmarks are below)\nmresult = infer(\n    model = mmodel, \n    data  = mdata,\n    constraints   = mconstraints,\n    meta          = mmeta,\n    options       = moptions,\n    initmarginals = minitmarginals,\n    returnvars    = mreturnvars,\n    free_energy   = true,\n    iterations    = 50, \n    showprogress  = false\n);","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"@unpack x, γ, θ = mresult.posteriors","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Dict{Symbol, Vector} with 3 entries:\n  :γ => GammaShapeRate{Float64}[GammaShapeRate{Float64}(a=251.0, b=47.5918)\n, Ga…\n  :θ => MvNormalWeightedMeanPrecision{Float64, Vector{Float64}, Matrix{Floa\nt64}…\n  :x => MvNormalWeightedMeanPrecision{Float64, Vector{Float64}, Matrix{Floa\nt64}…","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"We will use different initial marginals depending on type of our AR process","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"p1 = plot(first.(states), label=\"Hidden state\")\np1 = scatter!(p1, observations, label=\"Observations\")\np1 = plot!(p1, first.(mean.(x)), ribbon = first.(std.(x)), label=\"Inferred states\", legend = :bottomright)\n\np2 = plot(mean.(γ), ribbon = std.(γ), label = \"Inferred transition precision\", legend = :topright)\np2 = plot!([ real_γ ], seriestype = :hline, label = \"Real transition precision\")\n\np3 = plot(mresult.free_energy, label = \"Bethe Free Energy\")\n\nplot(p1, p2, p3, layout = @layout([ a; b c ]))","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Let's also plot a subrange of our results:","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"subrange = div(n,5):(div(n, 5) + div(n, 5))\n\nplot(subrange, first.(states)[subrange], label=\"Hidden state\")\nscatter!(subrange, observations[subrange], label=\"Observations\")\nplot!(subrange, first.(mean.(x))[subrange], ribbon = sqrt.(first.(var.(x)))[subrange], label=\"Inferred states\", legend = :bottomright)","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"It is also interesting to see where our AR coefficients converge to:","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"let\n    pθ = plot()\n\n    θms = mean.(θ)\n    θvs = var.(θ)\n    \n    l = length(θms)\n\n    edim(e) = (a) -> map(r -> r[e], a)\n\n    for i in 1:length(first(θms))\n        pθ = plot!(pθ, θms |> edim(i), ribbon = θvs |> edim(i) .|> sqrt, label = \"Estimated θ[$i]\")\n    end\n    \n    for i in 1:length(real_θ)\n        pθ = plot!(pθ, [ real_θ[i] ], seriestype = :hline, label = \"Real θ[$i]\")\n    end\n    \n    plot(pθ, legend = :outertopright, size = (800, 300))\nend","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"println(\"$(length(real_θ))-order AR inference Bethe Free Energy: \", last(mresult.free_energy))","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"5-order AR inference Bethe Free Energy: 1024.0775661298358","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"We can also run a 1-order AR inference on 5-order AR data:","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"uorder  = 1\nuartype = Univariate\nuc      = ReactiveMP.ar_unit(uartype, uorder)\nuconstraints = ar_constraints()\numeta        = ar_meta(uartype, uorder, ARsafe())\n\nuoptions = (limit_stack_depth = 100, )\n\numodel         = lar_model(uartype, length(observations), uorder, uc, real_τ)\nudata          = (y = observations, )\nuinitmarginals = (γ = GammaShapeRate(1.0, 1.0), θ = NormalMeanPrecision(0.0, 1.0))\nureturnvars    = (x = KeepLast(), γ = KeepEach(), θ = KeepEach())\n\nuresult = infer(\n    model = umodel, \n    data  = udata,\n    meta  = umeta,\n    constraints   = uconstraints,\n    initmarginals = uinitmarginals,\n    returnvars    = ureturnvars,\n    free_energy   = true,\n    iterations    = 50, \n    showprogress  = false\n);","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"println(\"1-order AR inference Bethe Free Energy: \", last(uresult.free_energy))","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"1-order AR inference Bethe Free Energy: 1025.8792949319954","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"if uresult.free_energy[end] > mresult.free_energy[end]\n    println(\"We can see that, according to final Bethe Free Energy value, in this example 5-order AR process can describe data better than 1-order AR.\")\nelse\n    error(\"AR-1 performs better than AR-5...\") \nend","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"We can see that, according to final Bethe Free Energy value, in this exampl\ne 5-order AR process can describe data better than 1-order AR.","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/#Autoregressive-Moving-Average-Model","page":"Autoregressive Models","title":"Autoregressive Moving Average Model","text":"","category":"section"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Bayesian ARMA model can be effectively implemeted in RxInfer.jl. For theoretical details on Varitional Inference for ARMA model, we refer the reader to the following paper.  The Bayesian ARMA model can be written as follows:","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"beginaligned\ne_t sim mathcalN(0 gamma^-1) quad\ntheta sim mathcalMN(mathbf0 mathbfI) quad\neta sim mathcalMN(mathbf0 mathbfI) \nmathbfh_0 sim mathcalMNleft(beginbmatrix\ne_-1 \ne_-2\nendbmatrix mathbfIright) \nmathbfh_t = mathbfSmathbfh_t-1 + mathbfc e_t-1 \nmathbfx_t = boldsymboltheta^topmathbfx_t-1 + boldsymboleta^topmathbfh_t + e_t \nendaligned","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"where shift matrix mathbfS is","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"beginaligned\nmathbfS = beginpmatrix\n0  0 \n1  0 \nendpmatrix\nendaligned","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"and unit vector mathbfc: ","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"beginaligned\nmathbfc=1 0\nendaligned","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"when MA order is 2","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"In this way, mathbfh_t containing errors e_t can be viewed as hidden state.","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"In short, the Bayesian ARMA model has two intractabilities: (1) induced by the multiplication of two Gaussian RVs, i.e., boldsymboleta^topmathbfh_t, (2) induced by errors e_t that prevents analytical update of precision parameter gamma (this can be easily seen when constructing the Factor Graph, i.e. there is a loop). Both problems can be easily resolved in RxInfer.jl, by creating a hybrid inference algorithm based on Loopy Variational Message Passing.","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# Load packages\nusing RxInfer, LinearAlgebra, CSV, DataFrames, Plots","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# Define shift function\nfunction shift(dim)\n    S = Matrix{Float64}(I, dim, dim)\n    for i in dim:-1:2\n           S[i,:] = S[i-1, :]\n    end\n    S[1, :] = zeros(dim)\n    return S\nend","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"shift (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"@model function ARMA(n, x_prev, h_prior, γ_prior, τ_prior, η_prior, θ_prior, p, q)\n        \n    c = zeros(p); c[1] = 1.0; # AR\n    b = zeros(q); b[1] = 1.0; # MA\n    S = shift(q); # MA\n    \n    # initialize variables\n    h  = randomvar(n-1)\n    e  = randomvar(n)\n    z  = randomvar(n)\n    \n    x  = datavar(Float64, n) where { allow_missing = true }\n    \n    # priors\n    γ  ~ Gamma(shape = shape(γ_prior), rate = rate(γ_prior))\n    η  ~ MvNormal(mean = mean(η_prior), precision = precision(η_prior))\n    θ  ~ MvNormal(mean = mean(θ_prior), precision = precision(θ_prior))\n    τ  ~ Gamma(shape = shape(τ_prior), rate = rate(τ_prior))\n    \n    # initial\n    h_0  ~ MvNormal(mean = mean(h_prior), precision = precision(h_prior))\n    z[1] ~ AR(h_0, η, τ)\n    e[1] ~ Normal(mean = 0.0, precision = γ)\n\n    x[1] ~ dot(b, z[1]) + dot(θ, x_prev[1]) + e[1]\n    \n    h_prev = h_0\n    for t in 1:n-1\n        \n        e[t+1] ~ Normal(mean = 0.0, precision = γ)\n        h[t]   ~ S*h_prev + b*e[t]\n        z[t+1] ~ AR(h[t], η, τ)\n        x[t+1] ~ dot(z[t+1], b) + dot(θ, x_prev[t]) + e[t+1]\n        h_prev = h[t]\n    end\nend","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"To validate our model and inference, we will use American Airlines stock data downloaded from Kaggle","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"x_df = CSV.read(\"../data/arma/aal_stock.csv\", DataFrame)","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"1259×7 DataFrame\n  Row │ date        open     high     low      close    volume    Name\n      │ Date        Float64  Float64  Float64  Float64  Int64     String3\n──────┼───────────────────────────────────────────────────────────────────\n    1 │ 2013-02-08    15.07    15.12   14.63     14.75   8407500  AAL\n    2 │ 2013-02-11    14.89    15.01   14.26     14.46   8882000  AAL\n    3 │ 2013-02-12    14.45    14.51   14.1      14.27   8126000  AAL\n    4 │ 2013-02-13    14.3     14.94   14.25     14.66  10259500  AAL\n    5 │ 2013-02-14    14.94    14.96   13.16     13.99  31879900  AAL\n    6 │ 2013-02-15    13.93    14.61   13.93     14.5   15628000  AAL\n    7 │ 2013-02-19    14.33    14.56   14.08     14.26  11354400  AAL\n    8 │ 2013-02-20    14.17    14.26   13.15     13.33  14725200  AAL\n  ⋮   │     ⋮          ⋮        ⋮        ⋮        ⋮        ⋮         ⋮\n 1253 │ 2018-01-30    52.45    53.05   52.36     52.59   4741808  AAL\n 1254 │ 2018-01-31    53.08    54.71   53.0      54.32   5962937  AAL\n 1255 │ 2018-02-01    54.0     54.64   53.59     53.88   3623078  AAL\n 1256 │ 2018-02-02    53.49    53.99   52.03     52.1    5109361  AAL\n 1257 │ 2018-02-05    51.99    52.39   49.75     49.76   6878284  AAL\n 1258 │ 2018-02-06    49.32    51.5    48.79     51.18   6782480  AAL\n 1259 │ 2018-02-07    50.91    51.98   50.89     51.4    4845831  AAL\n                                                         1244 rows omitted","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# we will use \"close\" column\nx_data = filter(!ismissing, x_df[:, 5]);","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# Plot data\nplot(x_data, xlabel=\"day\", ylabel=\"price\", label=false)","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"p_order = 10 # AR\nq_order = 4 # MA","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"4","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# Training set\ntrain_size = 1000\nx_prev_train = [Float64.(x_data[i+p_order-1:-1:i]) for i in 1:length(x_data)-p_order][1:train_size]\nx_train = Float64.(x_data[p_order+1:end])[1:train_size];","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# Test set\nx_prev_test = [Float64.(x_data[i+p_order-1:-1:i]) for i in 1:length(x_data)-p_order][train_size+1:end]\nx_test = Float64.(x_data[p_order+1:end])[train_size+1:end];","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/#Inference","page":"Autoregressive Models","title":"Inference","text":"","category":"section"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# Constraints are needed for performing VMP\narma_constraints = @constraints begin\n    q(z, h_0, h, η, τ, γ,e) = q(h_0)q(z, h)q(η)q(τ)q(γ)q(e)\nend;","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# This cell defines prior knowledge for model parameters\nh_prior = MvNormalMeanPrecision(zeros(q_order), diageye(q_order))\nγ_prior = GammaShapeRate(1e4, 1.0)\nτ_prior = GammaShapeRate(1e2, 1.0)\nη_prior = MvNormalMeanPrecision(zeros(q_order), diageye(q_order))\nθ_prior = MvNormalMeanPrecision(zeros(p_order), diageye(p_order));","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# Model's graph has structural loops, hence, it requires pre-initialisation\narma_imarginals = (h_0 = h_prior, h = h_prior, γ = γ_prior, τ = τ_prior, η = η_prior, θ = θ_prior);\narma_imessages  = (h_0 = h_prior, h = h_prior);\narma_meta       = ar_meta(Multivariate, q_order, ARsafe());","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"result = infer(\n    model = ARMA(length(x_train), x_prev_train, h_prior, γ_prior, τ_prior, η_prior, θ_prior, p_order, q_order), \n    data  = (x = x_train, ),\n    initmarginals = arma_imarginals,\n    initmessages  = arma_imessages,\n    constraints   = arma_constraints,\n    meta          = arma_meta,\n    iterations    = 50,\n    options       = (limit_stack_depth = 400, ),\n);","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"plot(mean.(result.posteriors[:e][end]), ribbon = var.(result.posteriors[:e][end]), label = \"eₜ\")","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# extract posteriors\nh_posterior = result.posteriors[:h][end][end]\nγ_posterior = result.posteriors[:γ][end]\nτ_posterior = result.posteriors[:τ][end]\nη_posterior = result.posteriors[:η][end]\nθ_posterior = result.posteriors[:θ][end];","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/#Prediction","page":"Autoregressive Models","title":"Prediction","text":"","category":"section"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"Here we are going to use our inference results in order to predict the dataset itself","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"# The prediction function is aimed at approximating the predictive posterior distribution\n# It triggers the rules in the generative order (in future, RxInfer.jl will provide this function out of the box)\nfunction prediction(x_prev, h_posterior, γ_posterior, τ_posterior, η_posterior, θ_posterior, p, q)\n    h_out = MvNormalMeanPrecision(mean(h_posterior), precision(h_posterior))\n    ar_out = @call_rule AR(:y, Marginalisation) (m_x=h_out, q_θ=η_posterior, q_γ=τ_posterior, meta=ARMeta(Multivariate, p, ARsafe()))\n    c = zeros(p); c[1] = 1.0\n    b = zeros(q); b[1] = 1.0\n    ar_dot_out = @call_rule typeof(dot)(:out, Marginalisation) (m_in1=PointMass(b), m_in2=ar_out)\n    θ_out = MvNormalMeanPrecision(mean(θ_posterior), precision(θ_posterior))\n    ma_dot_out = @call_rule typeof(dot)(:out, Marginalisation) (m_in1=PointMass(x_prev), m_in2=θ_out)\n    e_out = @call_rule NormalMeanPrecision(:out, Marginalisation) (q_μ=PointMass(0.0), q_τ=mean(γ_posterior))\n    ar_ma = @call_rule typeof(+)(:out, Marginalisation) (m_in1=ar_dot_out, m_in2=ma_dot_out)  \n    @call_rule typeof(+)(:out, Marginalisation) (m_in1=ar_ma, m_in2=e_out)  \nend","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"prediction (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"predictions = []\nfor x_prev in x_prev_test\n    push!(predictions, prediction(x_prev, h_posterior, γ_posterior, τ_posterior, η_posterior, θ_posterior, p_order, q_order))\n    # after every new prediction we can actually \"retrain\" the model to use the power of Bayesian approach\n    # we will skip this part at this notebook\nend","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"plot(x_test, label=\"test data\", legend=:topleft)\nplot!(mean.(predictions)[1:end], ribbon=std.(predictions)[1:end], label=\"predicted\", xlabel=\"day\", ylabel=\"price\")","category":"page"},{"location":"examples/problem_specific/Autoregressive Models/","page":"Autoregressive Models","title":"Autoregressive Models","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/#examples-how-to-train-your-hidden-markov-model","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"","category":"section"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate()\n;","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"In this example, we'll be tracking a Roomba as it moves throughout a 3-bedroom apartment consisting of a bathroom, a master bedroom, and a living room. It's important to keep track of your AI's, so we want to make sure we can keep tabs on it whenever we leave the apartment. ","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"First, in order to track the Roomba's movements using RxInfer, we need to come up with a model. Since we have a discrete set of rooms in the apartment, we can use a categorical distribution to represent the Roomba's position. There are three  rooms in the apartment, meaning we need three states in our categorical distribution. At time t, let's call the estimate of the Roomba's position s_t.","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"However, we also know that some rooms are more accessible than others, meaning the Roomba is more likely to move between these rooms - for example, it's rare to have a door directly between the bathroom and the master bedroom. We can encode this information using a transition matrix, which we will call A.","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"Our Roomba is equipped with a small camera that tracks the surface it is moving over. We will use this camera to obtain our observations since we know that there is a carpet in the living room, tiles in the bathroom, and hardwood floors in the master bedroom. However, this method is not foolproof, and sometimes the Roomba will make mistakes and mistake the hardwood floor for tiles or the carpet for hardwood. Don't be too hard on the little guy, it's just a Roomba after all.","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"At time t, we will call our observations x_t and encode the mapping from the Roomba's position to the observations in a matrix we call B. B also encodes the likelihood that the Roomba will make a mistake and get the wrong observation. This leaves us with the following model specification:","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"beginaligned\n    s_t  sim mathcalCat(A s_t-1)\n    x_t  sim mathcalCat(B s_t)\nendaligned","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"This type of discrete state space model is known as a Hidden Markov Model or HMM for short. Our goal is to learn the matrices A and B so we can use them to track the whereabouts of our little cleaning agent.","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"using RxInfer, Random, BenchmarkTools, Distributions, LinearAlgebra, Plots","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"In order to generate data to mimic the observations of the Roomba, we need to specify two things: the actual transition probabilities between the states (i.e., how likely is the Roomba to move from one room to another), and the observation distribution (i.e., what type of texture will the Roomba encounter in each room). We can then use these specifications to generate observations from our hidden Markov model (HMM).","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"To generate our observation data, we'll follow these steps:","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"Assume an initial state for the Roomba. For example, we can start the Roomba in the bedroom.\nDetermine where the Roomba went next by drawing from a Categorical distribution with the transition probabilities between the different rooms.\nDetermine the observation encountered in this room by drawing from a Categorical distribution with the corresponding observation probabilities.\nRepeat steps 2-3 for as many samples as we want.","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"The following code implements this process and generates our observation data:","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"\"\"\"\n    rand_vec(rng, distribution::Categorical)\n\nThis function returns a one-hot encoding of a random sample from a categorical distribution. The sample is drawn with the `rng` random number generator.\n\"\"\"\nfunction rand_vec(rng, distribution::Categorical) \n    k = ncategories(distribution)\n    s = zeros(k)\n    drawn_category = rand(rng, distribution)\n    s[drawn_category] = 1.0\n    return s\nend\n\nfunction generate_data(n_samples; seed = 42)\n    \n    rng = MersenneTwister(seed)\n    \n    # Transition probabilities \n    state_transition_matrix = [0.9 0.0 0.1;\n                                                        0.0 0.9 0.1; \n                                                        0.05 0.05 0.9] \n    # Observation noise\n    observation_distribution_matrix = [0.9 0.05 0.05;\n                                                                         0.05 0.9 0.05;\n                                                                         0.05 0.05 0.9] \n    # Initial state\n    s_initial = [1.0, 0.0, 0.0] \n    \n    states = Vector{Vector{Float64}}(undef, n_samples) # one-hot encoding of the states\n    observations = Vector{Vector{Float64}}(undef, n_samples) # one-hot encoding of the observations\n    \n    s_prev = s_initial\n    \n    for t = 1:n_samples\n        s_probvec = state_transition_matrix * s_prev\n        states[t] = rand_vec(rng, Categorical(s_probvec ./ sum(s_probvec)))\n        obs_probvec = observation_distribution_matrix * states[t]\n        observations[t] = rand_vec(rng, Categorical(obs_probvec ./ sum(obs_probvec)))\n        s_prev = states[t]\n    end\n    \n    return observations, states\nend","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"generate_data (generic function with 1 method)","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"We will generate 100 data points to simulate 100 ticks of the Roomba moving through the apartment. x_data will contain the Roomba's measurements of the floor it's currently on, and s_data will contain information on the room the Roomba was actually in.","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"# Test data\nN = 100\nx_data, s_data = generate_data(N);\n\nscatter(argmax.(s_data), leg=false, xlabel=\"Time\",yticks= ([1,2,3],[\"Bedroom\",\"Living room\",\"Bathroom\"]))","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"Now it is time to build our model. As mentioned earlier, we will use Categorical distributions for the states and observations. To learn the A and B matrices we can use MatrixDirichlet priors. For the A-matrix, since we have no apriori idea how the roomba is actually going to move we will assume that it moves randomly. We can represent this by filling our MatrixDirichlet prior on A with ones. Remember that this will get updated once we start learning, so it's fine if our initial guess is not quite accurate. As for the observations, we have good reason to trust our Roomba's measurements. To represent this, we will add large values to the diagonal of our prior on B. However, we also acknowledge that the Roomba is not infallible, so we will add some noise on the off-diagonal entries.","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"Since we will use Variational Inference, we also have to specify inference constraints. We will use a structured variational approximation to the true posterior distribution, where we decouple the variational posterior over the states (q(s_0, s)) from the posteriors over the transition matrices (q(A) and q(B)). This dependency decoupling in the approximate posterior distribution ensures that inference is tractable. Let's build the model!","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"# Model specification\n@model function hidden_markov_model(n)\n    \n    A ~ MatrixDirichlet(ones(3,3))\n    B ~ MatrixDirichlet([ 10.0 1.0 1.0; \n                                            1.0 10.0 1.0; \n                                            1.0 1.0 10.0 ])\n    \n    s_0 ~ Categorical(fill(1.0 / 3.0, 3))\n    \n    s = randomvar(n)\n    x = datavar(Vector{Float64}, n)\n    \n    s_prev = s_0\n    \n    for t in 1:n\n        s[t] ~ Transition(s_prev, A) \n        x[t] ~ Transition(s[t], B)\n        s_prev = s[t]\n    end\n    \nend\n\n# Constraints specification\n@constraints function hidden_markov_model_constraints()\n    q(s_0, s, A, B) = q(s_0, s)q(A)q(B)\nend","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"hidden_markov_model_constraints (generic function with 1 method)","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"Now it's time to perform inference and find out where the Roomba went in our absence. Did it remember to clean the bathroom?","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"We'll be using Variational Inference to perform inference, which means we need to set some initial marginals as a starting point. RxInfer makes this easy with the vague function, which provides an uninformative guess. If you have better ideas, you can try a different initial guess and see what happens.","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"Since we're only interested in the final result - the best guess about the Roomba's position - we'll only keep the last results. Let's start the inference process!","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"imodel = hidden_markov_model(N)\nidata = (x = x_data,)\n\nimarginals = (\n    A = vague(MatrixDirichlet, 3, 3), \n    B = vague(MatrixDirichlet, 3, 3), \n    s = vague(Categorical, 3)\n)\n\nireturnvars = (\n    A = KeepLast(),\n    B = KeepLast(),\n    s = KeepLast()\n)\n\nresult = infer(\n    model         = imodel, \n    data          = idata,\n    constraints   = hidden_markov_model_constraints(),\n    initmarginals = imarginals, \n    returnvars    = ireturnvars, \n    iterations    = 20, \n    free_energy   = true\n);","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"That was fast! Let's take a look at our results. If we're successful, we should have a good idea about the actual layout of the apartment (a good posterior marginal over A) and about the uncertainty in the roombas observations (A good posterior over B). Let's see if it worked","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"println(\"Posterior Marginal for A:\")\nmean(result.posteriors[:A])","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"Posterior Marginal for A:\n3×3 Matrix{Float64}:\n 0.871955   0.0290008  0.0572283\n 0.0349989  0.817591   0.135675\n 0.0930457  0.153408   0.807097","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"println(\"Posterior Marginal for B:\")\nmean(result.posteriors[:B])","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"Posterior Marginal for B:\n3×3 Matrix{Float64}:\n 0.949636   0.0974741  0.0518997\n 0.0244903  0.833632   0.0277884\n 0.0258734  0.0688939  0.920312","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"Finally, we can check if we were successful in keeping tabs on our Roomba's whereabouts. We can also check if our model has converged by looking at the Free Energy. ","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"p1 = scatter(argmax.(s_data), \n                        title=\"Inference results\", \n                        label = \"Real\", \n                        ms = 6, \n                        legend=:right,\n                        xlabel=\"Time\" ,\n                        yticks= ([1,2,3],[\"Bedroom\",\"Living room\",\"Bathroom\"]),\n                        size=(900,550)\n                        )\n\np1 = scatter!(p1, argmax.(ReactiveMP.probvec.(result.posteriors[:s])),\n                        label = \"Inferred\",\n                        ms = 3\n                        )\n\np2 = plot(result.free_energy, \n                    label=\"Free energy\",\n                    xlabel=\"Iteration Number\"\n                    )\n\nplot(p1, p2, layout = @layout([ a; b ]))","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Hidden Markov Model/","page":"How to train your Hidden Markov Model","title":"How to train your Hidden Markov Model","text":"Neat! Now you know how to track a Roomba if you ever need to. You also learned how to fit a Hidden Markov Model using RxInfer in the process.","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/#examples-rts-vs-bifm-smoothing","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"","category":"section"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"___Credits to Martin de Quincey___","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"This notebook performs Kalman smoothing on a factor graph using message passing, based on the BIFM Kalman smoother. This notebook is based on:","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"F. Wadehn, “State Space Methods with Applications in Biomedical Signal Processing,” ETH Zurich, 2019. Accessed: Jun. 16, 2021. [Online]. Available: https://www.research-collection.ethz.ch/handle/20.500.11850/344762\nH. Loeliger, L. Bruderer, H. Malmberg, F. Wadehn, and N. Zalmai, “On sparsity by NUV-EM, Gaussian message passing, and Kalman smoothing,” in 2016 Information Theory and Applications Workshop (ITA), Jan. 2016, pp. 1–10. doi: 10.1109/ITA.2016.7888168.","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"We perform Kalman smoothing in the linear state space model, represented by:","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"beginaligned\n    Z_k+1 = A Z_k + B U_k \n    Y_k = C Z_k + W_k\nendaligned","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"with observations Y_k, latent states Z_k and inputs U_k. W_k is the observation noise. A in mathrmR^n times n, B in mathrmR^n times m and C in mathrmR^d times n are the transition matrices in the model. Here n, m and d denote the dimensionality of the latent, input and output dimension, respectively.","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"The corresponding probabilistic model can be represented as ","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"beginaligned\n        p(y z u)\n        = p(z_0) prod_k=1^N p(y_k mid z_k) p(z_kmid z_k-1 u_k-1) p(u_k-1) \n        = mathcalN(z_0 mid mu_z_0 Sigma_z_0) left( prod_k=1^N mathcalN(y_k mid C z_k Sigma_W) delta(z_k - (Az_k-1 + Bu_k-1)) mathcalN(u_k-1 mid mu_i_k-1 Sigma_u_k-1) right)\nendaligned","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/#Import-packages","page":"RTS vs BIFM Smoothing","title":"Import packages","text":"","category":"section"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"using RxInfer, Random, LinearAlgebra, BenchmarkTools, ProgressMeter, Plots, StableRNGs","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/#Data-generation","page":"RTS vs BIFM Smoothing","title":"Data generation","text":"","category":"section"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"function generate_parameters(dim_out::Int64, dim_in::Int64, dim_lat::Int64; seed::Int64 = 123)\n    \n    # define noise levels\n    input_noise  = 500.0\n    output_noise = 50.0\n\n    # create random generator for reproducibility\n    rng = MersenneTwister(seed)\n\n    # generate matrices, input statistics and noise matrices\n    A      = diagm(0.8 .* ones(dim_lat) .+ 0.2 * rand(rng, dim_lat))                                            # size (dim_lat x dim_lat)\n    B      = rand(dim_lat, dim_in)                                                                              # size (dim_lat x dim_in)\n    C      = rand(dim_out, dim_lat)                                                                             # size (dim_out x dim_lat)\n    μu     = rand(dim_in) .* collect(1:dim_in)                                                                  # size (dim_in x 1)\n    Σu     = input_noise  .* collect(Hermitian(randn(rng, dim_in, dim_in) + diagm(10 .+ 10*rand(dim_in))))      # size (dim_in x dim_in)\n    Σy     = output_noise .* collect(Hermitian(randn(rng, dim_out, dim_out) + diagm(10 .+ 10*rand(dim_out))))   # size (dim_out x dim_out)\n    Wu     = inv(Σu)\n    Wy     = inv(Σy)\n    \n    # return parameters\n    return A, B, C, μu, Σu, Σy, Wu, Wy\n\nend;","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"function generate_data(nr_samples::Int64, A::Array{Float64,2}, B::Array{Float64,2}, C::Array{Float64,2}, μu::Array{Float64,1}, Σu::Array{Float64,2}, Σy::Array{Float64,2}; seed::Int64 = 123)\n        \n    # create random data generator\n    rng = StableRNG(seed)\n    \n    # preallocate space for variables\n    z = Vector{Vector{Float64}}(undef, nr_samples)\n    y = Vector{Vector{Float64}}(undef, nr_samples)\n    u = rand(rng, MvNormal(μu, Σu), nr_samples)'\n    \n    # set initial value of latent states\n    z_prev = zeros(size(A,1))\n    \n    # generate data\n    for i in 1:nr_samples\n\n        # generate new latent state\n        z[i] = A * z_prev + B * u[i,:]\n\n        # generate new observation\n        y[i] = C * z[i] + rand(rng, MvNormal(zeros(dim_out), Σy))\n        \n        # generate new observation\n        z_prev .= z[i]\n        \n    end\n    \n    # return generated data\n    return z, y, u\n    \nend","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"generate_data (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"# specify settings\nnr_samples = 200\ndim_out = 3\ndim_in = 3\ndim_lat = 25\n\n# generate parameters\nA, B, C, μu, Σu, Σy, Wu, Wy = generate_parameters(dim_out, dim_in, dim_lat);\n            \n# generate data\ndata_z, data_y, data_u = generate_data(nr_samples, A, B, C, μu, Σu, Σy);\n\n# visualise data\np = Plots.plot(xlabel = \"sample\", ylabel = \"observations\")\n# plot each dimension independently\nfor i in 1:dim_out\n    Plots.scatter!(p, getindex.(data_y, i), label = \"y_$i\", alpha = 0.5, ms = 2)\nend\np","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/#Model-specification","page":"RTS vs BIFM Smoothing","title":"Model specification","text":"","category":"section"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"@model function RTS_smoother(nr_samples::Int64, A::Array{Float64,2}, B::Array{Float64,2}, C::Array{Float64,2}, μu::Array{Float64,1}, Wu::Array{Float64,2}, Wy::Array{Float64,2})\n    \n    # fetch dimensionality\n    dim_lat = size(A, 1)\n    dim_out = size(C, 1)\n    \n    # initialize variables\n    z = randomvar(nr_samples)                 # hidden states (random variable)\n    u = randomvar(nr_samples)                 # inputs (random variable)\n    y = datavar(Vector{Float64}, nr_samples)  # outputs (observed variables)\n    \n    # set initial hidden state\n    z_prior ~ MvNormalMeanPrecision(zeros(dim_lat), 1e-5*diagm(ones(dim_lat)))\n    \n    # update last/previous hidden state\n    z_prev = z_prior\n\n    # loop through observations\n    for i in 1:nr_samples\n\n        # specify input as random variable\n        u[i] ~ MvNormalMeanPrecision(μu, Wu)\n        \n        # specify updated hidden state\n        z[i] ~ A * z_prev + B * u[i]\n        \n        # specify observation\n        y[i] ~ MvNormalMeanPrecision(C * z[i], Wy)\n        \n        # update last/previous hidden state\n        z_prev = z[i]\n\n    end\n    \nend","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"@model function BIFM_smoother(nr_samples::Int64, A::Array{Float64,2}, B::Array{Float64,2}, C::Array{Float64,2}, μu::Array{Float64,1}, Wu::Array{Float64,2}, Wy::Array{Float64,2})\n\n    # fetch dimensionality\n    dim_lat = size(A, 1)\n    \n    # initialize variables\n    z  = randomvar(nr_samples)                  # latent states\n    yt = randomvar(nr_samples)                  # latent observations\n    y  = datavar(Vector{Float64}, nr_samples)   # actual observations\n    u  = randomvar(nr_samples)                  # inputs\n    \n    # set priors\n    z_prior ~ MvNormalMeanPrecision(zeros(dim_lat), 1e-5*diagm(ones(dim_lat)))\n    z_tmp   ~ BIFMHelper(z_prior) where { q = MeanField() }\n    \n    # update last/previous hidden state\n    z_prev = z_tmp\n    \n    # loop through observations\n    for i in 1:nr_samples\n\n        # specify input as random variable\n        u[i]   ~ MvNormalMeanPrecision(μu, Wu)\n\n        # specify observation\n        yt[i]  ~ BIFM(u[i], z_prev, z[i]) where { meta = BIFMMeta(A, B, C) }\n        y[i]   ~ MvNormalMeanPrecision(yt[i], Wy)\n        \n        # update last/previous hidden state\n        z_prev = z[i]\n\n    end\n    \n    # set final value\n    z[nr_samples] ~ MvNormalMeanPrecision(zeros(dim_lat), zeros(dim_lat, dim_lat))\n    \nend","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/#Probabilistic-inference","page":"RTS vs BIFM Smoothing","title":"Probabilistic inference","text":"","category":"section"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"function inference_RTS(data_y, A, B, C, μu, Wu, Wy)\n    \n    # In this task the inference is unstable and can diverge\n    meta = @meta begin \n        *() -> ReactiveMP.MatrixCorrectionTools.ClampSingularValues(tiny, Inf)\n    end\n    \n    result = infer(\n        model      = RTS_smoother(length(data_y), A, B, C, μu, Wu, Wy),\n        data       = (y = data_y, ),\n        returnvars = (z = KeepLast(), u = KeepLast()),\n        meta = meta\n    )\n    qs = result.posteriors\n    return (qs[:z], qs[:u])\nend","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"inference_RTS (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"function inference_BIFM(data_y, A, B, C, μu, Wu, Wy)\n    result = infer(\n        model      = BIFM_smoother(length(data_y), A, B, C, μu, Wu, Wy),\n        data       = (y = data_y, ),\n        returnvars = (z = KeepLast(), u = KeepLast())\n    )\n    qs = result.posteriors\n    return (qs[:z], qs[:u])\nend","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"inference_BIFM (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/#Experiments-for-200-observations","page":"RTS vs BIFM Smoothing","title":"Experiments for 200 observations","text":"","category":"section"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"z_RTS, u_RTS = inference_RTS(data_y, A, B, C, μu, Wu, Wy)\nz_BIFM, u_BIFM = inference_BIFM(data_y, A, B, C, μu, Wu, Wy);","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"ax1 = Plots.plot(title = \"RTS smoother\", xlabel = \"sample\", ylabel = \"latent state z\")\nax2 = Plots.plot(title = \"BIFM smoother\", xlabel = \"sample\", ylabel = \"latent state z\")\n\nmz_RTS = mean.(z_RTS)\nmz_BIFM = mean.(z_BIFM)\n\n# Do not plot all latent states, otherwise the output is just too cluttered\n# The main idea here is to check that both algorithms return the (approximately) same output\nfor i in 1:5\n    Plots.scatter!(ax1, getindex.(data_z, i), alpha = 0.1, ms = 2, color = :blue, label = nothing)\n    Plots.plot!(ax1, getindex.(mz_RTS, i), label = nothing)\n    Plots.scatter!(ax2, getindex.(data_z, i), alpha = 0.1, ms = 2, color = :blue, label = nothing)    \n    Plots.plot!(ax2, getindex.(mz_BIFM, i), label = nothing)\nend\n\nPlots.plot(ax1, ax2, layout = @layout([ a; b ]))","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"ax1 = Plots.plot(title = \"RTS smoother\", xlabel = \"sample\", ylabel = \"latent state u\")\nax2 = Plots.plot(title = \"BIFM smoother\", xlabel = \"sample\", ylabel = \"latent state u\")\n\nrdata_u = collect(eachrow(data_u))\nmu_RTS = mean.(u_RTS)\nmu_BIFM = mean.(u_BIFM)\n\n# Do not plot all latent states, otherwise the output is just too cluttered\n# The main idea here is to check that both algorithms return the (approximately) same output\nfor i in 1:1\n    Plots.scatter!(ax1, getindex.(rdata_u, i), alpha = 0.1, ms = 2, color = :blue, label = nothing)\n    Plots.plot!(ax1, getindex.(mu_RTS, i), label = nothing)\n    Plots.scatter!(ax2, getindex.(rdata_u, i), alpha = 0.1, ms = 2, color = :blue, label = nothing)    \n    Plots.plot!(ax2, getindex.(mu_BIFM, i), label = nothing)\nend\n\nPlots.plot(ax1, ax2, layout = @layout([ a; b ]))","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/#Benchmark","page":"RTS vs BIFM Smoothing","title":"Benchmark","text":"","category":"section"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"# This example runs in our documentation pipeline, benchmark executes approximatelly in 20 minutes so we bypass it in the documentation\n# For those who are interested in exact benchmark numbers clone this example and set `run_benchmark = true`\nrun_benchmark = false\n\nif run_benchmark\n    trials_range = 30\n    trials_n = 500\n    trials_RTS  = Array{BenchmarkTools.Trial, 1}(undef, trials_range)\n    trials_BIFM = Array{BenchmarkTools.Trial, 1}(undef, trials_range)\n\n\n    @showprogress for k = 1 : trials_range\n\n        # generate parameters\n        local A, B, C, μu, Σu, Σy, Wu, Wy = generate_parameters(3, 3, k);\n                    \n        # generate data|\n        local data_z, data_y, data_u = generate_data(trials_n, A, B, C, μu, Σu, Σy);\n\n        # run inference\n        trials_RTS[k] = @benchmark inference_RTS($data_y, $A, $B, $C, $μu, $Wu, $Wy)\n        trials_BIFM[k] = @benchmark inference_BIFM($data_y, $A, $B, $C, $μu, $Wu, $Wy)\n\n    end\n\n    m_RTS = [median(trials_RTS[k].times) for k=1:trials_range] ./ 1e9\n    q1_RTS = [quantile(trials_RTS[k].times, 0.25) for k=1:trials_range] ./ 1e9\n    q3_RTS = [quantile(trials_RTS[k].times, 0.75) for k=1:trials_range] ./ 1e9\n    m_BIFM = [median(trials_BIFM[k].times) for k=1:trials_range] ./ 1e9\n    q1_BIFM = [quantile(trials_BIFM[k].times, 0.25) for k=1:trials_range] ./ 1e9\n    q3_BIFM = [quantile(trials_BIFM[k].times, 0.75) for k=1:trials_range] ./ 1e9;\n\n    p = Plots.plot(ylabel = \"duration [sec]\", xlabel = \"latent state dimension\", title = \"Benchmark\", yscale = :log)\n    p = Plots.plot!(p, m_RTS, ribbon = ((q1_RTS .- q3_RTS) ./ 2), color = \"blue\", label = \"mean (RTS)\")\n    p = Plots.plot!(p, 1:trials_range, m_BIFM, ribbon = ((q1_BIFM .- q3_BIFM) ./ 2), color = \"orange\", label = \"mean (BIFM)\")\n    Plots.savefig(p, \"../pics/rts_bifm_benchmark.png\")\n    p\nend","category":"page"},{"location":"examples/problem_specific/RTS vs BIFM Smoothing/","page":"RTS vs BIFM Smoothing","title":"RTS vs BIFM Smoothing","text":"(Image: )","category":"page"},{"location":"manuals/meta-specification/#user-guide-meta-specification","page":"Meta specification","title":"Meta Specification","text":"","category":"section"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"Some nodes in the RxInfer.jl inference engine require a meta structure that may be used to either provide additional information to the nodes or customise the inference procedure or the way the nodes compute outbound messages. For instance, the AR node, modeling the Auto-Regressive process, necessitates knowledge of the order of the AR process, or the GCV node (Gaussian Controlled Variance) needs an approximation method to handle non-conjugate relationships between variables in this node. To facilitate these requirements, RxInfer.jl exports @meta macro to specify node-specific meta and contextual information.","category":"page"},{"location":"manuals/meta-specification/#General-syntax","page":"Meta specification","title":"General syntax","text":"","category":"section"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"@meta macro accepts either regular Julia function or a single begin ... end block:","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"@meta function MyModelMeta(arg1, arg2)\n    Node(y,x) -> MetaObject(arg1, arg2)\nend\n\nmy_meta = @meta begin \n    Node(y,x) -> MetaObject(arg1, arg2)\nend","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"In the first case it returns a function that returns meta upon calling. For example, the meta for the AR node of order 5 can be specified as follows:","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"@meta function ARmodel_meta(num_order)\n    AR() -> ARMeta(Univariate, num_order, ARsafe())\nend\n\nmy_meta = ARmodel_meta(5)","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"In the second case it returns the meta object directly. For example, the same meta for the AR node can also be defined as follows:","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"num_order = 5\n\nmy_meta = @meta begin \n    AR() -> ARMeta(Multivariate, num_order, ARsafe())\nend","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"Both syntax variations provide the same meta specification and there is no preference given to one over the other. ","category":"page"},{"location":"manuals/meta-specification/#Options-specification","page":"Meta specification","title":"Options specification","text":"","category":"section"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"@meta macro accepts optional list of options as a first argument and specified as an array of key = value pairs, e.g. ","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"my_meta = @meta [ warn = false ] begin \n   ...\nend\n\n@meta [ warn = false ] function MyModelMeta()\n    ...\nend","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"List of available options:","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"warn::Bool - enables/disables various warnings with an incompatible model/meta specification","category":"page"},{"location":"manuals/meta-specification/#Meta-specification","page":"Meta specification","title":"Meta specification","text":"","category":"section"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"First, let's start with an example:","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"my_meta = @meta begin \n    GCV(x, k, w) -> GCVMetadata(GaussHermiteCubature(20))\nend","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"This meta specification indicates that for every GCV node in the model with x, k and w as connected variables should use the GCVMetadata(GaussHermiteCubature(20)) meta object.","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"You can have a list of as many meta specification entries as possible for different nodes:","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"my_meta = @meta begin \n    GCV(x1, k1, w1) -> GCVMetadata(GaussHermiteCubature(20))\n    AR() -> ARMeta(Multivariate, 5, ARsafe())\n    MyCustomNode() -> MyCustomMetaObject(arg1, arg2)\nend","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"To create a model with meta structures the user may pass an optional meta keyword argument for the create_model function:","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"@model function my_model(arguments...)\n   ...\nend\n\nmy_meta = @meta begin \n    ...\nend\n\nmodel, returnval = create_model(my_model(arguments...); meta = my_meta)","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"Alternatively, it is possible to use meta directly in the automatic infer function that accepts meta keyword argument:","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"inferred_result = infer(\n    model = my_model(arguments...),\n    meta = my_meta,\n    ...\n)","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"Note: You can also specify metadata for your nodes directly inside @model, without the need to use @meta. For example:","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"@model function my_model()\n    ...\n\n    y ~ AR(x, θ, γ) where { meta = ARMeta(Multivariate, 5, ARsafe()) }\n\n    ...\nend","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"If you add node-specific meta to your model this way, then you do not need to use the meta keyword argument in the infer function.","category":"page"},{"location":"manuals/meta-specification/#Create-your-own-meta","page":"Meta specification","title":"Create your own meta","text":"","category":"section"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"Although some nodes in RxInfer.jl already come with their own meta structure, you have the flexibility to define different meta structures for those nodes and also for your custorm ones. A meta structure is created by using the struct statement in Julia. For example, the following snippet of code illustrates how you can create your own meta structures for your custom node:","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"# create your own meta structure for your custom node\nstruct MyCustomMeta\n    arg1 \n    arg2 \nend\n\n# apply the new meta structure to your node\n@meta function model_meta(arg1, arg2)\n    MyCustomNode() -> MyCustomMeta(arg1,arg2)\nend\n\nmy_meta = model_meta(value_arg1, value_arg2)","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"or create different meta structures for a node, e.g. AR node:","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"# create your own meta structure for the AR node\nstruct MyARMeta\n    arg1\n    arg2\nend\n\n# apply the new meta structure to the AR node\n@meta function model_meta(arg1, arg2)\n    AR() -> MyARMeta(arg1, arg2)\nend\n\nmy_meta = model_meta(value_arg1, value_arg2)","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"Note: When you define a meta structure for a node, keep in mind that you must define message update rules with the meta for that node. See node manual for more details of how to define rules for a node.","category":"page"},{"location":"manuals/meta-specification/#Example","page":"Meta specification","title":"Example","text":"","category":"section"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"This section provides a concrete example of how to create and use meta in RxInfer.jl. Suppose that we have the following Gaussian model:","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"beginaligned\n x  sim mathrmNormal(25 05)\n y  sim mathrmNormal(2*x 20)\nendaligned","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"where y is observable data and x is a latent variable. In RxInfer.jl, the inference procedure for this model is well defined without the need of specifying any meta data for the Normal node.","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"using RxInfer\n\n#create data\ny_data = 4.0 \n\n#make model\n@model function gaussian_model()\n    y = datavar(Float64)\n\n    x ~ NormalMeanVariance(2.5, 0.5)\n    y ~ NormalMeanVariance(2*x, 2.)\nend\n\n#do inference\ninference_result = infer(\n    model = gaussian_model(),\n    data = (y = y_data,)\n)","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"However, let's say we would like to experiment with message update rules and define a new inference procedure by introducing a meta structure to the Normal node that always yields a Normal distribution with mean m clamped between lower_limit and upper_limit for the outbound messages of the node. This is done as follows:","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"#create your new meta structure for Normal node\nstruct MetaConstrainedMeanNormal{T}\n    lower_limit :: T\n    upper_limit :: T\nend\n\n#define rules with meta for the Normal node\n@rule NormalMeanVariance(:out, Marginalisation) (q_μ::Any, q_v::Any, meta::MetaConstrainedMeanNormal) = begin\n    return NormalMeanVariance(clamp(mean(q_μ), meta.lower_limit, meta.upper_limit), mean(q_v))\nend\n\n@rule NormalMeanVariance(:μ, Marginalisation) (q_out::Any, q_v::Any, meta::MetaConstrainedMeanNormal) = begin\n    return NormalMeanVariance(clamp(mean(q_out), meta.lower_limit, meta.upper_limit), mean(q_v))\nend","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"#make model\n@model function gaussian_model_with_meta()\n    y = datavar(Float64)\n\n    x ~ NormalMeanVariance(2.5, 0.5)\n    y ~ NormalMeanVariance(2*x, 2.)\nend\n\ncustom_meta = @meta begin\n    NormalMeanVariance(y,x) -> MetaConstrainedMeanNormal(-1, 1)\nend\n\n#do inference\ninference_result = inference(\n    model = gaussian_model(),\n    data = (y = y_data,),\n    meta = custom_meta\n)","category":"page"},{"location":"manuals/meta-specification/","page":"Meta specification","title":"Meta specification","text":"Disclaimer: The above example is not mathematically correct. It is only used to show how we can work with @meta as well as how to create a meta structure for a node in RxInfer.jl.","category":"page"},{"location":"manuals/inference/overview/#user-guide-inference-execution","page":"Overview","title":"Inference execution","text":"","category":"section"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"The RxInfer inference API supports different types of message-passing algorithms (including hybrid algorithms combining several different types):","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"Belief Propagation\nVariational Message Passing","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"Whereas belief propagation computes exact inference for the random variables of interest, the variational message passing (VMP) in an approximation method that can be applied to a larger range of models.","category":"page"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"The inference engine itself isn't aware of different algorithm types and simply does message passing between nodes, however during model specification stage user may specify different factorisation constraints around factor nodes by using where { q = ... } syntax or with the help of the @constraints macro. Different factorisation constraints lead to a different message passing update rules. See more documentation about constraints specification in the corresponding section.","category":"page"},{"location":"manuals/inference/overview/#user-guide-inference-execution-automatic-specification-static","page":"Overview","title":"Automatic inference specification on static datasets","text":"","category":"section"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"RxInfer exports the infer function to quickly run and test you model with static datasets. See more information about the infer function on the separate documentation section. ","category":"page"},{"location":"manuals/inference/overview/#user-guide-inference-execution-automatic-specification-realtime","page":"Overview","title":"Automatic inference specification on real-time datasets","text":"","category":"section"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"RxInfer supports running inference the with dynamic and potentially real-time datasets with enabled autoupdates keyword. See more information about the infer function on the separate documentation section. ","category":"page"},{"location":"manuals/inference/overview/#user-guide-inference-execution-manual-specification","page":"Overview","title":"Manual inference specification","text":"","category":"section"},{"location":"manuals/inference/overview/","page":"Overview","title":"Overview","text":"While infer uses most of the RxInfer inference engine capabilities in some situations it might be beneficial to write inference code manually. The Manual inference documentation section explains how to write your custom inference routines.","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/#examples-chance-constrained-active-inference","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"","category":"section"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"This notebook applies reactive message passing for active inference in the context of chance-constraints. The implementation is based on (van de Laar et al., 2021, \"Chance-constrained active inference\") and discussion with John Boik.","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"We consider a 1-D agent that tries to elevate itself above ground level. Instead of a goal prior, we impose a chance constraint on future states, such that the agent prefers to avoid the ground with a preset probability (chance) level. ","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"using Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"using Plots, Distributions, StatsFuns, RxInfer","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/#Chance-Constraint-Node-Definition","page":"Chance-Constrained Active Inference","title":"Chance-Constraint Node Definition","text":"","category":"section"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"A chance-constraint is meant to constraint a marginal distribution to abide by certain properties. In this case, a (posterior) probability distribution should not \"overflow\" a given region by more than a certain probability mass. This constraint then affects adjacent beliefs and ultimately the controls to (hopefully) account for the imposed constraint.","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"In order to enforce this constraint on a marginal distribution, an auxiliary chance-constraint node is included in the graphical model. This node then sends messages that enforce the marginal to abide by the preset conditions. In other words, the (chance) constraint on the (posterior) marginal, is converted to a prior constraint on the generative model that sends an adaptive message. We start by defining this chance-constraint node and its message.","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"struct ChanceConstraint end  \n\n# Node definition with safe region limits (lo, hi), overflow chance epsilon and tolerance atol\n@node ChanceConstraint Stochastic [out, lo, hi, epsilon, atol]","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"# Function to compute normalizing constant and central moments of a truncated Gaussian distribution\nfunction truncatedGaussianMoments(m::Float64, V::Float64, a::Float64, b::Float64)\n    V = clamp(V, tiny, huge)\n    StdG = Distributions.Normal(m, sqrt(V))\n    TrG = Distributions.Truncated(StdG, a, b)\n    \n    Z = Distributions.cdf(StdG, b) - Distributions.cdf(StdG, a)  # safe mass for standard Gaussian\n    \n    if Z < tiny\n        # Invalid region; return undefined mean and variance of truncated distribution\n        Z    = 0.0\n        m_tr = 0.0\n        V_tr = 0.0\n    else\n        m_tr = Distributions.mean(TrG)\n        V_tr = Distributions.var(TrG)\n    end\n    \n    return (Z, m_tr, V_tr)\nend;","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"@rule ChanceConstraint(:out, Marginalisation) (\n    m_out::UnivariateNormalDistributionsFamily, # Require inbound message\n    q_lo::PointMass, \n    q_hi::PointMass, \n    q_epsilon::PointMass, \n    q_atol::PointMass) = begin \n\n    # Extract parameters\n    lo = mean(q_lo)\n    hi = mean(q_hi)\n    epsilon = mean(q_epsilon)\n    atol = mean(q_atol)\n    \n    (m_bw, V_bw) = mean_var(m_out)\n    (xi_bw, W_bw) = (m_bw, 1. /V_bw)  # check division by  zero\n    (m_tilde, V_tilde) = (m_bw, V_bw)\n    \n    # Compute statistics (and normalizing constant) of q in safe region G\n    # Phi_G is called the \"safe mass\" \n    (Phi_G, m_G, V_G) = truncatedGaussianMoments(m_bw, V_bw, lo, hi)\n\n    xi_fw = xi_bw\n    W_fw  = W_bw\n    if epsilon <= 1.0 - Phi_G # If constraint is active\n        # Initialize statistics of uncorrected belief\n        m_tilde = m_bw\n        V_tilde = V_bw\n        for i = 1:100 # Iterate at most this many times\n            (Phi_lG, m_lG, V_lG) = truncatedGaussianMoments(m_tilde, V_tilde, -Inf, lo) # Statistics for q in region left of G\n            (Phi_rG, m_rG, V_rG) = truncatedGaussianMoments(m_tilde, V_tilde, hi, Inf) # Statistics for q in region right of G\n\n            # Compute moments of non-G region as a mixture of left and right truncations\n            Phi_nG = Phi_lG + Phi_rG\n            m_nG = Phi_lG / Phi_nG * m_lG + Phi_rG / Phi_nG * m_rG\n            V_nG = Phi_lG / Phi_nG * (V_lG + m_lG^2) + Phi_rG/Phi_nG * (V_rG + m_rG^2) - m_nG^2\n\n            # Compute moments of corrected belief as a mixture of G and non-G regions\n            m_tilde = (1.0 - epsilon) * m_G + epsilon * m_nG\n            V_tilde = (1.0 - epsilon) * (V_G + m_G^2) + epsilon * (V_nG + m_nG^2) - m_tilde^2\n            # Re-compute statistics (and normalizing constant) of corrected belief\n            (Phi_G, m_G, V_G) = truncatedGaussianMoments(m_tilde, V_tilde, lo, hi)\n            if (1.0 - Phi_G) < (1.0 + atol)*epsilon\n                break # Break the loop if the belief is sufficiently corrected\n            end\n        end\n        \n        # Convert moments of corrected belief to canonical form\n        W_tilde = inv(V_tilde)\n        xi_tilde = W_tilde * m_tilde\n\n        # Compute canonical parameters of forward message\n        xi_fw = xi_tilde - xi_bw\n        W_fw  = W_tilde - W_bw\n    end\n\n    return NormalWeightedMeanPrecision(xi_fw, W_fw)\nend","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/#Definition-of-the-Environment","page":"Chance-Constrained Active Inference","title":"Definition of the Environment","text":"","category":"section"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"We consider an environment where the agent has an elevation level, and where the agent directly controls its vertical velocity. After some time, an unexpected and sudden gust of wind tries to push the agent to the ground.","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"wind(t::Int64) = -0.1*(60 <= t < 100) # Time-dependent wind profile\n\nfunction initializeWorld()\n    x_0 = 0.0 # Initial elevation\n    \n    x_t_last = x_0\n    function execute(t::Int64, a_t::Float64)\n        x_t = x_t_last + a_t + wind(t) # Update elevation\n    \n        x_t_last = x_t # Reset state\n                \n        return x_t\n    end\n\n    x_t = x_0 # Predefine outcome variable\n    observe() = x_t # State is fully observed\n\n    return (execute, observe)\nend;","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/#Generative-Model-for-Regulator","page":"Chance-Constrained Active Inference","title":"Generative Model for Regulator","text":"","category":"section"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"We consider a fully observed Markov decision process, where the agent directly observes the true state (elevation) of the world. In this case we only need to define a chance-constrained generative model of future states. Inference for controls on this model then derives our controller.","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"@model function regulator_model(; T, lo, hi, epsilon, atol)\n    # Fully observed state\n    x_t = datavar(Float64)\n\n    # Control prior statistics\n    m_u = datavar(Float64, T)\n    v_u = datavar(Float64, T)    \n        \n    # Random variables\n    u = randomvar(T) # Control\n    x = randomvar(T) # Elevation\n    \n    # Loop over horizon\n    x_k_last = x_t\n    for k = 1:T\n        u[k] ~ NormalMeanVariance(m_u[k], v_u[k]) # Control prior\n        x[k] ~ x_k_last + u[k] # Transition model\n        x[k] ~ ChanceConstraint(lo, hi, epsilon, atol) where { # Simultaneous constraint on state\n            pipeline = RequireMessage(out = NormalWeightedMeanPrecision(0, 0.01))} # Predefine inbound message to break circular dependency\n        x_k_last = x[k]\n    end\n    \n    return (u, x)\nend","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/#Reactive-Agent-Definition","page":"Chance-Constrained Active Inference","title":"Reactive Agent Definition","text":"","category":"section"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"function initializeAgent()\n    # Set control prior statistics\n    m_u = zeros(T)\n    v_u = lambda^(-1)*ones(T)\n    \n    function compute(x_t::Float64)\n        model_t = regulator_model(; T=T, lo=lo, hi=hi, epsilon=epsilon, atol=atol)\n        data_t = (m_u = m_u, v_u = v_u, x_t = x_t)\n        \n        result = infer(\n            model = model_t, \n            data = data_t,\n            iterations = n_its)\n\n        # Extract policy from inference results\n        pol = mode.(result.posteriors[:u][end])\n\n        return pol\n    end\n\n    pol = zeros(T) # Predefine policy variable\n    act() = pol[1]\n\n    return (compute, act)\nend;","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/#Action-Perception-Cycle","page":"Chance-Constrained Active Inference","title":"Action-Perception Cycle","text":"","category":"section"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"Next we define and execute the action-perception cycle. Because the state is fully observed, these is no slide (estimator) step in the cycle. ","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"# Simulation parameters\nN = 160 # Total simulation time\nT = 1 # Lookahead time horizon\nlambda = 1.0 # Control prior precision\nlo = 1.0 # Chance region lower bound\nhi = Inf # Chance region upper bound\nepsilon = 0.01 # Allowed chance violation\natol = 0.01 # Convergence tolerance for chance constraints\nn_its = 10;  # Number of inference iterations","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"(execute, observe) = initializeWorld() # Let there be a world\n(compute, act) = initializeAgent() # Let there be an agent\n\na = Vector{Float64}(undef, N) # Actions\nx = Vector{Float64}(undef, N) # States\nfor t = 1:N\n    a[t] = act()\n           execute(t, a[t])\n    x[t] = observe()\n           compute(x[t])\nend","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/#Results","page":"Chance-Constrained Active Inference","title":"Results","text":"","category":"section"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"Results show that the agent does not allow the wind to push it all the way to the ground.","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"p1 = plot(1:N, wind.(1:N), color=\"blue\", label=\"Wind\", ylabel=\"Velocity\", lw=2)\nplot!(p1, 1:N, a, color=\"red\", label=\"Control\", lw=2)\np2 = plot(1:N, x, color=\"black\", lw=2, label=\"Agent\", ylabel=\"Elevation\")\nplot(p1, p2, layout=(2,1))","category":"page"},{"location":"examples/advanced_examples/Chance Constraints/","page":"Chance-Constrained Active Inference","title":"Chance-Constrained Active Inference","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/#examples-gamma-mixture-model","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"","category":"section"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"This notebook implements one of the experiments outlined in https://biaslab.github.io/publication/mp-based-inference-in-gmm/.","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/#Load-packages","page":"Gamma Mixture Model","title":"Load packages","text":"","category":"section"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"using RxInfer, Random, StatsPlots","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"# create custom structure for model parameters for simplicity\nstruct GammaMixtureModelParameters\n    nmixtures   # number of mixtures\n    priors_as   # tuple of priors for variable a\n    priors_bs   # tuple of priors for variable b\n    prior_s     # prior of variable s\nend","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/#Model-specification","page":"Gamma Mixture Model","title":"Model specification","text":"","category":"section"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"@model function gamma_mixture_model(nobservations, parameters::GammaMixtureModelParameters)\n\n    # fetch information from struct\n    nmixtures = parameters.nmixtures\n    priors_as = parameters.priors_as\n    priors_bs = parameters.priors_bs\n    prior_s   = parameters.prior_s\n\n    # set prior on global selection variable\n    s ~ Dirichlet(probvec(prior_s))\n\n    # allocate vectors of random variables\n    as = randomvar(nmixtures)\n    bs = randomvar(nmixtures)\n\n    # set priors on variables of mixtures\n    for i in 1:nmixtures\n        as[i] ~ Gamma(shape = shape(priors_as[i]), rate = rate(priors_as[i]))\n        bs[i] ~ Gamma(shape = shape(priors_bs[i]), rate = rate(priors_bs[i]))\n    end\n\n    # introduce random variables for local selection variables and data\n    z = randomvar(nobservations)\n    y = datavar(Float64, nobservations)\n\n    # convert vector to tuples for proper functioning of GammaMixture node\n    tas = tuple(as...)\n    tbs = tuple(bs...)\n\n    # specify local selection variable and data generating process\n    for i in 1:nobservations\n        z[i] ~ Categorical(s)\n        y[i] ~ GammaMixture(z[i], tas, tbs)\n    end\n    \nend","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"constraints = @constraints begin \n\n    q(z, as, bs, s) = q(z)q(as)q(bs)q(s)\n\n    q(as) = q(as[begin])..q(as[end])\n    q(bs) = q(bs[begin])..q(bs[end])\n    \n    q(as) :: PointMass(starting_point = (args...) -> [ 1.0 ])\nend","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"Constraints:\n  marginals form:\n    q(as) :: PointMassFormConstraint() [ prod_constraint = GenericProd() ]\n  messages form:\n  factorisation:\n    q(z, as, bs, s) = q(z)q(as)q(bs)q(s)\n    q(as) = q(as[(begin)..(end)])\n    q(bs) = q(bs[(begin)..(end)])\nOptions:\n  warn = true","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"# specify seed and number of data points\nrng = MersenneTwister(43)\nn_samples = 2500\n\n# specify parameters of mixture model that generates the data\n# Note that mixture components have exactly the same means\nmixtures  = [ Gamma(9.0, inv(27.0)), Gamma(90.0, inv(270.0)) ]\nnmixtures = length(mixtures)\nmixing    = rand(rng, nmixtures)\nmixing    = mixing ./ sum(mixing)\nmixture   = MixtureModel(mixtures, mixing)\n\n# generate data set\ndataset = rand(rng, mixture, n_samples);","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"# specify priors of probabilistic model\n# NOTE: As the means of the mixtures \"collide\", we specify informative prior for selector variable\nnmixtures = 2\ngpriors = GammaMixtureModelParameters(\n    nmixtures,                                                    # number of mixtures\n    [ Gamma(1.0, 0.1), Gamma(1.0, 1.0) ],                         # priors on variables a\n    [ GammaShapeRate(10.0, 2.0), GammaShapeRate(1.0, 3.0) ],      # priors on variables b\n    Dirichlet(1e3*mixing)                                         # prior on variable s\n)\n\ngmodel         = gamma_mixture_model(length(dataset), gpriors)\ngdata          = (y = dataset, )\nginitmarginals = (s = gpriors.prior_s, z = vague(Categorical, gpriors.nmixtures), bs = GammaShapeRate(1.0, 1.0))\ngreturnvars    = (s = KeepLast(), z = KeepLast(), as = KeepEach(), bs = KeepEach())\n\ngoptions = (\n     \n    default_factorisation = MeanField() # Mixture models require Mean-Field assumption currently\n)\n\ngresult = infer(\n    model         = gmodel, \n    data          = gdata,\n    constraints   = constraints,\n    options       = (limit_stack_depth = 100,),\n    initmarginals = ginitmarginals,\n    returnvars    = greturnvars,\n    free_energy   = true,\n    iterations    = 250, \n    showprogress  = true\n);","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"# extract inferred parameters\n_as, _bs = mean.(gresult.posteriors[:as][end]), mean.(gresult.posteriors[:bs][end])\n_dists   = map(g -> Gamma(g[1], inv(g[2])), zip(_as, _bs))\n_mixing = mean(gresult.posteriors[:s])\n\n# create model from inferred parameters\n_mixture   = MixtureModel(_dists, _mixing);","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"# report on outcome of inference\nprintln(\"Generated means: $(mean(mixtures[1])) and $(mean(mixtures[2]))\")\nprintln(\"Inferred means: $(mean(_dists[1])) and $(mean(_dists[2]))\")\nprintln(\"========\")\nprintln(\"Generated mixing: $(mixing)\")\nprintln(\"Inferred mixing: $(_mixing)\")","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"Generated means: 0.3333333333333333 and 0.33333333333333337\nInferred means: 0.33822470398804994 and 0.33305534672666653\n========\nGenerated mixing: [0.18923488676601088, 0.8107651132339891]\nInferred mixing: [0.11395888467453213, 0.8860411153254679]","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"# plot results\np1 = histogram(dataset, ylim = (0, 13), xlim = (0, 1), normalize=:pdf, label=\"data\", opacity=0.3)\np1 = plot!(mixture, label=false, title=\"Generated mixtures\", linewidth=3.0)\n\np2 = histogram(dataset, ylim = (0, 13), xlim = (0, 1), normalize=:pdf, label=\"data\", opacity=0.3)\np2 = plot!(_mixture, label=false, title=\"Inferred mixtures\", linewidth=3.0)\n\n# evaluate the convergence of the algorithm by monitoring the BFE\np3 = plot(gresult.free_energy, label=false, xlabel=\"iterations\", title=\"Bethe FE\")\n\nplot(plot(p1, p2, layout = @layout([ a; b ])), plot(p3), layout = @layout([ a b ]), size = (800, 400))","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Gamma Mixture/","page":"Gamma Mixture Model","title":"Gamma Mixture Model","text":"","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/#examples-gaussian-mixture","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"","category":"section"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"This notebook illustrates how to use the NormalMixture node in RxInfer.jl for both univariate and multivariate observations.","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/#Load-packages","page":"Gaussian Mixture","title":"Load packages","text":"","category":"section"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"using RxInfer, Plots, Random, LinearAlgebra, StableRNGs, LaTeXStrings","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/#Univariate-Gaussian-Mixture-Model","page":"Gaussian Mixture","title":"Univariate Gaussian Mixture Model","text":"","category":"section"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"Consider the data set of length N observed below.","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"function generate_univariate_data(nr_samples; rng = MersenneTwister(123))\n\n    # data generating parameters\n    class        = [1/3, 2/3]\n    mean1, mean2 = -10, 10\n    precision    = 1.777\n\n    # generate data\n    z = rand(rng, Categorical(class), nr_samples)\n    y = zeros(nr_samples)\n    for k in 1:nr_samples\n        y[k] = rand(rng, Normal(z[k] == 1 ? mean1 : mean2, 1/sqrt(precision)))\n    end\n\n    return y\n\nend;","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"data_univariate = generate_univariate_data(100)\nhistogram(data_univariate, bins=50, label=\"data\", normed=true)\nxlims!(minimum(data_univariate), maximum(data_univariate))\nylims!(0, Inf)\nylabel!(\"relative occurrence [%]\")\nxlabel!(\"y\")","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/#Model-specification","page":"Gaussian Mixture","title":"Model specification","text":"","category":"section"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"The goal here is to create a model for the data set above. In this case a Gaussian mixture model with K components seems to suite the situation well. We specify the factorized model as  p(y z s m w) = prod_n=1^N bigg(p(y_n mid m w z_n) p(z_n mid s) bigg)prod_k=1^K bigg(p(m_k) p(w_k) bigg) p(s) where the individual terms are specified as beginaligned     p(s)                    = mathrmBeta(s mid alpha_s beta_s) \n    p(m_k)                = mathcalN(m_k mid mu_k sigma_k^2)          p(w_k)                = Gamma(w_k mid alpha_k beta_k) \n    p(z_n mid s)           = mathrmBer(z_n mid s) \n    p(y_n mid m w z_n)   = prod_k=1^K mathcalNleft(y_n mid m_k w_kright)^z_nk endaligned","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"The set of observations y = y_1 y_2 ldots y_N is modeled by a mixture of Gaussian distributions, parameterized by means m = m_1 m_2 ldots m_K and precisions w =  w_1 w_2 ldots w_K, where k denotes the component index. This component is selected per observation by the indicator variable z_n, which is a one-of-K encoded vector satisfying sum_k=1^K z_nk = 1 and z_nk in 0 1 forall k. We put a hyperprior on these variables, termed s, which represents the relative occurrence of the different realizations of z_n.","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"Here we implement the following model with uninformative values for the hyperparameters as","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"@model function univariate_gaussian_mixture_model(nr_samples)\n    \n    s ~ Beta(1.0, 1.0)\n    \n    m1 ~ Normal(mean = -2.0, var = 1e3)\n    w1 ~ Gamma(shape = 0.01, rate = 0.01)\n    \n    m2 ~ Normal(mean = 2.0, var = 1e3)\n    w2 ~ Gamma(shape = 0.01, rate = 0.01)\n    \n    z = randomvar(nr_samples)\n    y = datavar(Float64, nr_samples)\n    \n    for n in 1:nr_samples\n        z[n] ~ Bernoulli(s)\n        y[n] ~ NormalMixture(z[n], (m1, m2), (w1, w2))\n    end\n    \nend","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/#Probabilistic-inference","page":"Gaussian Mixture","title":"Probabilistic inference","text":"","category":"section"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"In order to fit the model to the data, we are interested in computing the posterior distribution p(z s m w mid y) However, computation of this term is intractable. Therefore, it is approximated by a naive mean-field approximation, specified as  p(z s m w mid y) approx prod_n=1^N q(z_n) prod_k=1^K bigg(q(m_k) q(w_k)bigg) q(s) with the functional forms beginaligned     q(s)   = mathrmBeta(s mid hatalpha_s hatbeta_s) \n    q(m_k) = mathcalN(m_k mid hatmu_k hatsigma^2_k) \n    q(w_k) = Gamma (w_k mid hatalpha_k hatbeta_k) \n    q(z_n) = mathrmBer(z_n mid hatp_n) endaligned In order to get the inference procedure started, these marginal distribution need to be initialized.","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"results_univariate = infer(\n    model = univariate_gaussian_mixture_model(length(data_univariate)), \n    constraints = MeanField(),\n    data  = (y = data_univariate,), \n    initmarginals = (\n        s  = vague(Beta), \n        m1 = NormalMeanVariance(-2.0, 1e3), \n        m2 = NormalMeanVariance(2.0, 1e3), \n        w1 = vague(GammaShapeRate), \n        w2 = vague(GammaShapeRate)\n    ), \n    iterations  = 10, \n    free_energy = true\n)","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"Inference results:\n  Posteriors       | available for (m2, m1, s, w2, w1, z)\n  Free Energy:     | Real[360.857, 226.858, 161.566, 135.301, 135.277, 135.\n277, 135.277, 135.277, 135.277, 135.277]","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/#Results","page":"Gaussian Mixture","title":"Results","text":"","category":"section"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"Below the inference results can be seen as a function of the iterations","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"mp = plot(mean.(results_univariate.posteriors[:m1]), ribbon = std.(results_univariate.posteriors[:m1]) .|> sqrt, label = L\"posterior $m_1$\")\nmp = plot!(mean.(results_univariate.posteriors[:m2]), ribbon = std.(results_univariate.posteriors[:m2]) .|> sqrt, label = L\"posterior $m_2$\")\nmp = plot!(mp, [ -10 ], seriestype = :hline, label = L\"true $m_1$\")\nmp = plot!(mp, [ 10 ], seriestype = :hline, label = L\"true $m_2$\")\n\nwp = plot(mean.(results_univariate.posteriors[:w1]), ribbon = std.(results_univariate.posteriors[:w1]) .|> sqrt, label = L\"posterior $w_1$\", legend = :bottomright, ylim = (-1, 3))\nwp = plot!(wp, mean.(results_univariate.posteriors[:w2]), ribbon = std.(results_univariate.posteriors[:w2]) .|> sqrt, label = L\"posterior $w_2$\")\nwp = plot!(wp, [ 1.777 ], seriestype = :hline, label = L\"true $w_1$\")\nwp = plot!(wp, [ 1.777 ], seriestype = :hline, label = L\"true $w_2$\")\n\nswp = plot(mean.(results_univariate.posteriors[:s]), ribbon = std.(results_univariate.posteriors[:s]) .|> sqrt, label = L\"posterior $s$\")\nswp = plot!(swp, [ 2/3 ], seriestype = :hline, label = L\"true $s$\")\n\nfep = plot(results_univariate.free_energy, label = \"Free Energy\", legend = :topright)\n\nplot(mp, wp, swp, fep, layout = @layout([ a b; c d ]), size = (800, 400))\nxlabel!(\"iteration\")","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/#Multivariate-Gaussian-Mixture-Model","page":"Gaussian Mixture","title":"Multivariate Gaussian Mixture Model","text":"","category":"section"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"The above example can also be extended to the multivariate case. Consider the data set below","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"function generate_multivariate_data(nr_samples; rng = MersenneTwister(123))\n\n    L         = 50.0\n    nr_mixtures = 6\n\n    probvec = normalize!(ones(nr_mixtures), 1)\n\n    switch = Categorical(probvec)\n\n    gaussians = map(1:nr_mixtures) do index\n        angle      = 2π / nr_mixtures * (index - 1)\n        basis_v    = L * [ 1.0, 0.0 ]\n        R          = [ cos(angle) -sin(angle); sin(angle) cos(angle) ]\n        mean       = R * basis_v \n        covariance = Matrix(Hermitian(R * [ 10.0 0.0; 0.0 20.0 ] * transpose(R)))\n        return MvNormal(mean, covariance)\n    end\n\n    z = rand(rng, switch, nr_samples)\n    y = Vector{Vector{Float64}}(undef, nr_samples)\n\n    for n in 1:nr_samples\n        y[n] = rand(rng, gaussians[z[n]])\n    end\n\n    return y\n\nend;","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"data_multivariate = generate_multivariate_data(500)\n\nsdim(n) = (a) -> map(d -> d[n], a) # helper function\nscatter(data_multivariate |> sdim(1), data_multivariate |> sdim(2), ms = 2, alpha = 0.4, size = (600, 400), legend=false)\nxlabel!(L\"y_1\")\nylabel!(L\"y_2\")","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/#Model-specification-2","page":"Gaussian Mixture","title":"Model specification","text":"","category":"section"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"The goal here is to create a model for the data set above. In this case a Gaussian mixture model with K components seems to suite the situation well. We specify the factorized model as  p(y z s m w) = prod_n=1^N bigg(p(y_n mid m W z_n) p(z_n mid s) bigg)prod_k=1^K bigg(p(m_k) p(W_k) bigg) p(s) where the individual terms are specified as beginaligned     p(s)                    = mathrmDir(s mid alpha_s) \n    p(m_k)                = mathcalN(m_k mid mu_k Sigma_k)          p(W_k)                = mathcalW(W_k mid V_k nu_k) \n    p(z_n mid s)           = mathrmCat(z_n mid s) \n    p(y_n mid m W z_n)   = prod_k=1^K mathcalNleft(y_n mid m_k W_kright)^z_nk endaligned","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"The set of observations y = y_1 y_2 ldots y_N is modeled by a mixture of Gaussian distributions, parameterized by means m = m_1 m_2 ldots m_K and precisions W =  W_1 W_2 ldots W_K, where k denotes the component index. This component is selected per observation by the indicator variable z_n, which is a one-of-K encoded vector satisfying sum_k=1^K z_nk = 1 and z_nk in 0 1 forall k. We put a hyperprior on these variables, termed s, which represents the relative occurrence of the different realizations of z_n.","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"@model function multivariate_gaussian_mixture_model(nr_mixtures, nr_samples, priors_mean, priors_cov)\n    \n    z = randomvar(nr_samples)\n    m = randomvar(nr_mixtures)\n    w = randomvar(nr_mixtures)\n    y = datavar(Vector{Float64}, nr_samples)\n    \n    for k in 1:nr_mixtures        \n        m[k] ~ MvNormal(μ = priors_mean[k], Σ = priors_cov[k])\n        w[k] ~ Wishart(3, 1e2*diagm(ones(2)))\n    end\n    \n    s ~ Dirichlet(ones(nr_mixtures))\n    \n    means = tuple(m...)\n    precs = tuple(w...)\n    \n    for n in 1:nr_samples\n        z[n] ~ Categorical(s) \n        y[n] ~ NormalMixture(z[n], means, precs)\n    end\n    \nend","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/#Probabilistic-inference-2","page":"Gaussian Mixture","title":"Probabilistic inference","text":"","category":"section"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"In order to fit the model to the data, we are interested in computing the posterior distribution p(z s m W mid y) However, computation of this term is intractable. Therefore, it is approximated by a naive mean-field approximation, specified as  p(z s m W mid y) approx prod_n=1^N q(z_n) prod_k=1^K bigg(q(m_k) q(W_k)bigg) q(s) with the functional forms beginaligned     q(s)   = mathrmDir(s mid hatalpha_s) \n    q(m_k) = mathcalN(m_k mid hatmu_k hatSigma_k) \n    q(w_k) = mathcalW(W_k mid hatV_k hatnu_k) \n    q(z_n) = mathrmCat(z_n mid hatp_n) endaligned In order to get the inference procedure started, these marginal distribution need to be initialized.","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"rng = MersenneTwister(121)\nm = [[cos(k*2π/6), sin(k*2π/6)] for k in 1:6]\nresults_multivariate = infer(\n    model = multivariate_gaussian_mixture_model(\n        6, \n        length(data_multivariate), \n        m,\n        [diagm(1e2 * ones(2)) for k in 1:6]\n    ), \n    data  = (y = data_multivariate,), \n    constraints   = MeanField(),\n    initmarginals = (\n        s = vague(Dirichlet, 6), \n        m = [MvNormalMeanCovariance(m[k], diagm(1e2 * ones(2))) for k in 1:6], \n        w = Wishart(3, diagm(1e2 * ones(2)))\n    ), \n    iterations  = 50, \n    free_energy = true\n)","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"Inference results:\n  Posteriors       | available for (m, w, s, z)\n  Free Energy:     | Real[4166.93, 4025.27, 3908.18, 3894.5, 3894.49, 3894.\n49, 3894.49, 3894.49, 3894.49, 3894.49  …  3894.49, 3894.49, 3894.49, 3894.\n49, 3894.49, 3894.49, 3894.49, 3894.49, 3894.49, 3894.49]","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/#Results-2","page":"Gaussian Mixture","title":"Results","text":"","category":"section"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"Below the inference results can be seen","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"p_data = scatter(data_multivariate |> sdim(1), data_multivariate |> sdim(2), ms = 2, alpha = 0.4, legend=false, title=\"Data\", xlims=(-75, 75), ylims=(-75, 75))\np_result = plot(xlims = (-75, 75), ylims = (-75, 75), title=\"Inference result\", legend=false, colorbar = false)\nfor (e_m, e_w) in zip(results_multivariate.posteriors[:m][end], results_multivariate.posteriors[:w][end])\n    gaussian = MvNormal(mean(e_m), Matrix(Hermitian(mean(inv, e_w))))\n    global p_result = contour!(p_result, range(-75, 75, step = 0.25), range(-75, 75, step = 0.25), (x, y) -> pdf(gaussian, [ x, y ]), title=\"Inference result\", legend=false, levels = 7, colorbar = false)\nend\np_fe = plot(results_multivariate.free_energy, label = \"Free Energy\")\n\nplot(p_data, p_result, p_fe, layout = @layout([ a b; c ]))","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Gaussian Mixture/","page":"Gaussian Mixture","title":"Gaussian Mixture","text":"","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/#examples-kalman-filtering-and-smoothing","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"","category":"section"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"In the following set of examples the goal is to estimate hidden states of a Dynamical process where all hidden states are Gaussians.","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"We start our journey with a simple multivariate Linear Gaussian State Space Model (LGSSM), which can be solved analytically.","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"We then solve an identification problem which does not have an analytical solution.","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"Utimately, we show how RxInfer.jl can deal with missing observations.","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/#Gaussian-Linear-Dynamical-System","page":"Kalman filtering and smoothing","title":"Gaussian Linear Dynamical System","text":"","category":"section"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"LGSSM can be described with the following equations:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"beginaligned\n p(x_ix_i - 1)  = mathcalN(x_iA * x_i - 1 mathcalP)\n p(y_ix_i)  = mathcalN(y_iB * x_i mathcalQ)\nendaligned","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"where x_i are hidden states, y_i are noisy observations, A, B are state transition and observational matrices, mathcalP and mathcalQ are state transition noise and observation noise covariance matrices. For a more rigorous introduction to Linear Gaussian Dynamical systems we refer to Simo Sarkka, Bayesian Filtering and Smoothing book.","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"To model this process in RxInfer, first, we start with importing all needed packages:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"using RxInfer, BenchmarkTools, Random, LinearAlgebra, Plots","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"Next step, is to generate some synthetic data:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"function generate_data(rng, A, B, Q, P)\n    x_prev = [ 10.0, -10.0 ]\n\n    x = Vector{Vector{Float64}}(undef, n)\n    y = Vector{Vector{Float64}}(undef, n)\n\n    for i in 1:n\n        x[i] = rand(rng, MvNormal(A * x_prev, Q))\n        y[i] = rand(rng, MvNormal(B * x[i], P))\n        x_prev = x[i]\n    end\n    \n    return x, y\nend","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"generate_data (generic function with 1 method)","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"# Seed for reproducibility\nseed = 1234\n\nrng = MersenneTwister(1234)\n\n# We will model 2-dimensional observations with rotation matrix `A`\n# To avoid clutter we also assume that matrices `A`, `B`, `P` and `Q`\n# are known and fixed for all time-steps\nθ = π / 35\nA = [ cos(θ) -sin(θ); sin(θ) cos(θ) ]\nB = diageye(2)\nQ = diageye(2)\nP = 25.0 .* diageye(2)\n\n# Number of observations\nn = 300;","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"x, y = generate_data(rng, A, B, Q, P);","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"Let's plot our synthetic dataset. Lines represent our hidden states we want to estimate using noisy observations, which are represented as dots.","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"px = plot()\n\npx = plot!(px, getindex.(x, 1), label = \"Hidden Signal (dim-1)\", color = :orange)\npx = scatter!(px, getindex.(y, 1), label = false, markersize = 2, color = :orange)\npx = plot!(px, getindex.(x, 2), label = \"Hidden Signal (dim-2)\", color = :green)\npx = scatter!(px, getindex.(y, 2), label = false, markersize = 2, color = :green)\n\nplot(px)","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"To create a model we use GraphPPL package and @model macro:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"@model function rotate_ssm(n, x0, A, B, Q, P)\n    \n    # We create constvar references for better efficiency\n    cA = constvar(A)\n    cB = constvar(B)\n    cQ = constvar(Q)\n    cP = constvar(P)\n    \n    # `x` is a sequence of hidden states\n    x = randomvar(n)\n    # `y` is a sequence of \"clamped\" observations\n    y = datavar(Vector{Float64}, n)\n    \n    x_prior ~ MvNormalMeanCovariance(mean(x0), cov(x0))\n    x_prev = x_prior\n    \n    for i in 1:n\n        x[i] ~ MvNormalMeanCovariance(cA * x_prev, cQ)\n        y[i] ~ MvNormalMeanCovariance(cB * x[i], cP)\n        x_prev = x[i]\n    end\n\nend","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"To run inference we also specify prior for out first hidden state:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"x0 = MvNormalMeanCovariance(zeros(2), 100.0 * diageye(2));","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"# For large number of observations you need to use `limit_stack_depth = 100` option during model creation, e.g. \n# infer(..., options = (limit_stack_depth = 500, ))`\nresult = infer(\n    model = rotate_ssm(length(y), x0, A, B, Q, P), \n    data = (y = y,),\n    free_energy = true\n);\n\nxmarginals  = result.posteriors[:x]\nlogevidence = -result.free_energy; # given the analytical solution, free energy will be equal to the negative log evidence","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"px = plot()\n\npx = plot!(px, getindex.(x, 1), label = \"Hidden Signal (dim-1)\", color = :orange)\npx = plot!(px, getindex.(x, 2), label = \"Hidden Signal (dim-2)\", color = :green)\n\npx = plot!(px, getindex.(mean.(xmarginals), 1), ribbon = getindex.(var.(xmarginals), 1) .|> sqrt, fillalpha = 0.5, label = \"Estimated Signal (dim-1)\", color = :teal)\npx = plot!(px, getindex.(mean.(xmarginals), 2), ribbon = getindex.(var.(xmarginals), 2) .|> sqrt, fillalpha = 0.5, label = \"Estimated Signal (dim-1)\", color = :violet)\n\nplot(px)","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"As we can see from our plot, estimated signal resembles closely to the real hidden states with small variance. We maybe also interested in the value for minus log evidence:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"logevidence","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"1-element Vector{Float64}:\n -1882.2434870099778","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"We may be also interested in performance of our resulting Belief Propagation algorithm:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"@benchmark infer(\n    model = rotate_ssm(length($y), $x0, $A, $B, $Q, $P), \n    data = (y = $y,)\n)","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"BenchmarkTools.Trial: 140 samples with 1 evaluation.\n Range (min … max):  27.308 ms … 79.422 ms  ┊ GC (min … max): 0.00% … 9.37%\n Time  (median):     33.728 ms              ┊ GC (median):    0.00%\n Time  (mean ± σ):   35.935 ms ±  7.598 ms  ┊ GC (mean ± σ):  5.40% ± 9.31%\n\n       ▄█▂▇▇                                                   \n  ▃▃▃▃███████▄▃▃▃▁▃▁▃▅▃▃▄▃▃▃▃▅▁▁▁▁▃▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▁▁▁▃▁▁▁▁▁▁▃ ▃\n  27.3 ms         Histogram: frequency by time          71 ms <\n\n Memory estimate: 9.73 MiB, allocs estimate: 226250.","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/#System-Identification-Problem","page":"Kalman filtering and smoothing","title":"System Identification Problem","text":"","category":"section"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"In this example we are going to attempt to run Bayesian inference and decouple two random-walk signals, which were combined into a single single through some deterministic function f. We do not have access to the real values of these signals, but only to their combination. First, we create the generate_data function that accepts f as an argument:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"using RxInfer, Distributions, StableRNGs, Plots","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"function generate_data(f, n; seed = 123, x_i_min = -20.0, w_i_min = 20.0, noise = 20.0, real_x_τ = 0.1, real_w_τ = 1.0)\n\n    rng = StableRNG(seed)\n\n    real_x = Vector{Float64}(undef, n)\n    real_w = Vector{Float64}(undef, n)\n    real_y = Vector{Float64}(undef, n)\n\n    for i in 1:n\n        real_x[i] = rand(rng, Normal(x_i_min, sqrt(1.0 / real_x_τ)))\n        real_w[i] = rand(rng, Normal(w_i_min, sqrt(1.0 / real_w_τ)))\n        real_y[i] = rand(rng, Normal(f(real_x[i], real_w[i]), sqrt(noise)))\n\n        x_i_min = real_x[i]\n        w_i_min = real_w[i]\n    end\n    \n    return real_x, real_w, real_y\nend","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"generate_data (generic function with 2 methods)","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"The function returns the real signals real_x and  real_w for later comparison (we are not going to use them during inference) and their combined version real_y (we are going to use it as our observations during the inference). We also assume that real_y is corrupted with some measurement noise.","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/#Combination-1:-y-x-w","page":"Kalman filtering and smoothing","title":"Combination 1: y = x + w","text":"","category":"section"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"In our first example, we are going to use a simple addition (+) as the function f. In general, it is impossible to decouple the signals x and w without strong priors, but we can try and see how good an inference can be. The + operation on two random variables also has a special meaning in the probabilistic inference, namely the convolution of pdf's of the two random variables, and RxInfer treats it specially with many precomputed analytical rules, which may make the inference task easier. First, let us create a test dataset:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"n = 250\nreal_x, real_w, real_y = generate_data(+, n);\n\npl = plot(title = \"Underlying signals\")\npl = plot!(pl, real_x, label = \"x\")\npl = plot!(pl, real_w, label = \"w\")\n\npr = plot(title = \"Combined y = x + w\")\npr = scatter!(pr, real_y, ms = 3, color = :red, label = \"y\")\n\nplot(pl, pr, size = (800, 300))","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"To run inference, we need to create a probabilistic model: our beliefs about how our data could have been generated. For this we can use the @model macro from RxInfer.jl:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"@model function identification_problem(f, n, m_x_0, τ_x_0, a_x, b_x, m_w_0, τ_w_0, a_w, b_w, a_y, b_y)\n    \n    x0 ~ Normal(mean = m_x_0, precision = τ_x_0)\n    τ_x ~ Gamma(shape = a_x, rate = b_x)\n    w0 ~ Normal(mean = m_w_0, precision = τ_w_0)\n    τ_w ~ Gamma(shape = a_w, rate = b_w)\n    τ_y ~ Gamma(shape = a_y, rate = b_y)\n    \n    x = randomvar(n)\n    w = randomvar(n)\n    s = randomvar(n)\n    y = datavar(Float64, n)\n    \n    x_i_min = x0\n    w_i_min = w0\n    \n    for i in 1:n\n        x[i] ~ Normal(mean = x_i_min, precision = τ_x)\n        w[i] ~ Normal(mean = w_i_min, precision = τ_w)\n        s[i] ~ f(x[i], w[i])\n        y[i] ~ Normal(mean = s[i], precision = τ_y)\n        \n        x_i_min = x[i]\n        w_i_min = w[i]\n    end\n    \nend","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"RxInfer runs Bayesian inference as a variational optimisation procedure between the real solution and its variational proxy q. In our model specification we assumed noise components to be unknown, thus, we need to enforce a structured mean-field assumption for the variational family of distributions q. This inevitably reduces the accuracy of the result, but makes the task easier and allows for fast and analytical message passing-based variational inference:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"constraints = @constraints begin \n    q(x0, w0, x, w, τ_x, τ_w, τ_y, s) = q(x, x0, w, w0, s)q(τ_w)q(τ_x)q(τ_y)\nend","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"Constraints:\n  marginals form:\n  messages form:\n  factorisation:\n    q(x0, w0, x, w, τ_x, τ_w, τ_y, s) = q(x, x0, w, w0, s)q(τ_w)q(τ_x)q(τ_y\n)\nOptions:\n  warn = true","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"The next step is to assign priors, initialise needed messages and marginals and call the inference function:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"m_x_0, τ_x_0 = -20.0, 1.0\nm_w_0, τ_w_0 = 20.0, 1.0\n\n# We set relatively strong priors for random walk noise components\n# and sort of vague prior for the noise of the observations\na_x, b_x = 0.01, 0.01var(real_x)\na_w, b_w = 0.01, 0.01var(real_w)\na_y, b_y = 1.0, 1.0\n\n# We set relatively strong priors for messages\nxinit = map(r -> NormalMeanPrecision(r, τ_x_0), reverse(range(-60, -20, length = n)))\nwinit = map(r -> NormalMeanPrecision(r, τ_w_0), range(20, 60, length = n))\n\nimessages = (x = xinit, w = winit)\nimarginals = (τ_x = GammaShapeRate(a_x, b_x), τ_w = GammaShapeRate(a_w, b_w), τ_y = GammaShapeRate(a_y, b_y))\n\nresult = infer(\n    model = identification_problem(+, n, m_x_0, τ_x_0, a_x, b_x, m_w_0, τ_w_0, a_w, b_w, a_y, b_y),\n    data  = (y = real_y,), \n    options = (limit_stack_depth = 500, ), \n    constraints = constraints, \n    initmessages = imessages, \n    initmarginals = imarginals, \n    iterations = 50\n)","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"Inference results:\n  Posteriors       | available for (w0, w, x0, s, τ_x, τ_w, τ_y, x)","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"Let's examine our inference results:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"τ_x_marginals = result.posteriors[:τ_x]\nτ_w_marginals = result.posteriors[:τ_w]\nτ_y_marginals = result.posteriors[:τ_y]\n\nsmarginals = result.posteriors[:s]\nxmarginals = result.posteriors[:x]\nwmarginals = result.posteriors[:w];","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"px1 = plot(legend = :bottomleft, title = \"Estimated hidden signals\")\npx2 = plot(legend = :bottomright, title = \"Estimated combined signals\")\n\npx1 = plot!(px1, real_x, label = \"Real hidden X\")\npx1 = plot!(px1, mean.(xmarginals[end]), ribbon = var.(xmarginals[end]), label = \"Estimated X\")\n\npx1 = plot!(px1, real_w, label = \"Real hidden W\")\npx1 = plot!(px1, mean.(wmarginals[end]), ribbon = var.(wmarginals[end]), label = \"Estimated W\")\n\npx2 = scatter!(px2, real_y, label = \"Observations\", ms = 2, alpha = 0.5, color = :red)\npx2 = plot!(px2, mean.(smarginals[end]), ribbon = std.(smarginals[end]), label = \"Combined estimated signal\", color = :green)\n\nplot(px1, px2, size = (800, 300))","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"The inference results are not so bad, even though RxInfer missed the correct values of the signals between 100 and 150.","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/#Combination-2:-y-min(x,-w)","page":"Kalman filtering and smoothing","title":"Combination 2: y = min(x, w)","text":"","category":"section"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"In this example we use a slightly more complex function, for which RxInfer does not have precomputed analytical message update rules. We are going to attempt to run Bayesian inference with min as a combination function. Note, however, that directly using min may cause problems for the built-in approximation methods as it has zero partial derviates with respect to all but one of the variables. We generate data with the min function directly however we model it with a somewhat smoothed version:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"# Smoothed version of `min` without zero-ed derivatives\nfunction smooth_min(x, y)    \n    if x < y\n        return x + 1e-4 * y\n    else\n        return y + 1e-4 * x\n    end\nend","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"smooth_min (generic function with 1 method)","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"RxInfer supports arbitrary nonlinear functions, but it requires an explicit approximation method specification. That can be achieved with the built-in @meta macro:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"min_meta = @meta begin \n    # In this example we are going to use a simple `Linearization` method\n    smooth_min() -> Linearization()\nend","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"Meta specification:\n  smooth_min() -> Linearization()\nOptions:\n  warn = true","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"n = 200\nmin_real_x, min_real_w, min_real_y = generate_data(min, n, seed = 1, x_i_min = 0.0, w_i_min = 0.0, noise = 1.0, real_x_τ = 1.0, real_w_τ = 1.0);\n\npl = plot(title = \"Underlying signals\")\npl = plot!(pl, min_real_x, label = \"x\")\npl = plot!(pl, min_real_w, label = \"w\")\n\npr = plot(title = \"Combined y = min(x, w)\")\npr = scatter!(pr, min_real_y, ms = 3, color = :red, label = \"y\")\n\nplot(pl, pr, size = (800, 300))","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"min_m_x_0, min_τ_x_0 = -1.0, 1.0\nmin_m_w_0, min_τ_w_0 = 1.0, 1.0\n\nmin_a_x, min_b_x = 1.0, 1.0\nmin_a_w, min_b_w = 1.0, 1.0\nmin_a_y, min_b_y = 1.0, 1.0\n\nmin_imessages = (x = NormalMeanPrecision(min_m_x_0, min_τ_x_0), w = NormalMeanPrecision(min_m_w_0, min_τ_w_0))\nmin_imarginals = (τ_x = GammaShapeRate(min_a_x, min_b_x), τ_w = GammaShapeRate(min_a_w, min_b_w), τ_y = GammaShapeRate(min_a_y, min_b_y))\n\nmin_result = infer(\n    model = identification_problem(smooth_min, n, min_m_x_0, min_τ_x_0, min_a_x, min_b_x, min_m_w_0, min_τ_w_0, min_a_w, min_b_w, min_a_y, min_b_y),\n    data  = (y = min_real_y,), \n    meta = min_meta,\n    options = (limit_stack_depth = 500, ), \n    constraints = constraints, \n    initmessages = min_imessages, \n    initmarginals = min_imarginals, \n    iterations = 100\n)","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"Inference results:\n  Posteriors       | available for (w0, w, x0, s, τ_x, τ_w, τ_y, x)","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"min_τ_x_marginals = min_result.posteriors[:τ_x]\nmin_τ_w_marginals = min_result.posteriors[:τ_w]\nmin_τ_y_marginals = min_result.posteriors[:τ_y]\n\nmin_smarginals = min_result.posteriors[:s]\nmin_xmarginals = min_result.posteriors[:x]\nmin_wmarginals = min_result.posteriors[:w]\n\npx1 = plot(legend = :bottomleft, title = \"Estimated hidden signals\")\npx2 = plot(legend = :bottomright, title = \"Estimated combined signals\")\n\npx1 = plot!(px1, min_real_x, label = \"Real hidden X\")\npx1 = plot!(px1, mean.(min_xmarginals[end]), ribbon = var.(min_xmarginals[end]), label = \"Estimated X\")\n\npx1 = plot!(px1, min_real_w, label = \"Real hidden W\")\npx1 = plot!(px1, mean.(min_wmarginals[end]), ribbon = var.(min_wmarginals[end]), label = \"Estimated W\")\n\npx2 = scatter!(px2, min_real_y, label = \"Observations\", ms = 2, alpha = 0.5, color = :red)\npx2 = plot!(px2, mean.(min_smarginals[end]), ribbon = std.(min_smarginals[end]), label = \"Combined estimated signal\", color = :green)\n\nplot(px1, px2, size = (800, 300))","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"As we can see inference with the min function is significantly harder. Even though the combined signal has been inferred with high precision the underlying x and w signals are barely inferred. This may be expected, since the min function essentially destroy the information about one of the signals, thus, making it impossible to decouple two seemingly identical random walk signals. The only one inferred signal is the one which is lower and we have no inference information about the signal which is above. It might be possible to infer the states, however, with more informative priors and structural information about two different signals (e.g. if these are not random walks). ","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/#Online-(filtering)-identification:-y-min(x,-w)","page":"Kalman filtering and smoothing","title":"Online (filtering) identification: y = min(x, w)","text":"","category":"section"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"Another way to approach to this problem is to use online (filtering) inference procedure from RxInfer, but for that we also need to modify our model specification a bit:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"@model function rx_identification(f)\n    \n    # We are going to continuosly update our priors\n    # based on new posteriors\n    m_x_0 = datavar(Float64) \n    τ_x_0 = datavar(Float64)\n    m_w_0 = datavar(Float64) \n    τ_w_0 = datavar(Float64)\n    a_x   = datavar(Float64) \n    b_x   = datavar(Float64)\n    a_y   = datavar(Float64) \n    b_y   = datavar(Float64)\n    a_w   =  datavar(Float64) \n    b_w   = datavar(Float64)\n    s     = randomvar()\n    y     = datavar(Float64)\n    \n    x0 ~ Normal(mean = m_x_0, precision = τ_x_0)\n    τ_x ~ Gamma(shape = a_x, rate = b_x)\n    w0 ~ Normal(mean = m_w_0, precision = τ_w_0)\n    τ_w ~ Gamma(shape = a_w, rate = b_w)\n    τ_y ~ Gamma(shape = a_y, rate = b_y)\n    \n    x ~ Normal(mean = x0, precision = τ_x)\n    w ~ Normal(mean = w0, precision = τ_w)\n\n    s ~ f(x, w)\n    y ~ Normal(mean = s, precision = τ_y)\n    \nend","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"We impose structured mean-field assumption for this model as well:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"rx_constraints = @constraints begin \n    q(x0, x, w0, w, τ_x, τ_w, τ_y, s) = q(x0, x)q(w, w0)q(τ_w)q(τ_x)q(s)q(τ_y)\nend","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"Constraints:\n  marginals form:\n  messages form:\n  factorisation:\n    q(x0, x, w0, w, τ_x, τ_w, τ_y, s) = q(x0, x)q(w, w0)q(τ_w)q(τ_x)q(s)q(τ\n_y)\nOptions:\n  warn = true","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"Online inference in the RxInfer supports the @autoupdates specification, which tells inference procedure how to update priors based on new computed posteriors:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"autoupdates = @autoupdates begin \n    m_x_0, τ_x_0 = mean_precision(q(x))\n    m_w_0, τ_w_0 = mean_precision(q(w))\n    a_x = shape(q(τ_x)) \n    b_x = rate(q(τ_x))\n    a_y = shape(q(τ_y))\n    b_y = rate(q(τ_y))\n    a_w = shape(q(τ_w)) \n    b_w = rate(q(τ_w))\nend","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"(m_x_0,τ_x_0 = mean_precision(q(x)), m_w_0,τ_w_0 = mean_precision(q(w)), a_\nx = shape(q(τ_x)), b_x = rate(q(τ_x)), a_y = shape(q(τ_y)), b_y = rate(q(τ_\ny)), a_w = shape(q(τ_w)), b_w = rate(q(τ_w)))","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"As previously we need to define the @meta structure that specifies the approximation method for the nonlinear function smooth_min (f in the model specification):","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"rx_meta = @meta begin \n    smooth_min() -> Linearization()\nend","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"Meta specification:\n  smooth_min() -> Linearization()\nOptions:\n  warn = true","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"Next step is to generate our dataset and to run the actual inference procedure! For that we use the infer function with autoupdates keyword:","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"n = 300\nrx_real_x, rx_real_w, rx_real_y = generate_data(min, n, seed = 1, x_i_min = 1.0, w_i_min = -1.0, noise = 1.0, real_x_τ = 1.0, real_w_τ = 1.0);\n\npl = plot(title = \"Underlying signals\")\npl = plot!(pl, rx_real_x, label = \"x\")\npl = plot!(pl, rx_real_w, label = \"w\")\n\npr = plot(title = \"Combined y = min(x, w)\")\npr = scatter!(pr, rx_real_y, ms = 3, color = :red, label = \"y\")\n\nplot(pl, pr, size = (800, 300))","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"engine = infer(\n    model         = rx_identification(smooth_min),\n    constraints   = rx_constraints,\n    data          = (y = rx_real_y,),\n    autoupdates   = autoupdates,\n    meta          = rx_meta,\n    returnvars    = (:x, :w, :τ_x, :τ_w, :τ_y, :s),\n    keephistory   = 1000,\n    historyvars   =  KeepLast(),\n    initmarginals = (w = NormalMeanVariance(-2.0, 1.0), x = NormalMeanVariance(2.0, 1.0), τ_x = GammaShapeRate(1.0, 1.0), τ_w = GammaShapeRate(1.0, 1.0), τ_y = GammaShapeRate(1.0, 20.0)),\n    iterations    = 10,\n    free_energy = true, \n    free_energy_diagnostics = nothing,\n    autostart     = true,\n)","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"RxInferenceEngine:\n  Posteriors stream    | enabled for (w, s, τ_x, τ_w, τ_y, x)\n  Free Energy stream   | enabled\n  Posteriors history   | available for (x, w, x0, s, τ_x, τ_w, τ_y, w0)\n  Free Energy history  | available\n  Enabled events       | [  ]","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"rx_smarginals = engine.history[:s]\nrx_xmarginals = engine.history[:x]\nrx_wmarginals = engine.history[:w];","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"px1 = plot(legend = :bottomleft, title = \"Estimated hidden signals\")\npx2 = plot(legend = :bottomright, title = \"Estimated combined signals\")\n\npx1 = plot!(px1, rx_real_x, label = \"Real hidden X\")\npx1 = plot!(px1, mean.(rx_xmarginals), ribbon = var.(rx_xmarginals), label = \"Estimated X\")\n\npx1 = plot!(px1, rx_real_w, label = \"Real hidden W\")\npx1 = plot!(px1, mean.(rx_wmarginals), ribbon = var.(rx_wmarginals), label = \"Estimated W\")\n\npx2 = scatter!(px2, rx_real_y, label = \"Observations\", ms = 2, alpha = 0.5, color = :red)\npx2 = plot!(px2, mean.(rx_smarginals), ribbon = std.(rx_smarginals), label = \"Combined estimated signal\", color = :green)\n\nplot(px1, px2, size = (800, 300))","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"The results are quite similar to the smoothing case and, as we can see, one of the random walk is again in the \"disabled\" state, does not infer anything and simply increases its variance (which is expected for the random walk).","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/#Handling-Missing-Data","page":"Kalman filtering and smoothing","title":"Handling Missing Data","text":"","category":"section"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"An interesting case in filtering and smoothing problems is the processing of missing data. It can happen that sometimes your reading devices failt to acquire the data leading to missing observation.","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"Let us assume that the following model generates the data","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"beginaligned\n    x_t sim mathcalNleft(x_t-1 10right) \n    y_t sim mathcalNleft(x_t P right) \nendaligned","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"with prior x_0 sim mathcalN(m_x_0 v_x_0). Suppose that our measurement device fails to acquire data from time to time.  In this case, instead of scalar observation haty_t in mathrmR we sometimes will catch missing observations.","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"using RxInfer, Plots","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"@model function smoothing(n, x0)\n    \n    P ~ Gamma(shape = 0.001, scale = 0.001)\n    x_prior ~ Normal(mean = mean(x0), var = var(x0)) \n\n    x = randomvar(n)\n    y = datavar(Float64, n) where { allow_missing = true }\n\n    x_prev = x_prior\n\n    for i in 1:n\n        x[i] ~ Normal(mean = x_prev, precision = 1.0)\n        y[i] ~ Normal(mean = x[i], precision = P)\n        \n        x_prev = x[i]\n    end\n\nend","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"We need to manually extend the set of rules to support ::Missing values","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"P = 1.0\nn = 250\n\nreal_signal     = map(e -> sin(0.05 * e), collect(1:n))\nnoisy_data      = real_signal + rand(Normal(0.0, sqrt(P)), n);\nmissing_indices = 100:125\nmissing_data    = similar(noisy_data, Union{Float64, Missing}, )\n\ncopyto!(missing_data, noisy_data)\n\nfor index in missing_indices\n    missing_data[index] = missing\nend","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"constraints = @constraints begin\n    q(x_prior, x, y, P) = q(x_prior, x)q(P)q(y)\nend","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"Constraints:\n  marginals form:\n  messages form:\n  factorisation:\n    q(x_prior, x, y, P) = q(x_prior, x)q(P)q(y)\nOptions:\n  warn = true","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"x0_prior = NormalMeanVariance(0.0, 1000.0)\n\nresult = infer(\n    model = smoothing(n, x0_prior), \n    data  = (y = missing_data,), \n    constraints = constraints,\n    initmarginals = (P = Gamma(0.001, 0.001), ),\n    returnvars = (x = KeepLast(),),\n    iterations = 20\n);","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"plot(real_signal, label = \"Noisy signal\", legend = :bottomright)\nscatter!(missing_indices, real_signal[missing_indices], ms = 2, opacity = 0.75, label = \"Missing region\")\nplot!(mean.(result.posteriors[:x]), ribbon = var.(result.posteriors[:x]), label = \"Estimated hidden state\")","category":"page"},{"location":"examples/basic_examples/Kalman filtering and smoothing/","page":"Kalman filtering and smoothing","title":"Kalman filtering and smoothing","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/#examples-conjugate-computational-variational-message-passing-(cvi)","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"","category":"section"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"using RxInfer, Random, LinearAlgebra, Plots, Optimisers, Plots, StableRNGs, SpecialFunctions","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"In this notebook, the usage of Conjugate-NonConjugate Variational Inference (CVI) will be described. The implementation of CVI follows the paper Probabilistic programming with stochastic variational message passing.","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"This notebook will first describe an example in which CVI is used, then it discusses several limitations, followed by an explanation on how to extend upon CVI.","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/#An-example:-nonlinear-dynamical-system","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"An example: nonlinear dynamical system","text":"","category":"section"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"A group of researchers is performing a tracking experiment on some moving object along a 1-dimensional trajectory. The object is moving at a constant velocity, meaning that its position increases constantly over time. However, the researchers do not have access to the absolute position z_t at time t. Instead they have access to the observed squared distance y_t between the object and some reference point s. Because of budget cuts, the servo moving the object and the measurement devices are quite outdated and therefore lead to noisy measurements:","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"# data generating process\nnr_observations = 50\nreference_point = 53\nhidden_location = collect(1:nr_observations) + rand(StableRNG(124), NormalMeanVariance(0.0, sqrt(5)), nr_observations)\nmeasurements = (hidden_location .- reference_point).^2 + rand(MersenneTwister(124), NormalMeanVariance(0.0, 5), nr_observations);","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"# plot hidden location and reference frame\np1 = plot(1:nr_observations, hidden_location, linewidth=3, legend=:topleft, label=\"hidden location\")\nhline!([reference_point], linewidth=3, label=\"reference point\")\nxlabel!(\"time [sec]\"), ylabel!(\"location [cm]\")\n\n# plot measurements\np2 = scatter(1:nr_observations, measurements, linewidth=3, label=\"measurements\")\nxlabel!(\"time [sec]\"), ylabel!(\"squared distance [cm2]\")\n\nplot(p1, p2, size=(1200, 500))","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"The researchers are interested in quantifying this noise and in tracking the unobserved location of the object. As a result of this uncertainty, the researchers employ a probabilistic modeling approach. They formulate the probabilistic model","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"beginaligned\n p(tau)  = Gamma(tau mid alpha_tau beta_tau)\n p(gamma)  = Gamma(gamma mid alpha_gamma beta_gamma)\n p(z_t mid z_t - 1 tau)  = mathcalN(z_t mid z_t - 1 + 1 tau^-1)\n p(y_t mid z_t gamma)  = mathcalN(y_t mid (z_t - s)^2 gamma^-1)\nendaligned","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"where the researchers put priors on the process and measurement noise parameters, tau and gamma, respectively. They do this, because they do not know the accuracy of their devices.","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"The researchers have recently heard of this cool probabilistic programming package RxInfer.jl. They decided to give it a try and create the above model as follows:","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"function compute_squared_distance(z)\n    (z - reference_point)^2\nend;","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"@model function measurement_model(nr_observations)\n\n    # allocate random variables and observations\n    z = randomvar(nr_observations)\n    y = datavar(Float64, nr_observations)\n\n    # set priors on precision parameters\n    τ ~ Gamma(shape = 1.0, rate = 1.0e-12)\n    γ ~ Gamma(shape = 1.0, rate = 1.0e-12)\n    \n    # specify estimate of initial location\n    z[1] ~ Normal(mean = 0, precision = τ)\n    y[1] ~ Normal(mean = compute_squared_distance(z[1]), precision = γ)\n\n    # loop over observations\n    for t in 2:nr_observations\n\n        # specify state transition model\n        z[t] ~ Normal(mean = z[t-1] + 1, precision = τ)\n\n        # specify non-linear observation model\n        y[t] ~ Normal(mean = compute_squared_distance(z[t]), precision = γ)\n        \n    end\n\nend","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"But here is the problem, our compute_squared_distance function is already compelx enough such that the exact Bayesian inference is intractable in this model. But the researchers knew that the RxInfer.jl supports a various collection of approximation methods for exactly such cases. One of these approximations is called CVI. CVI allows us to perform probabilistic inference around the non-linear measurement function. In general, for any (non-)linear relationship y = f(x1, x2, ..., xN) CVI can be employed, by specifying the function f and by adding this relationship inside the @model macro as y ~ f(x1, x2, ...,xN). The @model macro will generate a factor node with node function p(y | x1, x2, ..., xN) = δ(y - f(x1, x2, ...,xN)).","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"The use of this non-linearity requires us to specify that we would like to use CVI. This can be done by specifying the metadata using the @meta macro as:","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"@meta function measurement_meta(rng, nr_samples, nr_iterations, optimizer)\n    compute_squared_distance() -> CVI(rng, nr_samples, nr_iterations, optimizer)\nend;","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"In general, for any (non-)linear function f(), CVI can be enabled with the @meta macro as:","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"@meta function model_meta(...)\n    f() -> CVI(args...)\nend","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"See ?CVI for more information about the args....","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"In our model, the z variables are connected to the non-linear node function. So in order to run probabilstic inference with CVI we need to enforce a constraint on the joint posterior distribution. Specifically, we need to create a factorization in which the variables that are directly connected to non-linearities are assumed to be independent from the rest of the variables.","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"In the above example, we will assume the following posterior factorization:","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"@constraints function measurement_constraints()\n    q(z, τ, γ) = q(z)q(τ)q(γ)\nend;","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"This constraint can be explained by the set of two constraints, one for getting CVI to run, and one for assuming a mean-field factorization around the normal node as ","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"@constraints function posterior_constraints() begin\n    q(z, γ) = q(z)q(γ) # CVI\n    q(z, τ) = q(z)q(τ) # the mean-field assumption around normal node\nend","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"Because the engineers are using RxInfer.jl, they can automate the inference procedure. They track the inference performance using the Bethe free energy.","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"results = infer(\n    model = measurement_model(nr_observations),\n    data = (y = measurements,),\n    iterations = 5,\n    free_energy = true,\n    returnvars = (z = KeepLast(),),\n    constraints = measurement_constraints(),\n    meta = measurement_meta(StableRNG(42), 100, 200, Optimisers.Descent(0.01)),\n    initmessages = (z = NormalMeanVariance(0, 5),),\n    initmarginals = (z = NormalMeanVariance(0, 5), τ = GammaShapeRate(1.0, 1.0e-12), γ = GammaShapeRate(1.0, 1.0e-12),),\n)","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"Inference results:\n  Posteriors       | available for (z)\n  Free Energy:     | Real[1241.91, 429.933, 376.869, 365.774, 362.879]","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"# plot estimates for location\np1 = plot(collect(1:nr_observations), hidden_location, label = \"hidden location\", legend=:topleft, linewidth=3, color = :red)\nplot!(map(mean, results.posteriors[:z]), label = \"estimated location (±2σ)\", ribbon = map(x -> 2*std(x), results.posteriors[:z]), fillalpha=0.5, linewidth=3, color = :orange)\nxlabel!(\"time [sec]\"), ylabel!(\"location [cm]\")\n\n# plot Bethe free energy\np2 = plot(results.free_energy, linewidth=3, label = \"\")\nxlabel!(\"iteration\"), ylabel!(\"Bethe free energy [nats]\")\n\nplot(p1, p2, size = (1200, 500))","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/#Requirements","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Requirements","text":"","category":"section"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"There are several main requirements for the CVI procedure to satisfy:","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"The out interface of the non-linearity must be independently factorized with respect to other variables in the model.\nThe messages on input interfaces (x1, x2, ..., xN) are required to be from the exponential family of distributions.","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"In RxInfer, you can satisfy the first requirement by using appropriate factor nodes (Normal, Gamma, Bernoulli, etc) and second requirement by specifying the @constraints macro. In general you can specify this procedure as","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"@model function model(...)\n    ...\n    y ~ f(x1, x2, ..., xN)\n    ... ~ Node2(z1,..., y, zM) # some node that is using the out interface of the non-linearity\n    ... \nend\n\n@constraints function constraints_meta() begin\n    q(y, z1, ..., zn) = q(y)q(z1,...,zM)\n    ...\nend;\n\n@meta function model_meta(...)\n    f() -> CVI(rng, nr_samples, nr_iterations, optimizer))\nend","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"Note that not all exponential family distributions are implemented.","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/#Extensions","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Extensions","text":"","category":"section"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/#Using-a-custom-optimizer","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Using a custom optimizer","text":"","category":"section"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"CVI only supports Optimisers optimizers out of the box.","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"Below an explanation on how to extend to it to a custom optimizer.","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"Suppose we have CustomDescent structure which we want to use inside CVI for optimization.","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"To do so, we need to implement ReactiveMP.cvi_update!(opt::CustomDescent, λ, ∇).","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"ReactiveMP.cvi_update! incapsulates the gradient step:","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"opt is used to select your optimizer structure\nλ is the current value\n∇ is a gradient value computed inside CVI.","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"struct CustomDescent \n    learning_rate::Float64\nend\n\n# Must return an optimizer and its initial state\nfunction ReactiveMP.cvi_setup(opt::CustomDescent, q)\n     return (opt, nothing)\nend\n\n# Must return an updated (opt, state) and an updated λ (can use new_λ for inplace operation)\nfunction ReactiveMP.cvi_update!(opt_and_state::Tuple{CustomDescent, Nothing}, new_λ, λ, ∇)\n    opt, _ = opt_and_state\n    λ̂ = vec(λ) - (opt.learning_rate .* vec(∇))\n    copyto!(new_λ, λ̂)\n    return opt_and_state, new_λ\nend","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"Let's try to apply it to a model: beginaligned  p(x)  = mathcalN(0 1)\n p(y_imid x)  = mathcalN(y_i mid x^2 1)\nendaligned","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"Let's generate some synthetic data for the model","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"# generate data\ny = rand(StableRNG(123), NormalMeanVariance(19^2, 1), 1000)\nhistogram(y)","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"Again we can create the corresponding model as:","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"# specify non-linearity\nf(x) = x ^ 2\n\n# specify model\n@model function normal_square_model(nr_observations)\n\n    # allocate observations\n    y = datavar(Float64, nr_observations)\n\n    # describe prior on latent state\n    x ~ NormalMeanPrecision(0, 100)\n\n    # transform latent state\n    mean ~ f(x)\n\n    # observation model\n    y .~ NormalMeanVariance(mean, 1)\n\nend\n\n# specify meta\n@meta function normal_square_meta(rng, nr_samples, nr_iterations, optimizer)\n    f() ->  CVI(rng, nr_samples, nr_iterations, optimizer)\nend","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"normal_square_meta (generic function with 1 method)","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"We will use the inference function from ReactiveMP to run inference, where we provide an instance of the CustomDescent structure in our meta macro function:","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"Side note: To run the inference for this model, we do not need to init the message for x but with this initialization, the inference procedure is more stable.","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"res = infer(\n    model = normal_square_model(1000),\n    data = (y = y,),\n    iterations = 10,\n    free_energy = true,\n    initmessages = (x = NormalMeanVariance(0, 100),),\n    meta = normal_square_meta(StableRNG(123), 100, 100, CustomDescent(0.1))\n)\n\nmean(res.posteriors[:x][end])","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"-18.998075156688632","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"The mean inferred value of x is indeed close to 19, which was used to generate the data. Inference is working! ","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"Note:  x^2 can not be inverted; the sign information can be lost: -19 and 19 are both equally good solutions.","category":"page"},{"location":"examples/advanced_examples/Conjugate-Computational Variational Message Passing/","page":"Conjugate-Computational Variational Message Passing (CVI)","title":"Conjugate-Computational Variational Message Passing (CVI)","text":"","category":"page"},{"location":"library/functional-forms/#lib-forms","page":"Built-in functional form constraints","title":"Built-in Functional Forms","text":"","category":"section"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"This section describes built-in functional forms that can be used for posterior marginal and/or messages form constraints specification. Read more information about constraints specification syntax in the Constraints Specification section.","category":"page"},{"location":"library/functional-forms/#lib-forms-custom-constraints","page":"Built-in functional form constraints","title":"Custom functional forms","text":"","category":"section"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"See the ReactiveMP.jl library documentation for more information about defining novel custom functional forms that are compatible with ReactiveMP inference backend.","category":"page"},{"location":"library/functional-forms/#lib-forms-unspecified-constraint","page":"Built-in functional form constraints","title":"UnspecifiedFormConstraint","text":"","category":"section"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"Unspecified functional form constraint is used by default and uses only analytical update rules for computing posterior marginals. Throws an error if a product of two colliding messages cannot be computed analytically.","category":"page"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"@constraints begin \n    q(x) :: Nothing # This is the default setting\nend","category":"page"},{"location":"library/functional-forms/#lib-forms-point-mass-constraint","page":"Built-in functional form constraints","title":"PointMassFormConstraint","text":"","category":"section"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"The most basic form of posterior marginal approximation is the PointMass function. In a few words PointMass represents delta function. In the context of functional form constraints PointMass approximation corresponds to the MAP estimate. For a given distribution d - PointMass functional form simply finds the argmax of the logpdf(d, x) by default. ","category":"page"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"@constraints begin \n    q(x) :: PointMass # Materializes to the `PointMassFormConstraint` object\nend","category":"page"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"RxInfer.PointMassFormConstraint","category":"page"},{"location":"library/functional-forms/#RxInfer.PointMassFormConstraint","page":"Built-in functional form constraints","title":"RxInfer.PointMassFormConstraint","text":"PointMassFormConstraint\n\nOne of the form constraint objects. Constraint a message to be in a form of dirac's delta point mass.  By default uses Optim.jl package to find argmin of -logpdf(x).  Accepts custom optimizer callback which might be used to customise optimisation procedure with different packages  or different arguments for Optim.jl package.\n\nKeyword arguments\n\noptimizer: specifies a callback function for logpdf optimisation. See also: ReactiveMP.default_point_mass_form_constraint_optimizer\nstarting_point: specifies a callback function for initial optimisation point: See also: ReactiveMP.default_point_mass_form_constraint_starting_point\nboundaries: specifies a callback function for determining optimisation boundaries: See also: ReactiveMP.default_point_mass_form_constraint_boundaries\n\nCustom optimizer callback interface\n\n# This is an example of the `custom_optimizer` interface\nfunction custom_optimizer(::Type{ Univariate }, ::Type{ Continuous }, constraint::PointMassFormConstraint, distribution)\n    # should return argmin of the -logpdf(distribution)\nend\n\nCustom starting point callback interface\n\n# This is an example of the `custom_starting_point` interface\nfunction custom_starting_point(::Type{ Univariate }, ::Type{ Continuous }, constraint::PointMassFormConstraint, distribution)\n    # built-in optimizer expects an array, even for a univariate distribution\n    return [ 0.0 ] \nend\n\nCustom boundaries callback interface\n\n# This is an example of the `custom_boundaries` interface\nfunction custom_boundaries(::Type{ Univariate }, ::Type{ Continuous }, constraint::PointMassFormConstraint, distribution)\n    # returns a tuple of `lower` and `upper` boundaries\n    return (-Inf, Inf)\nend\n\nTraits\n\nis_point_mass_form_constraint = true\ndefault_form_check_strategy   = FormConstraintCheckLast()\ndefault_prod_constraint       = GenericProd()\nmake_form_constraint          = PointMass (for use in @constraints macro)\n\n\n\n\n\n","category":"type"},{"location":"library/functional-forms/#lib-forms-sample-list-constraint","page":"Built-in functional form constraints","title":"SampleListFormConstraint","text":"","category":"section"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"SampleListFormConstraints approximates the resulting posterior marginal (product of two colliding messages) as a list of weighted samples. Hence, it requires one of the arguments to be a proper distribution (or at least be able to sample from it). This setting is controlled with LeftProposal(), RightProposal() or AutoProposal() objects. It also accepts an optional method object, but the only one available sampling method currently is the BootstrapImportanceSampling.","category":"page"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"@constraints begin \n    q(x) :: SampleList(1000)\n    # or \n    q(y) :: SampleList(1000, LeftProposal())\nend","category":"page"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"RxInfer.SampleListFormConstraint\nRxInfer.AutoProposal\nRxInfer.LeftProposal\nRxInfer.RightProposal","category":"page"},{"location":"library/functional-forms/#RxInfer.SampleListFormConstraint","page":"Built-in functional form constraints","title":"RxInfer.SampleListFormConstraint","text":"SampleListFormConstraint(rng, strategy, method)\n\nOne of the form constraint objects. Approximates DistProduct with a SampleList object. \n\nTraits\n\nis_point_mass_form_constraint = false\ndefault_form_check_strategy   = FormConstraintCheckLast()\ndefault_prod_constraint       = GenericProd()\nmake_form_constraint          = SampleList (for use in @constraints macro)\n\n\n\n\n\n","category":"type"},{"location":"library/functional-forms/#RxInfer.AutoProposal","page":"Built-in functional form constraints","title":"RxInfer.AutoProposal","text":"Tries to determine the proposal distribution in the SampleList approximation automatically.\n\n\n\n\n\n","category":"type"},{"location":"library/functional-forms/#RxInfer.LeftProposal","page":"Built-in functional form constraints","title":"RxInfer.LeftProposal","text":"Uses the left argument in the prod call as the proposal distribution in the SampleList approximation.\n\n\n\n\n\n","category":"type"},{"location":"library/functional-forms/#RxInfer.RightProposal","page":"Built-in functional form constraints","title":"RxInfer.RightProposal","text":"Uses the right argument in the prod call as the proposal distribution in the SampleList approximation.\n\n\n\n\n\n","category":"type"},{"location":"library/functional-forms/#lib-forms-fixed-marginal-constraint","page":"Built-in functional form constraints","title":"FixedMarginalFormConstraint","text":"","category":"section"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"Fixed marginal form constraint replaces the resulting posterior marginal obtained during the inference procedure with the prespecified one. Worth to note that the inference backend still tries to compute real posterior marginal and may fail during this process. Might be useful for debugging purposes. If nothing is passed then the computed posterior marginal is returned.","category":"page"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"@constraints function block_updates(x_posterior = nothing) \n    # `nothing` returns the computed posterior marginal\n    q(x) :: Marginal(x_posterior)\nend","category":"page"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"RxInfer.FixedMarginalFormConstraint","category":"page"},{"location":"library/functional-forms/#RxInfer.FixedMarginalFormConstraint","page":"Built-in functional form constraints","title":"RxInfer.FixedMarginalFormConstraint","text":"FixedMarginalFormConstraint\n\nOne of the form constraint objects. Provides a constraint on the marginal distribution such that it remains fixed during inference.  Can be viewed as blocking of updates of a specific edge associated with the marginal. If nothing is passed then the computed posterior marginal is returned.\n\nTraits\n\nis_point_mass_form_constraint = false\ndefault_form_check_strategy   = FormConstraintCheckLast()\ndefault_prod_constraint       = GenericProd()\nmake_form_constraint          = Marginal (for use in @constraints macro)\n\n\n\n\n\n","category":"type"},{"location":"library/functional-forms/#lib-forms-composite-constraint","page":"Built-in functional form constraints","title":"CompositeFormConstraint","text":"","category":"section"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"It is possible to create a composite functional form constraint with either + operator or using @constraints macro, e.g:","category":"page"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"form_constraint = SampleListFormConstraint(1000) + PointMassFormConstraint()","category":"page"},{"location":"library/functional-forms/","page":"Built-in functional form constraints","title":"Built-in functional form constraints","text":"@constraints begin \n    q(x) :: SampleList(1000) :: PointMass()\nend","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/#examples-hierarchical-gaussian-filter","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"","category":"section"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"In this demo the goal is to perform approximate variational Bayesian Inference for Univariate Hierarchical Gaussian Filter (HGF).","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Simple HGF model can be defined as:","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"beginaligned\n  x^(j)_k  sim  mathcalN(x^(j)_k - 1 f_k(x^(j - 1)_k)) \n  y_k  sim  mathcalN(x^(j)_k tau_k)\nendaligned","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"where j is an index of layer in hierarchy, k is a time step and f_k is a variance activation function. RxInfer.jl export Gaussian Controlled Variance (GCV) node with f_k = exp(kappa x + omega) variance activation function. By default the node uses Gauss-Hermite cubature with a prespecified number of approximation points in the cubature. In this demo we also show how we can change the hyperparameters in different approximation methods (iin this case Gauss-Hermite cubature) with the help of metadata structures. Here how our model will look like with the GCV node:","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"beginaligned\n  z_k  sim  mathcalN(z_k - 1 mathcaltau_z) \n  x_k  sim  mathcalN(x_k - 1 exp(kappa z_k + omega)) \n  y_k  sim  mathcalN(x_k mathcaltau_y)\nendaligned","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"In this experiment we will create a single time step of the graph and perform variational message passing filtering alrogithm to estimate hidden states of the system. For a more rigorous introduction to Hierarchical Gaussian Filter we refer to Ismail Senoz, Online Message Passing-based Inference in the Hierarchical Gaussian Filter paper.","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"For simplicity we will consider tau_z, tau_y, kappa and omega known and fixed, but there are no principled limitations to make them random variables too.","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"To model this process in RxInfer, first, we start with importing all needed packages:","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"using RxInfer, BenchmarkTools, Random, Plots","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Next step, is to generate some synthetic data:","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"function generate_data(rng, k, w, zv, yv)\n    z_prev = 0.0\n    x_prev = 0.0\n\n    z = Vector{Float64}(undef, n)\n    v = Vector{Float64}(undef, n)\n    x = Vector{Float64}(undef, n)\n    y = Vector{Float64}(undef, n)\n\n    for i in 1:n\n        z[i] = rand(rng, Normal(z_prev, sqrt(zv)))\n        v[i] = exp(k * z[i] + w)\n        x[i] = rand(rng, Normal(x_prev, sqrt(v[i])))\n        y[i] = rand(rng, Normal(x[i], sqrt(yv)))\n\n        z_prev = z[i]\n        x_prev = x[i]\n    end \n    \n    return z, x, y\nend","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"generate_data (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"# Seed for reproducibility\nseed = 42\n\nrng = MersenneTwister(seed)\n\n# Parameters of HGF process\nreal_k = 1.0\nreal_w = 0.0\nz_variance = abs2(0.2)\ny_variance = abs2(0.1)\n\n# Number of observations\nn = 300\n\nz, x, y = generate_data(rng, real_k, real_w, z_variance, y_variance);","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"Let's plot our synthetic dataset. Lines represent our hidden states we want to estimate using noisy observations.","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"let \n    pz = plot(title = \"Hidden States Z\")\n    px = plot(title = \"Hidden States X\")\n    \n    plot!(pz, 1:n, z, label = \"z_i\", color = :orange)\n    plot!(px, 1:n, x, label = \"x_i\", color = :green)\n    scatter!(px, 1:n, y, label = \"y_i\", color = :red, ms = 2, alpha = 0.2)\n    \n    plot(pz, px, layout = @layout([ a; b ]))\nend","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"To create a model we use the @model macro:","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"# We create a single-time step of corresponding state-space process to\n# perform online learning (filtering)\n@model function hgf(real_k, real_w, z_variance, y_variance)\n    \n    # Priors from previous time step for `z`\n    zt_min_mean = datavar(Float64)\n    zt_min_var  = datavar(Float64)\n    \n    # Priors from previous time step for `x`\n    xt_min_mean = datavar(Float64)\n    xt_min_var  = datavar(Float64)\n\n    zt_min ~ NormalMeanVariance(zt_min_mean, zt_min_var)\n    xt_min ~ NormalMeanVariance(xt_min_mean, xt_min_var)\n\n    # Higher layer is modelled as a random walk \n    zt ~ NormalMeanVariance(zt_min, z_variance)\n    \n    # Lower layer is modelled with `GCV` node\n    gcvnode, xt ~ GCV(xt_min, zt, real_k, real_w)\n    \n    # Noisy observations \n    y = datavar(Float64)\n    y ~ NormalMeanVariance(xt, y_variance)\n    \n    return gcvnode\nend\n\n@constraints function hgfconstraints() \n    q(xt, zt, xt_min) = q(xt, xt_min)q(zt)\nend\n\n@meta function hgfmeta()\n    # Lets use 31 approximation points in the Gauss Hermite cubature approximation method\n    GCV(xt_min, xt, zt) -> GCVMetadata(GaussHermiteCubature(31)) \nend","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"hgfmeta (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"function run_inference(data, real_k, real_w, z_variance, y_variance)\n\n    autoupdates   = @autoupdates begin\n        zt_min_mean, zt_min_var = mean_var(q(zt))\n        xt_min_mean, xt_min_var = mean_var(q(xt))\n    end\n\n    return infer(\n        model         = hgf(real_k, real_w, z_variance, y_variance),\n        constraints   = hgfconstraints(),\n        meta          = hgfmeta(),\n        data          = (y = data, ),\n        autoupdates   = autoupdates,\n        keephistory   = length(data),\n        historyvars    = (\n            xt = KeepLast(),\n            zt = KeepLast()\n        ),\n        initmarginals = (\n            zt = NormalMeanVariance(0.0, 5.0),\n            xt = NormalMeanVariance(0.0, 5.0),\n        ), \n        iterations    = 5,\n        free_energy   = true,\n        autostart     = true,\n        callbacks     = (\n            after_model_creation = (model, returnval) -> begin \n                gcvnode = returnval\n                setmarginal!(gcvnode, :y_x, MvNormalMeanCovariance([ 0.0, 0.0 ], [ 5.0, 5.0 ]))\n            end,\n        )\n    )\nend","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"run_inference (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"result = run_inference(y, real_k, real_w, z_variance, y_variance);\n\nmz = result.history[:zt];\nmx = result.history[:xt];","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"let \n    pz = plot(title = \"Hidden States Z\")\n    px = plot(title = \"Hidden States X\")\n    \n    plot!(pz, 1:n, z, label = \"z_i\", color = :orange)\n    plot!(pz, 1:n, mean.(mz), ribbon = std.(mz), label = \"estimated z_i\", color = :teal)\n    \n    plot!(px, 1:n, x, label = \"x_i\", color = :green)\n    plot!(px, 1:n, mean.(mx), ribbon = std.(mx), label = \"estimated x_i\", color = :violet)\n    \n    plot(pz, px, layout = @layout([ a; b ]))\nend","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"As we can see from our plot, estimated signal resembles closely to the real hidden states with small variance. We maybe also interested in the values for Bethe Free Energy functional:","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"plot(result.free_energy_history, label = \"Bethe Free Energy\")","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"As we can see BetheFreeEnergy converges nicely to a stable point.","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"At final, lets check the overall performance of our resulting Variational Message Passing algorithm:","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"@benchmark run_inference($y, $real_k, $real_w, $z_variance, $y_variance)","category":"page"},{"location":"examples/problem_specific/Hierarchical Gaussian Filter/","page":"Hierarchical Gaussian Filter","title":"Hierarchical Gaussian Filter","text":"BenchmarkTools.Trial: 40 samples with 1 evaluation.\n Range (min … max):  106.663 ms … 184.785 ms  ┊ GC (min … max): 0.00% … 0.0\n0%\n Time  (median):     125.543 ms               ┊ GC (median):    0.00%\n Time  (mean ± σ):   127.181 ms ±  14.793 ms  ┊ GC (mean ± σ):  1.51% ± 2.7\n2%\n\n   ▂     ▂     ▅█ ▅ ▂                                            \n  ▅█▁▁█▅▅█▁▁▁▁▅████▅█▁▁▁▅█▅▁▁▅▁▁▅▁▁▁▅▁▁▁▁▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅ ▁\n  107 ms           Histogram: frequency by time          185 ms <\n\n Memory estimate: 18.82 MiB, allocs estimate: 458387.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = RxInfer","category":"page"},{"location":"#RxInfer","page":"Home","title":"RxInfer","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"<div class=\"light-biglogo\">","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: RxInfer Logo)","category":"page"},{"location":"","page":"Home","title":"Home","text":"</div>","category":"page"},{"location":"","page":"Home","title":"Home","text":"<div class=\"dark-biglogo\">","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: RxInfer Logo)","category":"page"},{"location":"","page":"Home","title":"Home","text":"</div>","category":"page"},{"location":"","page":"Home","title":"Home","text":"Julia package for automatic Bayesian inference on a factor graph with reactive message passing.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Given a probabilistic model, RxInfer allows for an efficient message-passing based Bayesian inference. It uses the model structure to generate an algorithm that consists of a sequence of local computations on a Forney-style factor graph (FFG) representation of the model. RxInfer.jl has been designed with a focus on efficiency, scalability and maximum performance for running inference with message passing.","category":"page"},{"location":"#Package-Features","page":"Home","title":"Package Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"User friendly syntax for specification of probabilistic models.\nAutomatic generation of message passing algorithms including\nBelief propagation\nVariational message passing\nExpectation maximization\nSupport for hybrid models combining discrete and continuous latent variables.\nSupport for hybrid distinct message passing inference algorithm under a unified paradigm.\nFactorisation and functional form constraints specification.\nEvaluation of Bethe free energy as a model performance measure.\nSchedule-free reactive message passing API.\nHigh performance.\nScalability for large models with millions of parameters and observations.\nInference procedure is differentiable.\nEasy to extend with custom nodes and message update rules.","category":"page"},{"location":"#Why-RxInfer","page":"Home","title":"Why RxInfer","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Many important AI applications, including audio processing, self-driving vehicles, weather forecasting, and extended-reality video processing require continually solving an inference task in sophisticated probabilistic models with a large number of latent variables. Often, the inference task in these applications must be performed continually and in real-time in response to new observations. Popular MC-based inference methods, such as the No U-Turn Sampler (NUTS) or Hamiltonian Monte Carlo (HMC) sampling, rely on computationally heavy sampling procedures that do not scale well to probabilistic models with thousands of latent states. Therefore, MC-based inference is practically not suitable for real-time applications. While the alternative variational inference method (VI) promises to scale better to large models than sampling-based inference, VI requires the derivation of gradients of a \"variational Free Energy\" cost function. For large models, manual derivation of these gradients might not be feasible, while automated \"black-box\" gradient methods do not scale either because they are not capable of taking advantage of sparsity or conjugate pairs in the model. Therefore, while Bayesian inference is known as the optimal data processing framework, in practice, real-time AI applications rely on much simpler, often ad hoc, data processing algorithms.","category":"page"},{"location":"","page":"Home","title":"Home","text":"RxInfer aims to remedy these issues by running efficient Bayesian inference in sophisticated probabilistic models, taking advantage of local conjugate relationships in probabilistic models, and focusing on real-time Bayesian inference in large state-space models with thousands of latent variables. In addition, RxInfer provides a straightforward way to extend its functionality with custom factor nodes and message passing update rules. The engine is capable of running various Bayesian inference algorithms in different parts of the factor graph of a single probabilistic model. This makes it easier to explore different \"what-if\" scenarios and enables very efficient inference in specific cases.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Curious about how RxInfer compares to other tools you might be considering? We invite you to view a detailed comparison, where we put RxInfer head-to-head with other popular packages in the field.","category":"page"},{"location":"#How-to-get-started?","page":"Home","title":"How to get started?","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Head to the Getting started section to get up and running with RxInfer. Alternatively, explore various examples in the documentation.","category":"page"},{"location":"#Table-of-Contents","page":"Home","title":"Table of Contents","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n  \"manuals/background.md\",\n  \"manuals/comparison.md\",\n  \"manuals/getting-started.md\",\n  \"manuals/model-specification.md\",\n  \"manuals/constraints-specification.md\",\n  \"manuals/meta-specification.md\",\n  \"manuals/inference-execution.md\",\n  \"manuals/custom-node.md\",\n  \"manuals/debugging.md\",\n  \"manuals/delta-node.md\",\n  \"examples/overview.md\",\n  \"library/exported-methods.md\",\n  \"library/functional-forms.md\",\n  \"contributing/overview.md\",\n  \"contributing/new-example.md\"\n]\nDepth = 2","category":"page"},{"location":"#References","page":"Home","title":"References","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"RxInfer: A Julia package for reactive real-time Bayesian inference - a reference paper for the RxInfer.jl framwork.\nVariational Message Passing and Local Constraint Manipulation in Factor Graphs - describes theoretical aspects of the underlying Bayesian inference method.\nReactive Message Passing for Scalable Bayesian Inference - describes implementation aspects of the Bayesian inference engine and performs benchmarks and accuracy comparison on various models.\nA Julia package for reactive variational Bayesian inference - a reference paper for the ReactiveMP.jl package, the underlying inference engine.\nThe Factor Graph Approach to Model-Based Signal Processing - an introduction to message passing and FFGs.","category":"page"},{"location":"#Ecosystem","page":"Home","title":"Ecosystem","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The RxInfer unites 3 core packages into one powerful reactive message passing-based Bayesian inference framework:","category":"page"},{"location":"","page":"Home","title":"Home","text":"ReactiveMP.jl - core package for efficient and scalable for reactive message passing \nGraphPPL.jl - package for model and constraints specification\nRocket.jl - reactive programming tools","category":"page"},{"location":"#Index","page":"Home","title":"Index","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"manuals/inference/infer/#user-guide-inference","page":"Static vs Streamline inference","title":"Automatic Inference Specification","text":"","category":"section"},{"location":"manuals/inference/infer/","page":"Static vs Streamline inference","title":"Static vs Streamline inference","text":"RxInfer provides the infer function for quickly running and testing your model with both static and streaming datasets. To enable streaming behavior, the infer function accepts an autoupdates argument, which specifies how to update your priors for future states based on newly updated posteriors. ","category":"page"},{"location":"manuals/inference/infer/","page":"Static vs Streamline inference","title":"Static vs Streamline inference","text":"It's important to note that while this function covers most capabilities of the inference engine, advanced use cases may require resorting to the Manual Inference Specification.","category":"page"},{"location":"manuals/inference/infer/","page":"Static vs Streamline inference","title":"Static vs Streamline inference","text":"For details on manual inference specification, see the Manual Inference section.","category":"page"},{"location":"manuals/inference/infer/","page":"Static vs Streamline inference","title":"Static vs Streamline inference","text":"infer\nInferenceResult\nRxInfer.start\nRxInfer.stop\n@autoupdates\nRxInferenceEngine\nRxInferenceEvent","category":"page"},{"location":"manuals/inference/infer/#RxInfer.infer","page":"Static vs Streamline inference","title":"RxInfer.infer","text":"infer(\n    model; \n    data = nothing,\n    datastream = nothing,\n    autoupdates = nothing,\n    initmarginals = nothing,\n    initmessages = nothing,\n    constraints = nothing,\n    meta = nothing,\n    options = nothing,\n    returnvars = nothing, \n    predictvars = nothing, \n    historyvars = nothing,\n    keephistory = nothing,\n    iterations = nothing,\n    free_energy = false,\n    free_energy_diagnostics = BetheFreeEnergyDefaultChecks,\n    showprogress = false,\n    callbacks = nothing,\n    addons = nothing,\n    postprocess = DefaultPostprocess(),\n    warn = true,\n    events = nothing,\n    uselock = false,\n    autostart = true,\n    catch_exception = false\n)\n\nThis function provides a generic way to perform probabilistic inference for batch/static and streamline/online scenarios. Returns an InferenceResult (batch setting) or RxInferenceEngine (streamline setting) based on the parameters used.\n\nArguments\n\nFor more information about some of the arguments, please check below. \n\nmodel: specifies a model generator, required\ndata: NamedTuple or Dict with data, required (or datastream or predictvars)\ndatastream: A stream of NamedTuple with data, required (or data)\nautoupdates = nothing: auto-updates specification, required for streamline inference, see @autoupdates\ninitmarginals = nothing: NamedTuple or Dict with initial marginals, optional\ninitmessages = nothing: NamedTuple or Dict with initial messages, optional\nconstraints = nothing: constraints specification object, optional, see @constraints\nmeta  = nothing: meta specification object, optional, may be required for some models, see @meta\noptions = nothing: model creation options, optional, see ModelInferenceOptions\nreturnvars = nothing: return structure info, optional, defaults to return everything at each iteration, see below for more information\npredictvars = nothing: return structure info, optional, see below for more information (exclusive for batch inference)\nhistoryvars = nothing: history structure info, optional, defaults to no history, see below for more information (exclusive for streamline inference)\nkeephistory = nothing: history buffer size, defaults to empty buffer, see below for more information (exclusive for streamline inference)\niterations = nothing: number of iterations, optional, defaults to nothing, the inference engine does not distinguish between variational message passing or Loopy belief propagation or expectation propagation iterations, see below for more information\nfree_energy = false: compute the Bethe free energy, optional, defaults to false. Can be passed a floating point type, e.g. Float64, for better efficiency, but disables automatic differentiation packages, such as ForwardDiff.jl\nfree_energy_diagnostics = BetheFreeEnergyDefaultChecks: free energy diagnostic checks, optional, by default checks for possible NaNs and Infs. nothing disables all checks.\nshowprogress = false: show progress module, optional, defaults to false (exclusive for batch inference)\ncatch_exception  specifies whether exceptions during the inference procedure should be caught, optional, defaults to false (exclusive for batch inference)\ncallbacks = nothing: inference cycle callbacks, optional, see below for more info\naddons = nothing: inject and send extra computation information along messages, see below for more info\npostprocess = DefaultPostprocess(): inference results postprocessing step, optional, see below for more info\nevents = nothing: inference cycle events, optional, see below for more info (exclusive for streamline inference)\nuselock = false: specifies either to use the lock structure for the inference or not, if set to true uses Base.Threads.SpinLock. Accepts custom AbstractLock. (exclusive for streamline inference)\nautostart = true: specifies whether to call RxInfer.start on the created engine automatically or not (exclusive for streamline inference)\nwarn = true: enables/disables warnings\n\nNote on NamedTuples\n\nWhen passing NamedTuple as a value for some argument, make sure you use a trailing comma for NamedTuples with a single entry. The reason is that Julia treats returnvars = (x = KeepLast()) and returnvars = (x = KeepLast(), ) expressions differently. This first expression creates (or overwrites!) new local/global variable named x with contents KeepLast(). The second expression (note trailing comma) creates NamedTuple with x as a key and KeepLast() as a value assigned for this key.\n\nThe model argument accepts a ModelGenerator as its input. The easiest way to create the ModelGenerator is to use the @model macro.  For example:\n\n@model function coin_toss(some_argument, some_keyword_argument = 3)\n    ...\nend\n\nresult = infer(\n    model = coin_toss(some_argument; some_keyword_argument = 3)\n)\n\nNote: The model keyword argument does not accept a FactorGraphModel instance as a value, as it needs to inject constraints and meta during the inference procedure.\n\ndata\n\nEither data or datastream or predictvars keyword argument is required. Specifying both data and datastream is not supported and will result in an error. Specifying both datastream and predictvars is not supported and will result in an error.\n\nNote: The behavior of the data keyword argument depends on the inference setting (batch or streamline).\n\nThe data keyword argument must be a NamedTuple (or Dict) where keys (of Symbol type) correspond to all datavars defined in the model specification. For example, if a model defines x = datavar(Float64) the  data field must have an :x key (of Symbol type) which holds a value of type Float64. The values in the data must have the exact same shape as the datavar container. In other words, if a model defines x = datavar(Float64, n) then  data[:x] must provide a container with length n and with elements of type Float64.\n\nstreamline setting\n\nAll entries in the data argument are zipped together with the Base.zip function to form one slice of the data chunck. This means all containers in the data argument must be of the same size (zip iterator finished as soon as one container has no remaining values). In order to use a fixed value for some specific datavar it is not necessary to create a container with that fixed value, but rather more efficient to use Iterators.repeated to create an infinite iterator.\n\ndatastream\n\nThe datastream keyword argument must be an observable that supports subscribe! and unsubscribe! functions (streams from the Rocket.jl package are also supported). The elements of the observable must be of type NamedTuple where keys (of Symbol type) correspond to all datavars defined in the model specification, except for those which are listed in the autoupdates specification.  For example, if a model defines x = datavar(Float64) (which is not part of the autoupdates specification) the named tuple from the observable must have an :x key (of Symbol type) which holds a value of type Float64. The values in the named tuple must have the exact same shape as the datavar container. In other words, if a model defines x = datavar(Float64, n) then  namedtuple[:x] must provide a container with length n and with elements of type Float64.\n\nNote: The behavior of the individual named tuples from the datastream observable is similar to that which is used in the batch setting. In fact, you can see the streamline inference as an efficient version of the batch inference, which automatically updates some datavars with the autoupdates specification and listens to the datastream to update the rest of the datavars.\n\nFor specific types of inference algorithms, such as variational message passing, it might be required to initialize (some of) the marginals before running the inference procedure in order to break the dependency loop. If this is not done, the inference algorithm will not be executed due to the lack of information and message and/or marginals will not be updated. In order to specify these initial marginals, you can use the initmarginals argument, such as\n\ninfer(...\n    initmarginals = (\n        # initialize the marginal distribution of x as a vague Normal distribution\n        # if x is a vector, then it simply uses the same value for all elements\n        # However, it is also possible to provide a vector of distributions to set each element individually \n        x = vague(NormalMeanPrecision),  \n    ),\n)\n\nThis argument needs to be a named tuple, i.e. `initmarginals = (a = ..., )`, or dictionary.\n\n- ### `initmessages`\n\nFor specific types of inference algorithms, such as loopy belief propagation or expectation propagation, it might be required to initialize (some of) the messages before running the inference procedure in order to break the dependency loop. If this is not done, the inference algorithm will not be executed due to the lack of information and message and/or marginals will not be updated. In order to specify these initial messages, you can use the `initmessages` argument, such as\n\njulia infer(...     initmessages = (         # initialize the messages distribution of x as a vague Normal distribution         # if x is a vector, then it simply uses the same value for all elements         # However, it is also possible to provide a vector of distributions to set each element individually          x = vague(NormalMeanPrecision),       ), )\n\noptions\nlimit_stack_depth: limits the stack depth for computing messages, helps with StackOverflowError for some huge models, but reduces the performance of inference backend. Accepts integer as an argument that specifies the maximum number of recursive depth. Lower is better for stack overflow error, but worse for performance.\npipeline: changes the default pipeline for each factor node in the graph\nglobal_reactive_scheduler: changes the scheduler of reactive streams, see Rocket.jl for more info, defaults to no scheduler\nreturnvars\n\nreturnvars specifies latent variables of interest and their posterior updates. Its behavior depends on the inference type: streamline or batch.\n\nBatch inference:\n\nAccepts a NamedTuple or Dict of return variable specifications.\nTwo specifications available: KeepLast (saves the last update) and KeepEach (saves all updates).\nWhen iterations is set, returns every update for each iteration (equivalent to KeepEach()); if nothing, saves the last update (equivalent to KeepLast()).\nUse iterations = 1 to force KeepEach() for a single iteration or set returnvars = KeepEach() manually.\n\nExample:\n\nresult = infer(\n    ...,\n    returnvars = (\n        x = KeepLast(),\n        τ = KeepEach()\n    )\n)\n\nShortcut for setting the same option for all variables:\n\nresult = infer(\n    ...,\n    returnvars = KeepLast()  # or KeepEach()\n)\n\nStreamline inference:\n\nFor each symbol in returnvars, infer creates an observable stream of posterior updates.\nAgents can subscribe to these updates using the Rocket.jl package.\n\nExample:\n\nengine = infer(\n    ...,\n    autoupdates = my_autoupdates,\n    returnvars = (:x, :τ),\n    autostart  = false\n)\n\npredictvars\n\npredictvars specifies the variables which should be predicted. In the model definition these variables are specified as datavars, although they should not be passed inside data argument.\n\nSimilar to returnvars, predictvars accepts a NamedTuple or Dict. There are two specifications:\n\nKeepLast: saves the last update for a variable, ignoring any intermediate results during iterations\nKeepEach: saves all updates for a variable for all iterations\n\nExample: \n\nresult = infer(\n    ...,\n    predictvars = (\n        o = KeepLast(),\n        τ = KeepEach()\n    )\n)\n\nNote: The predictvars argument is exclusive for batch setting.\n\nhistoryvars\n\nhistoryvars specifies the variables of interests and the amount of information to keep in history about the posterior updates when performing streamline inference. The specification is similar to the returnvars when applied in batch setting. The historyvars requires keephistory to be greater than zero.\n\nhistoryvars accepts a NamedTuple or Dict or return var specification. There are two specifications:\n\nKeepLast: saves the last update for a variable, ignoring any intermediate results during iterations\nKeepEach: saves all updates for a variable for all iterations\n\nExample: \n\nresult = infer(\n    ...,\n    autoupdates = my_autoupdates,\n    historyvars = (\n        x = KeepLast(),\n        τ = KeepEach()\n    ),\n    keephistory = 10\n)\n\nIt is also possible to set either historyvars = KeepLast() or historyvars = KeepEach() that acts as an alias and sets the given option for all random variables in the model.\n\nExample:\n\nresult = infer(\n    ...,\n    autoupdates = my_autoupdates,\n    historyvars = KeepLast(),\n    keephistory = 10\n)\n\nkeep_history\n\nSpecifies the buffer size for the updates history both for the historyvars and the free_energy buffers in streamline inference.\n\niterations\n\nSpecifies the number of variational (or loopy belief propagation) iterations. By default set to nothing, which is equivalent of doing 1 iteration. \n\nfree_energy\n\nStreamline inference:\n\nSpecifies if the infer function should create an observable stream of Bethe Free Energy (BFE) values, computed at each VMP iteration.\n\nWhen free_energy = true and keephistory > 0, additional fields are exposed in the engine for accessing the history of BFE updates.\nengine.free_energy_history: Averaged BFE history over VMP iterations.\nengine.free_energy_final_only_history: BFE history of values computed in the last VMP iterations for each observation.\nengine.free_energy_raw_history: Raw BFE history.\n\nBatch inference:\n\nSpecifies if the infer function should return Bethe Free Energy (BFE) values.\n\nOptionally accepts a floating-point type (e.g., Float64) for improved BFE computation performance, but restricts the use of automatic differentiation packages.\nfree_energy_diagnostics\n\nThis settings specifies either a single or a tuple of diagnostic checks for Bethe Free Energy values stream. By default checks for NaNs and Infs. See also BetheFreeEnergyCheckNaNs and BetheFreeEnergyCheckInfs. Pass nothing to disable any checks.\n\ncatch_exception\n\nThe catch_exception keyword argument specifies whether exceptions during the batch inference procedure should be caught in the error field of the  result. By default, if exception occurs during the inference procedure the result will be lost. Set catch_exception = true to obtain partial result  for the inference in case if an exception occurs. Use RxInfer.issuccess and RxInfer.iserror function to check if the inference completed successfully or failed. If an error occurs, the error field will store a tuple, where first element is the exception itself and the second element is the caught backtrace. Use the stacktrace function  with the backtrace as an argument to recover the stacktrace of the error. Use Base.showerror function to display the error.\n\ncallbacks\n\nThe inference function has its own lifecycle. The user is free to provide some (or none) of the callbacks to inject some extra logging or other procedures in the inference function, e.g.\n\nresult = infer(\n    ...,\n    callbacks = (\n        on_marginal_update = (model, name, update) -> println(\"$(name) has been updated: $(update)\"),\n        after_inference    = (args...) -> println(\"Inference has been completed\")\n    )\n)\n\nThe callbacks keyword argument accepts a named-tuple of 'name = callback' pairs.  The list of all possible callbacks for different inference setting (batch or streamline) and their arguments is present below:\n\non_marginal_update:    args: (model::FactorGraphModel, name::Symbol, update) (exlusive for batch inference)\nbefore_model_creation: args: ()\nafter_model_creation:  args: (model::FactorGraphModel, returnval)\nbefore_inference:      args: (model::FactorGraphModel) (exlusive for batch inference)\nbefore_iteration:      args: (model::FactorGraphModel, iteration::Int)::Bool (exlusive for batch inference)\nbefore_data_update:    args: (model::FactorGraphModel, data) (exlusive for batch inference)\nafter_data_update:     args: (model::FactorGraphModel, data) (exlusive for batch inference)\nafter_iteration:       args: (model::FactorGraphModel, iteration::Int)::Bool (exlusive for batch inference)\nafter_inference:       args: (model::FactorGraphModel) (exlusive for batch inference)\nbefore_autostart:      args: (engine::RxInferenceEngine) (exlusive for streamline inference)\nafter_autostart:       args: (engine::RxInferenceEngine) (exlusive for streamline inference)\n\nbefore_iteration and after_iteration callbacks are allowed to return true/false value. true indicates that iterations must be halted and no further inference should be made.\n\naddons\n\nThe addons field extends the default message computation rules with some extra information, e.g. computing log-scaling factors of messages or saving debug-information. Accepts a single addon or a tuple of addons. If set, replaces the corresponding setting in the options. Automatically changes the default value of the postprocess argument to NoopPostprocess.\n\npostprocess\n\nThe postprocess keyword argument controls whether the inference results must be modified in some way before exiting the inference function. By default, the inference function uses the DefaultPostprocess strategy, which by default removes the Marginal wrapper type from the results. Change this setting to NoopPostprocess if you would like to keep the Marginal wrapper type, which might be useful in the combination with the addons argument. If the addons argument has been used, automatically changes the default strategy value to NoopPostprocess.\n\n\n\n\n\n","category":"function"},{"location":"manuals/inference/infer/#RxInfer.InferenceResult","page":"Static vs Streamline inference","title":"RxInfer.InferenceResult","text":"InferenceResult\n\nThis structure is used as a return value from the infer function. \n\nPublic Fields\n\nposteriors: Dict or NamedTuple of 'random variable' - 'posterior' pairs. See the returnvars argument for infer.\nfree_energy: (optional) An array of Bethe Free Energy values per VMP iteration. See the free_energy argument for infer.\nmodel: FactorGraphModel object reference.\nreturnval: Return value from executed @model.\nerror: (optional) A reference to an exception, that might have occurred during the inference. See the catch_exception argument for infer.\n\nSee also: infer\n\n\n\n\n\n","category":"type"},{"location":"manuals/inference/infer/#RxInfer.start","page":"Static vs Streamline inference","title":"RxInfer.start","text":"start(engine::RxInferenceEngine)\n\nStarts the RxInferenceEngine by subscribing to the data source, instantiating free energy (if enabled) and starting the event loop. Use RxInfer.stop to stop the RxInferenceEngine. Note that it is not always possible to stop/restart the engine and this depends on the data source type.\n\nSee also: RxInfer.stop\n\n\n\n\n\n","category":"function"},{"location":"manuals/inference/infer/#RxInfer.stop","page":"Static vs Streamline inference","title":"RxInfer.stop","text":"stop(engine::RxInferenceEngine)\n\nStops the RxInferenceEngine by unsubscribing to the data source, free energy (if enabled) and stopping the event loop. Use RxInfer.start to start the RxInferenceEngine again. Note that it is not always possible to stop/restart the engine and this depends on the data source type.\n\nSee also: RxInfer.start\n\n\n\n\n\n","category":"function"},{"location":"manuals/inference/infer/#RxInfer.@autoupdates","page":"Static vs Streamline inference","title":"RxInfer.@autoupdates","text":"@autoupdates\n\nCreates the auto-updates specification for the rxinference function. In the online-streaming Bayesian inference procedure it is important to update your priors for the future  states based on the new updated posteriors. The @autoupdates structure simplify such a specification. It accepts a single block of code where each line defines how to update  the datavar's in the probabilistic model specification. \n\nEach line of code in the auto-update specification defines datavars, which need to be updated, on the left hand side of the equality expression and the update function on the right hand side of the expression. The update function operates on posterior marginals in the form of the q(symbol) expression.\n\nFor example:\n\n@autoupdates begin \n    x = f(q(z))\nend\n\nThis structure specifies to automatically update x = datavar(...) as soon as the inference engine computes new posterior over z variable. It then applies the f function to the new posterior and calls update!(x, ...) automatically. \n\nAs an example consider the following model and auto-update specification:\n\n@model function kalman_filter()\n    x_current_mean = datavar(Float64)\n    x_current_var  = datavar(Float64)\n\n    x_current ~ Normal(mean = x_current_mean, var = x_current_var)\n\n    x_next ~ Normal(mean = x_current, var = 1.0)\n\n    y = datavar(Float64)\n    y ~ Normal(mean = x_next, var = 1.0)\nend\n\nThis model has two datavars that represent our prior knowledge of the x_current state of the system. The x_next random variable represent the next state of the system that  is connected to the observed variable y. The auto-update specification could look like:\n\nautoupdates = @autoupdates begin\n    x_current_mean, x_current_var = mean_cov(q(x_next))\nend\n\nThis structure specifies to update our prior as soon as we have a new posterior q(x_next). It then applies the mean_cov function on the updated posteriors and updates  datavars x_current_mean and x_current_var automatically.\n\nSee also: infer\n\n\n\n\n\n","category":"macro"},{"location":"manuals/inference/infer/#RxInfer.RxInferenceEngine","page":"Static vs Streamline inference","title":"RxInfer.RxInferenceEngine","text":"RxInferenceEngine\n\nThe return value of the rxinference function. \n\nPublic fields\n\nposteriors: Dict or NamedTuple of 'random variable' - 'posterior stream' pairs. See the returnvars argument for the infer.\nfree_energy: (optional) A stream of Bethe Free Energy values per VMP iteration. See the free_energy argument for the infer.\nhistory: (optional) Saves history of previous marginal updates. See the historyvars and keephistory arguments for the infer.\nfree_energy_history: (optional) Free energy history, average over variational iterations \nfree_energy_raw_history: (optional) Free energy history, returns returns computed values of all variational iterations for each data event (if available)\nfree_energy_final_only_history: (optional) Free energy history, returns computed values of final variational iteration for each data event (if available)\nevents: (optional) A stream of events send by the inference engine. See the events argument for the infer.\nmodel: FactorGraphModel object reference.\nreturnval: Return value from executed @model.\n\nUse the RxInfer.start(engine) function to subscribe on the data source and start the inference procedure. Use RxInfer.stop(engine) to unsubscribe from the data source and stop the inference procedure.  Note, that it is not always possible to start/stop the inference procedure.\n\nSee also: infer, RxInferenceEvent, RxInfer.start, RxInfer.stop\n\n\n\n\n\n","category":"type"},{"location":"manuals/inference/infer/#RxInfer.RxInferenceEvent","page":"Static vs Streamline inference","title":"RxInfer.RxInferenceEvent","text":"RxInferenceEvent{T, D}\n\nThe RxInferenceEngine sends events in a form of the RxInferenceEvent structure. T represents the type of an event, D represents the type of a data associated with the event. The type of data depends on the type of an event, but usually represents a tuple, which can be unrolled automatically with the Julia's splitting syntax, e.g. model, iteration = event.  See the documentation of the rxinference function for possible event types and their associated data types.\n\nThe events system itself uses the Rocket.jl library API. For example, one may create a custom event listener in the following way:\n\nusing Rocket\n\nstruct MyEventListener <: Rocket.Actor{RxInferenceEvent}\n    # ... extra fields\nend\n\nfunction Rocket.on_next!(listener::MyEventListener, event::RxInferenceEvent{ :after_iteration })\n    model, iteration = event\n    println(\"Iteration $(iteration) has been finished.\")\nend\n\nfunction Rocket.on_error!(listener::MyEventListener, err)\n    # ...\nend\n\nfunction Rocket.on_complete!(listener::MyEventListener)\n    # ...\nend\n\n\nand later on:\n\nengine = infer(events = Val((:after_iteration, )), ...)\n\nsubscription = subscribe!(engine.events, MyEventListener(...))\n\nSee also: infer, RxInferenceEngine\n\n\n\n\n\n","category":"type"},{"location":"examples/advanced_examples/overview/#examples-advanced_examples-overview","page":"Overview","title":"Advanced examples","text":"","category":"section"},{"location":"examples/advanced_examples/overview/","page":"Overview","title":"Overview","text":"This section contains a set of examples for Bayesian Inference with RxInfer package in various probabilistic models.","category":"page"},{"location":"examples/advanced_examples/overview/","page":"Overview","title":"Overview","text":"note: Note\nAll examples have been pre-generated automatically from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/advanced_examples/overview/","page":"Overview","title":"Overview","text":"Advanced examples contain more complex inference problems.","category":"page"},{"location":"examples/advanced_examples/overview/","page":"Overview","title":"Overview","text":"Active Inference Mountain car: This notebooks covers RxInfer usage in the Active Inference setting for the simple mountain car problem.\nAdvanced Tutorial: This notebook covers the fundamentals and advanced usage of the RxInfer.jl package.\nAssessing People’s Skills: The demo is inspired by the example from Chapter 2 of Bishop's Model-Based Machine Learning book. We are going to perform an exact inference to assess the skills of a student given the results of the test.\nChance-Constrained Active Inference: This notebook applies reactive message passing for active inference in the context of chance-constraints.\nConjugate-Computational Variational Message Passing (CVI): This example provides an extensive tutorial for the non-conjugate message-passing based inference by exploiting the local CVI approximation.\nGlobal Parameter Optimisation: This example shows how to use RxInfer.jl automated inference within other optimisation packages such as Optim.jl.\nSolve GP regression by SDE: In this notebook, we solve a GP regression problem by using 'Stochastic Differential Equation' (SDE). This method is well described in the dissertation 'Stochastic differential equation methods for spatio-temporal Gaussian process regression.' by Arno Solin and 'Sequential Inference for Latent Temporal Gaussian Process Models' by Jouni Hartikainen.\nInfinite Data Stream: This example shows RxInfer capabilities of running inference for infinite time-series data.\nNonlinear Sensor Fusion: Nonlinear object position identification using a sparse set of sensors","category":"page"},{"location":"manuals/constraints-specification/#user-guide-constraints-specification","page":"Constraints specification","title":"Constraints Specification","text":"","category":"section"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"RxInfer.jl exports @constraints macro for the extra constraints specification that can be used during the inference step in ReactiveMP.jl engine package.","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"using RxInfer","category":"page"},{"location":"manuals/constraints-specification/#General-syntax","page":"Constraints specification","title":"General syntax","text":"","category":"section"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints macro accepts either regular Julia function or a single begin ... end block. For example both are valid:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"\n# `functional` style\n@constraints function create_my_constraints(arg1, arg2)\n    ...\nend\n\n# `block` style\nmyconstraints = @constraints begin \n    ...\nend\n","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"In the first case it returns a function that return constraints upon calling, e.g. ","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints function make_constraints(mean_field)\n    q(x) :: PointMass\n\n    if mean_field\n        q(x, y) = q(x)q(y)\n    end\nend\n\nmyconstraints = make_constraints(true)","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"and in the second case it evaluates automatically and returns constraints object directly.","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"myconstraints = @constraints begin \n    q(x) :: PointMass\n    q(x, y) = q(x)q(y)\nend","category":"page"},{"location":"manuals/constraints-specification/#Options-specification","page":"Constraints specification","title":"Options specification","text":"","category":"section"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints macro accepts optional list of options as a first argument and specified as an array of key = value pairs, e.g. ","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"myconstraints = @constraints [ warn = false ] begin \n   ...\nend","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"List of available options:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"warn::Bool - enables/disables various warnings with an incompatible model/constraints specification","category":"page"},{"location":"manuals/constraints-specification/#Marginal-and-messages-form-constraints","page":"Constraints specification","title":"Marginal and messages form constraints","text":"","category":"section"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"To specify marginal or messages form constraints @constraints macro uses :: operator (in somewhat similar way as Julia uses it for multiple dispatch type specification)","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"The following constraint:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints begin \n    q(x) :: PointMass\nend","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"indicates that the resulting marginal of the variable (or array of variables) named x must be approximated with a PointMass object. Message passing based algorithms compute posterior marginals as a normalized product of two colliding messages on corresponding edges of a factor graph. In a few words q(x)::PointMass reads as:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"mathrmapproximate q(x) = fracoverrightarrowmu(x)overleftarrowmu(x)int overrightarrowmu(x)overleftarrowmu(x) mathrmdxmathrmasPointMass","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"Sometimes it might be useful to set a functional form constraint on messages too. For example if it is essential to keep a specific Gaussian parametrisation or if some messages are intractable and need approximation. To set messages form constraint @constraints macro uses μ(...) instead of q(...):","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints begin \n    q(x) :: PointMass\n    μ(x) :: SampleList(1000)\n    # it is possible to assign different form constraints on the same variable \n    # both for the marginal and for the messages \nend","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints macro understands \"stacked\" form constraints. For example the following form constraint","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints begin \n    q(x) :: SampleList(1000) :: PointMass\nend","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"indicates that the q(x) first must be approximated with a SampleList and in addition the result of this approximation should be approximated as a PointMass. ","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"note: Note\nNot all combinations of \"stacked\" form constraints are compatible between each other.","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"You can find more information about built-in functional form constraint in the Built-in Functional Forms section. In addition, the ReactiveMP library documentation explains the functional form interfaces and shows how to build a custom functional form constraint that is compatible with RxInfer.jl and ReactiveMP.jl inference engine.","category":"page"},{"location":"manuals/constraints-specification/#Factorisation-constraints-on-posterior-distribution-q()","page":"Constraints specification","title":"Factorisation constraints on posterior distribution q()","text":"","category":"section"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@model macro specifies generative model p(s, y) where s is a set of random variables and y is a set of observations. In a nutshell the goal of probabilistic programming is to find p(s|y). RxInfer approximates p(s|y) with a proxy distribution q(x) using KL divergence and Bethe Free Energy optimisation procedure. By default there are no extra factorisation constraints on q(s) and the optimal solution is q(s) = p(s|y). However, inference may be not tractable for every model without extra factorisation constraints. To circumvent this, RxInfer.jl and ReactiveMP.jl accept optional factorisation constraints specification syntax:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"For example:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints begin \n    q(x, y) = q(x)q(y)\nend","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"specifies a so-called mean-field assumption on variables x and y in the model. Furthermore, if x is an array of variables in our model we may induce extra mean-field assumption on x in the following way.","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints begin \n    q(x) = q(x[begin])..q(x[end])\n    q(x, y) = q(x)q(y)\nend","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"These constraints specify a mean-field assumption between variables x and y (either single variable or collection of variables) and additionally specify mean-field assumption on variables x_i.","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"note: Note\n@constraints macro does not support matrix-based collections of variables. E.g. it is not possible to write q(x[begin, begin])..q(x[end, end])","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"It is possible to write more complex factorisation constraints, for example:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints begin \n    q(x, y) = q(x[begin], y[begin])..q(x[end], y[end])\nend","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"specifies a mean-field assumption between collection of variables named x and y only for variables with different indices. Another example is","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@constraints function make_constraints(k)\n    q(x) = q(x[begin:k])q(x[k+1:end])\nend","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"In this example we specify a mean-field assumption between a set of variables x[begin:k] and x[k+1:end]. ","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"To create a model with extra constraints the user may pass an optional constraints keyword argument for the create_model function:","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"@model function my_model(arguments...)\n   ...\nend\n\nconstraints = @constraints begin \n    ...\nend\n\nmodel, returnval = create_model(my_model(arguments...); constraints = constraints)","category":"page"},{"location":"manuals/constraints-specification/","page":"Constraints specification","title":"Constraints specification","text":"Alternatively, it is possible to use constraints directly in the automatic infer function that accepts constraints keyword argument. ","category":"page"},{"location":"contributing/overview/#contributing-overview","page":"Overview","title":"Contributing","text":"","category":"section"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"We welcome all possible contributors. This page details some of the guidelines that should be followed when contributing to this package.","category":"page"},{"location":"contributing/overview/#Reporting-bugs","page":"Overview","title":"Reporting bugs","text":"","category":"section"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"We track bugs using GitHub issues. We encourage you to write complete, specific, reproducible bug reports. Mention the versions of Julia and RxInfer for which you observe unexpected behavior. Please provide a concise description of the problem and complement it with code snippets, test cases, screenshots, tracebacks or any other information that you consider relevant. This will help us to replicate the problem and narrow the search space for solutions.","category":"page"},{"location":"contributing/overview/#Nightly-Julia-status","page":"Overview","title":"Nightly Julia status","text":"","category":"section"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"The badge that indicates if RxInfer can be installed on a Julia nightly version. The failing badge may indicate either a problem with RxInfer itself of with one if the dependencies.  Click on the badge to get the latest evaluation report.","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"(Image: PkgEval)","category":"page"},{"location":"contributing/overview/#Suggesting-features","page":"Overview","title":"Suggesting features","text":"","category":"section"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"We welcome new feature proposals. However, before submitting a feature request, consider a few things:","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"Does the feature require changes in the core RxInfer code? If it doesn't (for example, you would like to add a factor node for a particular application), you can add local extensions in your script/notebook or consider making a separate repository for your extensions.\nIf you would like to add an implementation of a feature that changes a lot in the core RxInfer code, please open an issue on GitHub and describe your proposal first. This will allow us to discuss your proposal with you before you invest your time in implementing something that may be difficult to merge later on.","category":"page"},{"location":"contributing/overview/#Contributing-code","page":"Overview","title":"Contributing code","text":"","category":"section"},{"location":"contributing/overview/#Installing-RxInfer","page":"Overview","title":"Installing RxInfer","text":"","category":"section"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"We suggest that you use the dev command from the new Julia package manager to install RxInfer for development purposes. To work on your fork of RxInfer, use your fork's URL address in the dev command, for example:","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"] dev git@github.com:your_username/RxInfer.jl.git","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"The dev command clones RxInfer to ~/.julia/dev/RxInfer. All local changes to RxInfer code will be reflected in imported code.","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"note: Note\nIt is also might be useful to install Revise.jl package as it allows you to modify code and use the changes without restarting Julia.","category":"page"},{"location":"contributing/overview/#Core-dependencies","page":"Overview","title":"Core dependencies","text":"","category":"section"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"RxInfer.jl heavily depends on the ReactiveMP.jl, GraphPPL.jl and Rocket.jl packages. RxInfer.jl must be updated every time any of these packages has a major update and/or API changes. Developers are adviced to use the dev command for all of these packages while making changes to the RxInfer.jl. It is worth noting though that standard Julia testing utilities ignore the local development environment and always try to test the package with the latest released versions of the core dependencies. Read the section about the Makefile below to see how to test RxInfer.jl with the locally installed core dependencies.","category":"page"},{"location":"contributing/overview/#Committing-code","page":"Overview","title":"Committing code","text":"","category":"section"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"We use the standard GitHub Flow workflow where all contributions are added through pull requests. In order to contribute, first fork the repository, then commit your contributions to your fork, and then create a pull request on the main branch of the RxInfer repository.","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"Before opening a pull request, please make sure that all tests pass without failing! All examples (can be found in /examples/ directory) have to run without errors as well. ","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"note: Note\nUse make test, make examples and make docs commands to ensure that all tests, examples and the documentation build run without any issues. See below for the Makefile commands description in more details.","category":"page"},{"location":"contributing/overview/#Style-conventions","page":"Overview","title":"Style conventions","text":"","category":"section"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"We use default Julia style guide. We list here a few important points and our modifications to the Julia style guide:","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"Use 4 spaces for indentation\nType names use UpperCamelCase. For example: AbstractFactorNode, RandomVariable, etc..\nFunction names are lowercase with underscores, when necessary. For example: activate!, randomvar, as_variable, etc..\nVariable names and function arguments use snake_case\nThe name of a method that modifies its argument(s) must end in !","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"note: Note\nRxInfer repository contains scripts to automatically format code according to our guidelines. Use make format command to fix code style. This command overwrites files. Use make lint to run a linting procedure without overwriting the actual source files.","category":"page"},{"location":"contributing/overview/#Unit-tests","page":"Overview","title":"Unit tests","text":"","category":"section"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"We use the test-driven development (TDD) methodology for RxInfer development. The test coverage should be as complete as possible. Please make sure that you write tests for each piece of code that you want to add.","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"All unit tests are located in the /test/ directory. The /test/ directory follows the structure of the /src/ directory. Each test file should have the following filename format: test_*.jl. Some tests are also present in jldoctest docs annotations directly in the source code. See Julia's documentation about doctests.","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"The tests can be evaluated by running following command in the Julia REPL:","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"] test RxInfer","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"In addition tests can be evaluated by running following command in the RxInfer root directory:","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"make test","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"note: Note\nUse make devtest to use local dev-ed versions of the core packages.","category":"page"},{"location":"contributing/overview/#Makefile","page":"Overview","title":"Makefile","text":"","category":"section"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"RxInfer.jl uses Makefile for most common operations:","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"make help: Shows help snippet\nmake test: Run tests, supports extra arguments\nmake test test_args=\"distributions:normal_mean_variance\" would run tests only from distributions/test_normal_mean_variance.jl\nmake test test_args=\"distributions:normal_mean_variance models:lgssm\" would run tests both from distributions/test_normal_mean_variance.jl and models/test_lgssm.jl\nmake test dev=true would run tests while using dev-ed versions of core packages\nmake devtest: Alias for the make test dev=true ...\nmake docs: Compile documentation\nmake devdocs: Same as make docs, but uses dev-ed versions of core packages\nmake examples: Run all examples and put them in the docs/ folder if successfull \nmake devexamples: Same as make examples, but uses dev-ed versions of core packages\nmake lint: Check codestyle\nmake format: Check and fix codestyle ","category":"page"},{"location":"contributing/overview/","page":"Overview","title":"Overview","text":"note: Note\nCore packages include ReactiveMP.jl, GraphPPL.jl and Rocket.jl. When using any of the dev commands from the Makefile those packages must be present in the Pkg.devdir() directory.","category":"page"},{"location":"contributing/new-release/#contributing-new-release","page":"Publishing a new release","title":"Publishing a new release","text":"","category":"section"},{"location":"contributing/new-release/","page":"Publishing a new release","title":"Publishing a new release","text":"Please read first the general Contributing section. Also, please read the FAQ section in the Julia General registry.","category":"page"},{"location":"contributing/new-release/#Start-the-release-process","page":"Publishing a new release","title":"Start the release process","text":"","category":"section"},{"location":"contributing/new-release/","page":"Publishing a new release","title":"Publishing a new release","text":"In order to start the release process a person with the associated permissions should: ","category":"page"},{"location":"contributing/new-release/","page":"Publishing a new release","title":"Publishing a new release","text":"Open a commit page on GitHub\nWrite the @JuliaRegistrator register comment for the commit:","category":"page"},{"location":"contributing/new-release/","page":"Publishing a new release","title":"Publishing a new release","text":"(Image: Release comment)","category":"page"},{"location":"contributing/new-release/","page":"Publishing a new release","title":"Publishing a new release","text":"The Julia Registrator bot should automatically register a request for the new release. Once all checks have passed on the Julia Registrator's side, the new release will be published and tagged automatically.","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/#examples-solve-gp-regression-by-sde","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"","category":"section"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"In this notebook, we solve a GP regression problem by using \"Stochastic Differential Equation\" (SDE). This method is well described in the dissertation \"Stochastic differential equation methods for spatio-temporal Gaussian process regression.\" by Arno Solin and \"Sequential Inference for Latent Temporal Gaussian Process Models\" by Jouni Hartikainen. The idea of the method is as follows.","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"Suppose a function f(x) follows a zero-mean Gaussian Process beginaligned f(x) sim mathcalGP(0 k(xx)) endaligned","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"When the dimensionality of x is 1, we can consider f(x) as a stochastic process over time, i.e. f(t). For a certain classses of covariance functions, f(t) is a solution to an m-th order linear stochastic differential equation (SDE) beginaligned a_0 f(t) + a_1 fracd f(t)dt + dots + a_m fracd^m f(t)dt^m = w(t)  endaligned","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"where w(t) is a zero-mean white noise process with spectral density Q_c. If we define a vector-valued function mathbff(t) = (f(t) ddt f(t)dots d^m-1dt^m-1f(t)), then we can rewrite the above SDE under the companion form","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"beginaligned\nfracd mathbff(t)dt = mathbfF mathbff(t) + mathbfL w(t) quad (1)\nendaligned","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"where mathbfF and mathbfL are defined based on the choice of covariance functions.  From (1), we have the following state-space model: beginaligned mathbff_k = mathbfA_k-1  mathbff_k-1 + mathbfq_k-1 quad mathbfq_k-1 sim mathcalN(mathbf0 mathbfQ_k-1) quad(2a) \ny_k = mathbfH  mathbff(t_k) + epsilon_k  quad epsilon_k sim mathcalN(0 sigma^2_noise) quad(2b) \nendaligned","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"where mathbfA_k = exp(mathbfFDelta t_k), with Delta t_k = t_k+1 - t_k, is called the discrete-time state transition matrix, and mathbfQ_k the process noise covariance matrix. For the computation of mathbfQ_k, we will come back later. According to Arno Solin and Jouni Hartikainen's dissertation, the GP regression problem amounts to the inference problem of the above state-space model, and this can be solved by RTS-smoothing. The state-space model starts from  the initial state f_0 sim mathcalN(mathbf0 mathbfP_0). For stationary covariance function, the SDE has a stationary state f_infty sim mathcalN(mathbf0 mathbfP_infty), where mathbfP_infty is the solution to beginaligned fracdmathbfP_inftydt = mathbfF mathbfP_infty + mathbfP_infty mathbfF^T + mathbfL mathbfQ_c mathbfL^T = 0 quad (mathrmLyapunov  equation) endaligned","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"With this stationary condition, the process noise covariance mathbfQ_k is computed as follows beginaligned mathbfQ_k = mathbfP_infty - mathbfA_k mathbfP_infty mathbfA_k^T  endaligned","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"For one-dimensional problem the SDE representation of the GP is defined by the matrices mathbfF  mathbfL  mathbfQ_c  mathbfP_0 and mathbfH. Once we obtain all the matrices, we can do GP regression by implementing RTS-smoothing on the state-space model (2). In this notebook we will particularly use the Matern class of covariance functions for Gaussian Process.","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"using RxInfer, Random, Distributions, LinearAlgebra, Plots","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/#Create-state-space-model-for-GP-regression","page":"Solve GP regression by SDE","title":"Create state space model for GP regression","text":"","category":"section"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"Here we create a state-space model beginaligned mathbff_k = mathbfA_k-1  mathbff_k-1 + mathbfq_k-1 quad mathbfq_k-1 sim mathcalN(mathbf0 mathbfQ_k-1) \ny_k = mathbfH  mathbff(t_k) + epsilon_k  quad epsilon_k sim mathcalN(0 sigma^2_noise) \nendaligned where y_k is the noisy observation of the function f at time t_k, and sigma^2_noise is the noise variance and assumed to be known.","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"@model function gp_regression(n, P∞, A, Q, H, σ²_noise)\n    f_0 ~ MvNormalMeanCovariance(zeros(length(H)), P∞)\n    f = randomvar(n)\n    y = datavar(Float64, n) where { allow_missing = true }\n    \n    f_prev = f_0\n\n    for i=1:n\n        f[i] ~ MvNormalMeanCovariance(A[i] * f_prev, Q[i])\n        y[i] ~ NormalMeanVariance(dot(H , f[i]), σ²_noise)\n        f_prev = f[i]\n    end\n    return f, y\nend","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/#Generate-data","page":"Solve GP regression by SDE","title":"Generate data","text":"","category":"section"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"Random.seed!(10)\nn = 100\nσ²_noise = 0.04;\nt = collect(range(-2, 2, length=n)); #timeline\nf_true = sinc.(t); # true process\nf_noisy = f_true + sqrt(σ²_noise)*randn(n); #noisy process\n\npos = sort(randperm(75)[1:2:75]); \nt_obser = t[pos]; # time where we observe data\n\ny_data = Array{Union{Float64,Missing}}(missing, n)\nfor i in pos \n    y_data[i] = f_noisy[i]\nend\n\nθ = [1., 1.]; # store [l, σ²]\nΔt = [t[1]]; # time difference\nappend!(Δt, t[2:end] - t[1:end-1]);","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/#Let's-visualize-our-data","page":"Solve GP regression by SDE","title":"Let's visualize our data","text":"","category":"section"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"plot(t, f_true, label=\"True process f(t)\")\nscatter!(t_obser, y_data[pos], label = \"Noisy observations\")\nxlabel!(\"t\")\nylabel!(\"f(t)\")","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/#Covariance-function:-Matern-3/2","page":"Solve GP regression by SDE","title":"Covariance function: Matern-3/2","text":"","category":"section"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"The Matern is a stationary covariance function and defined as follows beginaligned k(tau) = sigma^2 frac2^1-nuGamma(nu) left(fracsqrt2nutaul right)^nu K_nuleft(fracsqrt2nutaul right) endaligned where  beginaligned sigma^2 textthe magnitude scale hyperparameter\nl textthe characteristic length-scale\nnu textthe smoothness hyperparameter\nK_nu() textthe modified Bessel function of the second kind endaligned When we say the Matern-3/2, we mean nu=32. The matrices for the state space model are computed as follows beginaligned mathbfF = beginpmatrix 0  1\n-lambda^2  -2lambda endpmatrix quad quad mathbfL = beginpmatrix 0  1 endpmatrix quad quad mathbfP_infty = beginpmatrix sigma^2  0  0  lambda^2sigma^2 endpmatrix quad quad mathbfH = beginpmatrix 1  0 endpmatrix quad quad Q_c = 4lambda^3sigma^2 endaligned  where lambda = fracsqrt3l  From these matrices we can define mathbfA_k and mathbfQ_k.","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"λ = sqrt(3)/θ[1];\n#### compute matrices for the state-space model ######\nL = [0., 1.];\nH = [1., 0.];\nF = [0. 1.; -λ^2 -2λ]\nP∞ = [θ[2] 0.; 0. (λ^2*θ[2]) ]\nA = [exp(F * i) for i in Δt]; \nQ = [P∞ - i*P∞*i' for i in A];","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"result_32 = infer(\n    model = gp_regression(n, P∞, A, Q, H, σ²_noise),\n    data = (y = y_data,)\n)","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"Inference results:\n  Posteriors       | available for (f, f_0)\n  Predictions      | available for (y)","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/#Covariance-function:-Matern-5/2","page":"Solve GP regression by SDE","title":"Covariance function: Matern-5/2","text":"","category":"section"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"Now let's try the Matern-5/2 kernel. The matrices for the SDE representation of the Matern-5/2 are:","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"beginaligned\nmathbfF = beginpmatrix\n0  1  0\n0  0  1 \n-lambda^3  -3lambda^2  -3lambda\nendpmatrix quad quad mathbfL = beginpmatrix\n0  0  1\nendpmatrix quad quad mathbfH = beginpmatrix\n1  0  0\nendpmatrix quad quad Q_c = frac163 sigma^2 lambda^5 \nendaligned","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"where lambda = sqrt5  l. To find mathbfP_infty, we solve the Lyapunov equation","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"beginaligned\nfracdmathbfP_inftydt = mathbfF mathbfP_infty + mathbfP_infty mathbfF^T + mathbfL mathbfQ_c mathbfL^T = 0\nendaligned","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"of which the solution is","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"beginaligned\nvec(mathbfP_infty) = (mathbfI otimes mathbfF + mathbfFotimesmathbfI)^-1 vec(-mathbfLQ_cmathbfL^T)\nendaligned","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"where vec() is the vectorization operator and otimes denotes the Kronecker product. Now we can find mathbfA_k and mathbfQ_k ","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"beginaligned\nmathbfA_k = exp(mathbfFDelta t_k) \nendaligned","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"beginaligned\nmathbfQ_k = mathbfP_infty - mathbfA_k mathbfP_infty mathbfA_k^T  \nendaligned","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"λ = sqrt(5)/θ[1];\n#### compute matrices for the state-space model ######\nL = [0., 0., 1.];\nH = [1., 0., 0.];\nF = [0. 1. 0.; 0. 0. 1.;-λ^3 -3λ^2 -3λ]\nQc = 16/3 * θ[2] * λ^5;\n\nI = diageye(3) ; \nvec_P = inv(kron(I,F) + kron(F,I)) * vec(-L * Qc * L'); \nP∞ = reshape(vec_P,3,3);\nA = [exp(F * i) for i in Δt]; \nQ = [P∞ - i*P∞*i' for i in A];","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"result_52 = infer(\n    model = gp_regression(n, P∞, A, Q, H, σ²_noise),\n    data = (y = y_data,)\n)","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"Inference results:\n  Posteriors       | available for (f, f_0)\n  Predictions      | available for (y)","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/#Result","page":"Solve GP regression by SDE","title":"Result","text":"","category":"section"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"slicedim(dim) = (a) -> map(e -> e[dim], a)\n\nplot(t, mean.(result_32.posteriors[:f]) |> slicedim(1), ribbon = var.(result_32.posteriors[:f]) |> slicedim(1) .|> sqrt, label =\"Approx. process_M32\", title = \"Matern-3/2\", legend =false, lw = 2)\nplot!(t, mean.(result_52.posteriors[:f]) |> slicedim(1), ribbon = var.(result_52.posteriors[:f]) |> slicedim(1) .|> sqrt, label =\"Approx. process_M52\",legend = :bottomleft, title = \"GPRegression by SSM\", lw = 2)\nplot!(t, f_true,label=\"true process\", lw = 2)\nscatter!(t_obser, f_noisy[pos], label=\"Observations\")\nxlabel!(\"t\")\nylabel!(\"f(t)\")","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/GP Regression by SSM/","page":"Solve GP regression by SDE","title":"Solve GP regression by SDE","text":"As we can see from the plot, both cases of Matern kernel provide good approximations (small variance) to the true process at the area with dense observations (namely from t = 0 to around 3.5), and when we move far away from this region the approximated processes become less accurate (larger variance). This result makes sense because GP regression exploits the correlation between observations to predict unobserved points, and the choice of covariance functions as well as their hyper-parameters might not be optimal. We can increase the accuracy of the approximated processes by simply adding more observations. This way of improvement does not trouble the state-space method much but it might cause computational problem for naive GP regression, because with N observations the complexity of naive GP regression scales with N^3 while the state-space method scales linearly with N.     ","category":"page"},{"location":"contributing/new-documentation/#guide-docs-contributing","page":"Contributing to the documentation","title":"Contributing to the documentation","text":"","category":"section"},{"location":"contributing/new-documentation/","page":"Contributing to the documentation","title":"Contributing to the documentation","text":"Contributing to our documentation is a valuable way to enhance the RxInfer ecosystem. To get started, you can follow these steps:","category":"page"},{"location":"contributing/new-documentation/","page":"Contributing to the documentation","title":"Contributing to the documentation","text":"Familiarize Yourself: First, take some time to explore our existing documentation. Understand the structure, style, and content to align your contributions with our standards.\nIdentify Needs: Identify areas that require improvement, clarification, or expansion. These could be missing explanations, code examples, or outdated information.\nFork the Repository: Fork our documentation repository on GitHub to create your own copy. This allows you to work on your changes independently.\nMake Your Edits: Create or modify content in your forked repository. Ensure your contributions are clear, concise, and well-structured.\nSubmit a Pull Request: When you're satisfied with your changes, submit a pull request (PR) to our main repository. Describe your changes in detail in the PR description.\nReview and Feedback: Our documentation maintainers will review your PR. They may provide feedback or request adjustments. Be responsive to this feedback to facilitate the merging process.\nMerging: Once your changes align with our documentation standards, they will be merged into the main documentation. Congratulations, you've successfully contributed to the RxInfer ecosystem!","category":"page"},{"location":"contributing/new-documentation/","page":"Contributing to the documentation","title":"Contributing to the documentation","text":"By following these steps, you can play an essential role in improving and expanding our documentation, making it more accessible and valuable to the RxInfer community.","category":"page"},{"location":"contributing/new-documentation/#Use-[LiveServer.jl](https://github.com/tlienart/LiveServer.jl)","page":"Contributing to the documentation","title":"Use LiveServer.jl","text":"","category":"section"},{"location":"contributing/new-documentation/","page":"Contributing to the documentation","title":"Contributing to the documentation","text":"LiveServer.jl is a simple and lightweight web server developed in Julia. It features live-reload capabilities, making it a valuable tool for automatically refreshing the documentation of a package while you work on its content.","category":"page"},{"location":"contributing/new-documentation/","page":"Contributing to the documentation","title":"Contributing to the documentation","text":"To use LiveServer.jl, simply follow these steps[1]","category":"page"},{"location":"contributing/new-documentation/","page":"Contributing to the documentation","title":"Contributing to the documentation","text":"[1]: Make sure to install the LiveServer and Documenter in your current working environment.","category":"page"},{"location":"contributing/new-documentation/","page":"Contributing to the documentation","title":"Contributing to the documentation","text":"Make sure to import the required packages ","category":"page"},{"location":"contributing/new-documentation/","page":"Contributing to the documentation","title":"Contributing to the documentation","text":"julia> using LiveServer, Documenter","category":"page"},{"location":"contributing/new-documentation/","page":"Contributing to the documentation","title":"Contributing to the documentation","text":"After importing the required packages, you can start the live server with the following command:","category":"page"},{"location":"contributing/new-documentation/","page":"Contributing to the documentation","title":"Contributing to the documentation","text":"julia> servedocs()","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#examples-bayesian-linear-regression-tutorial","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"This notebook is an extensive tutorial on Bayesian linear regression with RxInfer and consists of two major parts:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"The first part uses a regular Bayesian Linear Regression on a simple application of fuel consumption for a car with synthetic data.\nThe second part is an adaptation of a tutorial from NumPyro and uses Hierarchical Bayesian linear regression on the OSIC pulmonary fibrosis progression dataset from Kaggle.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"using RxInfer, Random, Plots, StableRNGs, LinearAlgebra, StatsPlots, LaTeXStrings, DataFrames, CSV, GLM","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Part-1.-Bayesian-Linear-Regression","page":"Bayesian Linear Regression Tutorial","title":"Part 1. Bayesian Linear Regression","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"John recently purchased a new car and is interested in its fuel consumption rate. He believes that this rate has a linear relationship with speed, and as such, he wants to conduct tests by driving his car on different types of roads, recording both the fuel usage and speed. In order to determine the fuel consumption rate, John employs Bayesian linear regression.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Univariate-regression-with-known-noise","page":"Bayesian Linear Regression Tutorial","title":"Univariate regression with known noise","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"First, he drives the car on a urban road. John enjoys driving on the well-built, wide, and flat urban roads. Urban roads also offer the advantage of precise fuel consumption measurement with minimal noise. Therefore John models the fuel consumption y_ninmathbbR as a normal distribution and treats x_n as a fixed hyper-parameter:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"beginaligned\np(y_n mid a b) = mathcalN(y_n mid a x_n + b  1)\nendaligned","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"The recorded speed is denoted as x_n in mathbbR and the recorded fuel consumption as y_n in mathbbR. Prior beliefs on a and b are informed by the vehicle manual.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"beginaligned\n    p(a) = mathcalN(a mid m_a v_a) \n    p(b) = mathcalN(b mid m_b v_b) \nendaligned","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Together they form the probabilistic model p(y a b) = p(a)p(b) prod_N=1^N p(y_n mid a b) where the goal is to infer the posterior distributions p(a mid y) and p(bmid y).","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"He records the speed and fuel consumption for the urban road which is the xdata and ydata.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"function generate_data(a, b, v, nr_samples; rng=StableRNG(1234))\n    x = float.(collect(1:nr_samples))\n    y = a .* x .+ b .+ randn(rng, nr_samples) .* sqrt(v)\n    return x, y\nend;","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"x_data, y_data = generate_data(0.5, 25.0, 1.0, 250)\n\nscatter(x_data, y_data, title = \"Dataset (City road)\", legend=false)\nxlabel!(\"Speed\")\nylabel!(\"Fuel consumption\")","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"In order to estimate the two parameters with the recorded data, he uses a RxInfer.jl to create the above described model.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"@model function linear_regression(nr_samples)\n    a ~ Normal(mean = 0.0, variance = 1.0)\n    b ~ Normal(mean = 0.0, variance = 100.0)\n    \n    x = datavar(Float64, nr_samples)\n    y = datavar(Float64, nr_samples)\n    \n    y .~ Normal(mean = a .* x .+ b, variance = 1.0)\nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"He is delighted that he can utilize the inference function from this package, saving him the effort of starting from scratch and enabling him to obtain the desired results for this road. He does note that there is a loop in his model, namely all a and b variables are connected over all observations, therefore he needs to initialize one of the messages and run multiple iterations for the loopy belief propagation algorithm. It is worth noting that loopy belief propagation is not guaranteed to converge in general and might be highly influenced by the choice of the initial messages in the initmessages argument. He is going to evaluate the convergency performance of the algorithm with the free_energy = true option:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"results = infer(\n    model        = linear_regression(length(x_data)), \n    data         = (y = y_data, x = x_data), \n    initmessages = (b = NormalMeanVariance(0.0, 100.0), ), \n    returnvars   = (a = KeepLast(), b = KeepLast()),\n    iterations   = 20,\n    free_energy  = true\n)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Inference results:\n  Posteriors       | available for (a, b)\n  Free Energy:     | Real[450.062, 8526.84, 4960.42, 2949.02, 1819.14, 1184\n.44, 827.897, 627.595, 515.064, 451.839, 416.313, 396.349, 385.129, 378.821\n, 375.274, 373.279, 372.156, 371.524, 371.167, 370.966]","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"He knows the theoretical coefficients and noise for this car from the manual. He is going to compare the experimental solution with theoretical results.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"pra = plot(range(-3, 3, length = 1000), (x) -> pdf(NormalMeanVariance(0.0, 1.0), x), title=L\"Prior for $a$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$p(a)$\", c=1,)\npra = vline!(pra, [ 0.5 ], label=L\"True $a$\", c = 3)\npsa = plot(range(0.45, 0.55, length = 1000), (x) -> pdf(results.posteriors[:a], x), title=L\"Posterior for $a$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$p(a\\mid y)$\", c=2,)\npsa = vline!(psa, [ 0.5 ], label=L\"True $a$\", c = 3)\n\nplot(pra, psa, size = (1000, 200), xlabel=L\"$a$\", ylabel=L\"$p(a)$\", ylims=[0,Inf])","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"prb = plot(range(-40, 40, length = 1000), (x) -> pdf(NormalMeanVariance(0.0, 100.0), x), title=L\"Prior for $b$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"p(b)\", c=1, legend = :topleft)\nprb = vline!(prb, [ 25 ], label=L\"True $b$\", c = 3)\npsb = plot(range(23, 28, length = 1000), (x) -> pdf(results.posteriors[:b], x), title=L\"Posterior for $b$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"p(b\\mid y)\", c=2, legend = :topleft)\npsb = vline!(psb, [ 25 ], label=L\"True $b$\", c = 3)\n\nplot(prb, psb, size = (1000, 200), xlabel=L\"$b$\", ylabel=L\"$p(b)$\", ylims=[0, Inf])","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"a = results.posteriors[:a]\nb = results.posteriors[:b]\n\nprintln(\"Real a: \", 0.5, \" | Estimated a: \", mean_var(a), \" | Error: \", abs(mean(a) - 0.5))\nprintln(\"Real b: \", 25.0, \" | Estimated b: \", mean_var(b), \" | Error: \", abs(mean(b) - 25.0))","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Real a: 0.5 | Estimated a: (0.501490188462706, 1.9162284531300301e-7) | Err\nor: 0.001490188462705988\nReal b: 25.0 | Estimated b: (24.81264210195605, 0.0040159675312827) | Error\n: 0.18735789804394898","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Based on the Bethe free energy below, John knows that the loopy belief propagation has actually converged after 20 iterations:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"# drop first iteration, which is influenced by the `initmessages`\nplot(2:20, results.free_energy[2:end], title=\"Free energy\", xlabel=\"Iteration\", ylabel=\"Free energy [nats]\", legend=false)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Univariate-regression-with-unknown-noise","page":"Bayesian Linear Regression Tutorial","title":"Univariate regression with unknown noise","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Afterwards, he plans to test the car on a mountain road. However, mountain roads are typically narrow and filled with small stones, which makes it more difficult to establish a clear relationship between fuel consumption and speed, leading to an unknown level of noise in the regression model. Therefore, he design a model with unknown Inverse-Gamma distribution on the variance. beginaligned p(y_n mid a b s) = mathcalN(y_n mid ax_n + b s)\np(s) = mathcalIG(smidalpha theta)\np(a) = mathcalN(a mid m_a v_a) \np(b) = mathcalN(b mid m_b v_b)  endaligned","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"@model function linear_regression_unknown_noise(nr_samples)\n    a ~ Normal(mean = 0.0, variance = 1.0)\n    b ~ Normal(mean = 0.0, variance = 100.0)\n    s ~ InverseGamma(1.0, 1.0)\n    \n    x = datavar(Float64, nr_samples)\n    y = datavar(Float64, nr_samples)\n    \n    y .~ Normal(mean = a .* x .+ b, variance = s)\nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"x_data_un, y_data_un = generate_data(0.5, 25.0, 400.0, 250)\n\nscatter(x_data_un, y_data_un, title = \"Dateset with unknown noise (mountain road)\", legend=false)\nxlabel!(\"Speed\")\nylabel!(\"Fuel consumption\")","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"To solve this problem in closed-from we need to resort to a variational approximation. The procedure will be a combination of variational inference and loopy belief propagation. He chooses constraints = MeanField() as a global variational approximation and provides initial marginals with the initmarginals argument. He is, again, going to evaluate the convergency performance of the algorithm with the free_energy = true option:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"results_unknown_noise = infer(\n    model           = linear_regression_unknown_noise(length(x_data_un)), \n    data            = (y = y_data_un, x = x_data_un), \n    initmessages    = (b = NormalMeanVariance(0.0, 100.0), ), \n    returnvars      = (a = KeepLast(), b = KeepLast(), s = KeepLast()), \n    iterations      = 20,\n    constraints     = MeanField(),\n    initmarginals   = (s = vague(InverseGamma), ),\n    free_energy     = true\n)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Inference results:\n  Posteriors       | available for (a, b, s)\n  Free Energy:     | Real[1657.49, 1192.08, 1142.31, 1135.43, 1129.19, 1125\n.47, 1123.34, 1122.13, 1121.44, 1121.05, 1120.82, 1120.69, 1120.61, 1120.56\n, 1120.53, 1120.52, 1120.5, 1120.5, 1120.49, 1120.49]","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Based on the Bethe free energy below, John knows that his algorithm has converged after 20 iterations:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"plot(results_unknown_noise.free_energy, title=\"Free energy\", xlabel=\"Iteration\", ylabel=\"Free energy [nats]\", legend=false)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Below he visualizes the obtained posterior distributions for parameters:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"pra = plot(range(-3, 3, length = 1000), (x) -> pdf(NormalMeanVariance(0.0, 1.0), x), title=L\"Prior for $a$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$p(a)$\", c=1,)\npra = vline!(pra, [ 0.5 ], label=L\"True $a$\", c = 3)\npsa = plot(range(0.45, 0.55, length = 1000), (x) -> pdf(results_unknown_noise.posteriors[:a], x), title=L\"Posterior for $a$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$q(a)$\", c=2,)\npsa = vline!(psa, [ 0.5 ], label=L\"True $a$\", c = 3)\n\nplot(pra, psa, size = (1000, 200), xlabel=L\"$a$\", ylabel=L\"$p(a)$\", ylims=[0, Inf])","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"prb = plot(range(-40, 40, length = 1000), (x) -> pdf(NormalMeanVariance(0.0, 100.0), x), title=L\"Prior for $b$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$p(b)$\", c=1, legend = :topleft)\nprb = vline!(prb, [ 25.0 ], label=L\"True $b$\", c = 3)\npsb = plot(range(23, 28, length = 1000), (x) -> pdf(results_unknown_noise.posteriors[:b], x), title=L\"Posterior for $b$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$q(b)$\", c=2, legend = :topleft)\npsb = vline!(psb, [ 25.0 ], label=L\"True $b$\", c = 3)\n\nplot(prb, psb, size = (1000, 200), xlabel=L\"$b$\", ylabel=L\"$p(b)$\", ylims=[0, Inf])","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"prb = plot(range(0.001, 400, length = 1000), (x) -> pdf(InverseGamma(1.0, 1.0), x), title=L\"Prior for $s$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$p(s)$\", c=1, legend = :topleft)\nprb = vline!(prb, [ 200 ], label=L\"True $s$\", c = 3)\npsb = plot(range(0.001, 400, length = 1000), (x) -> pdf(results_unknown_noise.posteriors[:s], x), title=L\"Posterior for $s$ parameter\", fillalpha=0.3, fillrange = 0, label=L\"$q(s)$\", c=2, legend = :topleft)\npsb = vline!(psb, [ 200 ], label=L\"True $s$\", c = 3)\n\nplot(prb, psb, size = (1000, 200), xlabel=L\"$s$\", ylabel=L\"$p(s)$\", ylims=[0, Inf])","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"He sees that in the presence of more noise the inference result is more uncertain about the actual values for a and b parameters.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"John samples a and b and plot many possible regression lines on the same plot:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"as = rand(results_unknown_noise.posteriors[:a], 100)\nbs = rand(results_unknown_noise.posteriors[:b], 100)\np = scatter(x_data_un, y_data_un, title = \"Linear regression with more noise\", legend=false)\nxlabel!(\"Speed\")\nylabel!(\"Fuel consumption\")\nfor (a, b) in zip(as, bs)\n    global p = plot!(p, x_data_un, a .* x_data_un .+ b, alpha = 0.05, color = :red)\nend\n\nplot(p, size = (900, 400))","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"From this plot John can see that many lines do fit the data well and there is no definite \"best\" answer to the regression coefficients. He realize that most of these lines, however, resemble a similar angle and shift.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Multivariate-linear-regression","page":"Bayesian Linear Regression Tutorial","title":"Multivariate linear regression","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"In addition to fuel consumption, he is also interested in evaluating the car's power performance, braking performance, handling stability, smoothness, and other factors. To investigate the car's performance, he includes additional measurements. Essentially, this approach involves performing multiple linear regression tasks simultaneously, using multiple data vectors for x and y with different levels of noise. As in the previous example, he assumes the level of noise to be unknown.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"@model function linear_regression_multivariate(dim, nr_samples)\n    a ~ MvNormal(mean = zeros(dim), covariance = 100 * diageye(dim))\n    b ~ MvNormal(mean = ones(dim), covariance = 100 * diageye(dim))\n    W ~ InverseWishart(dim + 2, 100 * diageye(dim))\n\n    # Here is a small trick to make the example work\n    # We treat the `x` vector as a Diagonal matrix such that we can easily multiply it with `a`\n    x = datavar(Diagonal{Float64, Vector{Float64}}, nr_samples)\n    y = datavar(Vector{Float64}, nr_samples)\n    z = randomvar(nr_samples)\n\n    z .~ x .* a .+ b\n    y .~ MvNormal(mean = z, covariance = W)\n\nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"After received all the measurement records, he plots the measurements and performance index:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"dim_mv = 6\nnr_samples_mv = 50\nrng_mv = StableRNG(42)\na_mv = randn(rng_mv, dim_mv)\nb_mv = 10 * randn(rng_mv, dim_mv)\nv_mv = 100 * rand(rng_mv, dim_mv)\n\nx_data_mv, y_data_mv = collect(zip(generate_data.(a_mv, b_mv, v_mv, nr_samples_mv)...));","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"p = plot(title = \"Multivariate linear regression\", legend = :topleft)\n\nplt = palette(:tab10)\n\ndata_set_label = [\"\"]\n\nfor k in 1:dim_mv\n    global p = scatter!(p, x_data_mv[k], y_data_mv[k], label = \"Measurement #$k\", ms = 2, color = plt[k])\nend\nxlabel!(L\"$x$\")\nylabel!(L\"$y$\")\np","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Before this data can be used to perform inference, John needs to change its format slightly.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"x_data_mv_processed = map(i -> Diagonal([getindex.(x_data_mv, i)...]), 1:nr_samples_mv)\ny_data_mv_processed = map(i -> [getindex.(y_data_mv, i)...], 1:nr_samples_mv);","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"results_mv = infer(\n    model           = linear_regression_multivariate(dim_mv, nr_samples_mv),\n    data            = (y = y_data_mv_processed, x = x_data_mv_processed),\n    initmarginals   = (W = InverseWishart(dim_mv + 2, 10 * diageye(dim_mv)), ),\n    initmessages    = (b = MvNormalMeanCovariance(ones(dim_mv), 10 * diageye(dim_mv)), ),\n    returnvars      = (a = KeepLast(), b = KeepLast(), W = KeepLast()),\n    free_energy     = true,\n    iterations      = 50,\n    constraints     = MeanField()\n)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Inference results:\n  Posteriors       | available for (a, b, W)\n  Free Energy:     | Real[864.485, 789.026, 769.094, 750.865, 737.67, 724.7\n22, 712.341, 700.865, 690.782, 682.505  …  664.434, 664.434, 664.434, 664.4\n34, 664.434, 664.434, 664.434, 664.434, 664.434, 664.434]","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Again, the algorithm nicely converged, because the Bethe free energy reached a plateau. John also draws the results for the linear regression parameters and sees that the lines very nicely follow the provided data.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"p = plot(title = \"Multivariate linear regression\", legend = :topleft, xlabel=L\"$x$\", ylabel=L\"$y$\")\n\n# how many lines to plot\nr = 50\n\ni_a = collect.(eachcol(rand(results_mv.posteriors[:a], r)))\ni_b = collect.(eachcol(rand(results_mv.posteriors[:b], r)))\n\nplt = palette(:tab10)\n\nfor k in 1:dim_mv\n    x_mv_k = x_data_mv[k]\n    y_mv_k = y_data_mv[k]\n\n    for i in 1:r\n        global p = plot!(p, x_mv_k, x_mv_k .* i_a[i][k] .+ i_b[i][k], label = nothing, alpha = 0.05, color = plt[k])\n    end\n\n    global p = scatter!(p, x_mv_k, y_mv_k, label = \"Measurement #$k\", ms = 2, color = plt[k])\nend\n\n# truncate the init step\nf = plot(results_mv.free_energy[2:end], title =\"Bethe free energy convergence\", label = nothing, xlabel = \"Iteration\", ylabel = \"Bethe free energy [nats]\") \n\nplot(p, f, size = (1000, 400))","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"He needs more iterations to converge in comparison to the very first example, but that is expected since the problem became multivariate and, hence, more difficult.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"i_a_mv = results_mv.posteriors[:a]\n\nps_a = []\n\nfor k in 1:dim_mv\n    \n    local _p = plot(title = L\"Estimated $a_{%$k}$\", xlabel=L\"$a_{%$k}$\", ylabel=L\"$p(a_{%$k})$\", xlims = (-1.5,1.5), xticks=[-1.5, 0, 1.5], ylims=[0, Inf])\n\n    local m_a_mv_k = mean(i_a_mv)[k]\n    local v_a_mv_k = std(i_a_mv)[k, k]\n    \n    _p = plot!(_p, Normal(m_a_mv_k, v_a_mv_k), fillalpha=0.3, fillrange = 0, label=L\"$q(a_{%$k})$\", c=2,)\n    _p = vline!(_p, [ a_mv[k] ], label=L\"True $a_{%$k}$\", c = 3)\n           \n    push!(ps_a, _p)\nend\n\nplot(ps_a...)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"i_b_mv = results_mv.posteriors[:b]\n\nps_b = []\n\nfor k in 1:dim_mv\n    \n    local _p = plot(title = L\"Estimated $b_{%$k}$\", xlabel=L\"$b_{%$k}$\", ylabel=L\"$p(b_{%$k})$\", xlims = (-20,20), xticks=[-20, 0, 20], ylims =[0, Inf])\n    local m_b_mv_k = mean(i_b_mv)[k]\n    local v_b_mv_k = std(i_b_mv)[k, k]\n\n    _p = plot!(_p, Normal(m_b_mv_k, v_b_mv_k), fillalpha=0.3, fillrange = 0, label=L\"$q(b_{%$k})$\", c=2,)\n    _p = vline!(_p, [ b_mv[k] ], label=L\"Real $b_{%$k}$\", c = 3)\n           \n    push!(ps_b, _p)\nend\n\nplot(ps_b...)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"He also checks the noise estimation procedure and sees that the noise variance are currently a bit underestimated. Note here that he neglects the covariance terms between the individual elements, which might result in this kind of behaviour.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"scatter(1:dim_mv, v_mv, ylims=(0, 100), label=L\"True $s_d$\")\nscatter!(1:dim_mv, diag(mean(results_mv.posteriors[:W])); yerror=sqrt.(diag(var(results_mv.posteriors[:W]))), label=L\"$\\mathrm{E}[s_d] \\pm \\sigma$\")\nplot!(; xlabel=L\"Dimension $d$\", ylabel=\"Variance\", title=\"Estimated variance of the noise\")","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Part-2.-Hierarchical-Bayesian-Linear-Regression","page":"Bayesian Linear Regression Tutorial","title":"Part 2. Hierarchical Bayesian Linear Regression","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Disclaimer The tutorial below is an adaptation of the Bayesian Hierarchical Linear Regression tutorial implemented in NumPyro. ","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"The original author in NumPyro is Carlos Souza. Updated by Chris Stoafer in NumPyro. Adapted to RxInfer by Dmitry Bagaev.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Probabilistic Machine Learning models can not only make predictions about future data but also model uncertainty. In areas such as personalized medicine, there might be a large amount of data, but there is still a relatively small amount available for each patient. To customize predictions for each person, it becomes necessary to build a model for each individual — considering its inherent uncertainties — and then couple these models together in a hierarchy so that information can be borrowed from other similar individuals [1].","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"The purpose of this tutorial is to demonstrate how to implement a Bayesian Hierarchical Linear Regression model using RxInfer. To provide motivation for the tutorial, I will use the OSIC Pulmonary Fibrosis Progression competition, hosted on Kaggle.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"# https://www.machinelearningplus.com/linear-regression-in-julia/\n# https://nbviewer.org/github/pyro-ppl/numpyro/blob/master/notebooks/source/bayesian_hierarchical_linear_regression.ipynb","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Understanding-the-Task","page":"Bayesian Linear Regression Tutorial","title":"Understanding the Task","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Pulmonary fibrosis is a disorder characterized by scarring of the lungs, and its cause and cure are currently unknown. In this competition, the objective was to predict the severity of decline in lung function for patients. Lung function is assessed based on the output from a spirometer, which measures the forced vital capacity (FVC), representing the volume of air exhaled.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"In medical applications, it is valuable to evaluate a model's confidence in its decisions. As a result, the metric used to rank the teams was designed to reflect both the accuracy and certainty of each prediction. This metric is a modified version of the Laplace Log Likelihood (further details will be provided later).","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Now, let's explore the data and delve deeper into the problem involved.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"dataset = CSV.read(\"../data/hbr/osic_pulmonary_fibrosis.csv\", DataFrame);","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"describe(dataset)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"7×7 DataFrame\n Row │ variable       mean     min                        median   max     \n    ⋯\n     │ Symbol         Union…   Any                        Union…   Any     \n    ⋯\n─────┼─────────────────────────────────────────────────────────────────────\n─────\n   1 │ Patient                 ID00007637202177411956430           ID004266\n372 ⋯\n   2 │ Weeks          31.8618  -5                         28.0     133\n   3 │ FVC            2690.48  827                        2641.0   6399\n   4 │ Percent        77.6727  28.8776                    75.6769  153.145\n   5 │ Age            67.1885  49                         68.0     88      \n    ⋯\n   6 │ Sex                     Female                              Male\n   7 │ SmokingStatus           Currently smokes                    Never sm\noke\n                                                               3 columns om\nitted","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"first(dataset, 5)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"5×7 DataFrame\n Row │ Patient                    Weeks  FVC    Percent  Age    Sex      Sm\noki ⋯\n     │ String31                   Int64  Int64  Float64  Int64  String7  St\nrin ⋯\n─────┼─────────────────────────────────────────────────────────────────────\n─────\n   1 │ ID00007637202177411956430     -4   2315  58.2536     79  Male     Ex\n-sm ⋯\n   2 │ ID00007637202177411956430      5   2214  55.7121     79  Male     Ex\n-sm\n   3 │ ID00007637202177411956430      7   2061  51.8621     79  Male     Ex\n-sm\n   4 │ ID00007637202177411956430      9   2144  53.9507     79  Male     Ex\n-sm\n   5 │ ID00007637202177411956430     11   2069  52.0634     79  Male     Ex\n-sm ⋯\n                                                                1 column om\nitted","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"The dataset provided us with a baseline chest CT scan and relevant clinical information for a group of patients. Each patient has an image taken at Week = 0, and they undergo numerous follow-up visits over approximately 1-2 years, during which their Forced Vital Capacity (FVC) is measured. For the purpose of this tutorial, we will only consider the Patient ID, the weeks, and the FVC measurements, discarding all other information. Restricting our analysis to these specific columns allowed our team to achieve a competitive score, highlighting the effectiveness of Bayesian hierarchical linear regression models, especially when dealing with uncertainty, which is a crucial aspect of the problem.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Since this is real medical data, the relative timing of FVC measurements varies widely, as shown in the 3 sample patients below:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"patientinfo(dataset, patient_id) = filter(:Patient => ==(patient_id), dataset)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"patientinfo (generic function with 1 method)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"function patientchart(dataset, patient_id; line_kws = true)\n    info = patientinfo(dataset, patient_id)\n    x = info[!, \"Weeks\"]\n    y = info[!, \"FVC\"]\n\n    p = plot(tickfontsize = 10, margin = 1Plots.cm, size = (400, 400), titlefontsize = 11)\n    p = scatter!(p, x, y, title = patient_id, legend = false, xlabel = \"Weeks\", ylabel = \"FVC\")\n    \n    if line_kws\n        # Use the `GLM.jl` package to estimate linear regression\n        linearFormulae = @formula(FVC ~ Weeks)\n        linearRegressor = lm(linearFormulae, patientinfo(dataset, patient_id))\n        linearPredicted = predict(linearRegressor)\n        p = plot!(p, x, linearPredicted, color = :red, lw = 3)\n    end\n\n    return p\nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"patientchart (generic function with 1 method)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"p1 = patientchart(dataset, \"ID00007637202177411956430\")\np2 = patientchart(dataset, \"ID00009637202177434476278\")\np3 = patientchart(dataset, \"ID00010637202177584971671\")\n\nplot(p1, p2, p3, layout = @layout([ a b c ]), size = (1200, 400))","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"On average, each of the 176 patients provided in the dataset had 9 visits during which their FVC was measured. These visits occurred at specific weeks within the interval [-12, 133]. The decline in lung capacity is evident, but it also varies significantly from one patient to another.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Our task was to predict the FVC measurements for each patient at every possible week within the [-12, 133] interval, along with providing a confidence score for each prediction. In other words, we were required to fill a matrix, as shown below, with the predicted values and their corresponding confidence scores:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"The task was ideal for applying Bayesian inference. However, the vast majority of solutions shared within the Kaggle community utilized discriminative machine learning models, disregarding the fact that most discriminative methods struggle to provide realistic uncertainty estimates. This limitation stems from their typical training process, which aims to optimize parameters to minimize certain loss criteria (such as predictive error). As a result, these models do not inherently incorporate uncertainty into their parameters or subsequent predictions. While some methods may produce uncertainty estimates as a by-product or through post-processing steps, these are often heuristic-based and lack a statistically principled approach to estimate the target uncertainty distribution [2].","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Modelling:-Bayesian-Hierarchical-Linear-Regression-with-Partial-Pooling","page":"Bayesian Linear Regression Tutorial","title":"Modelling: Bayesian Hierarchical Linear Regression with Partial Pooling","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"In a basic linear regression, which is not hierarchical, the assumption is that all FVC decline curves share the same α and β values. This model is known as the \"pooled model.\" On the other extreme, we could assume a model where each patient has a personalized FVC decline curve, and these curves are entirely independent of one another. This model is referred to as the \"unpooled model,\" where each patient has completely separate regression lines.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"In this analysis, we will adopt a middle ground approach known as \"Partial pooling.\" Specifically, we will assume that while α's and β's are different for each patient, as in the unpooled case, these coefficients share some similarities. This partial pooling will be achieved by modeling each individual coefficient as being drawn from a common group distribution.:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Mathematically, the model is described by the following equations:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"beginequation\n    beginaligned\n        mu_alpha sim mathcalN(mathrmmean = 00 mathrmvariance = 2500000) \n        sigma_alpha sim mathcalGamma(mathrmshape = 175 mathrmscale = 4554) \n        mu_beta sim mathcalN(mathrmmean = 00 mathrmvariance = 90) \n        sigma_beta sim mathcalGamma(mathrmshape = 175 mathrmscale = 136) \n        alpha_i sim mathcalN(mathrmmean = mu_alpha mathrmprecision = sigma_alpha) \n        beta sim mathcalN(mathrmmean = mu_beta mathrmprecision = sigma_beta) \n        sigma sim mathcalGamma(mathrmshape = 175 mathrmscale = 4554) \n        mathrmFVC_ij sim mathcalN(mathrmmean = alpha_i + t beta_i mathrmprecision = sigma)\n    endaligned\nendequation","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"where t is the time in weeks. Those are very uninformative priors, but that's ok: our model will converge!","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Implementing this model in RxInfer is pretty straightforward:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"@model function partially_pooled(patient_codes, weeks)\n    μ_α ~ Normal(mean = 0.0, var = 250000.0) # Prior for the mean of α (intercept)\n    μ_β ~ Normal(mean = 0.0, var = 9.0)      # Prior for the mean of β (slope)\n    σ_α ~ Gamma(shape = 1.75, scale = 45.54) # Prior for the precision of α (intercept)\n    σ_β ~ Gamma(shape = 1.75, scale = 1.36)  # Prior for the precision of β (slope)\n\n    n_codes = length(patient_codes)            # Total number of data points\n    n_patients = length(unique(patient_codes)) # Number of unique patients in the data\n\n    α = randomvar(n_patients)                # Individual intercepts for each patient\n    β = randomvar(n_patients)                # Individual slopes for each patient\n\n    for i in 1:n_patients\n        α[i] ~ Normal(mean = μ_α, precision = σ_α) # Sample the intercept α from a Normal distribution\n        β[i] ~ Normal(mean = μ_β, precision = σ_β) # Sample the slope β from a Normal distribution\n    end\n\n    σ ~ Gamma(shape = 1.75, scale = 45.54)   # Prior for the standard deviation of the error term\n    \n    FVC_est = randomvar(n_codes)\n    data = datavar(Int, n_codes) # Observed FVC measurements\n\n    for i in 1:n_codes\n        FVC_est[i] ~ α[patient_codes[i]] + β[patient_codes[i]] * weeks[i] # FVC estimation using patient-specific α and β\n        data[i] ~ Normal(mean = FVC_est[i], precision = σ)              # Likelihood of the observed FVC data\n    end\nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Variational constraints are used in variational methods to restrict the set of functions or probability distributions that the method can explore during optimization. These constraints help guide the optimization process towards more meaningful and tractable solutions. We need variational constraints to ensure that the optimization converges to valid and interpretable solutions, avoiding solutions that might not be meaningful or appropriate for the given problem. By incorporating constraints, we can control the complexity and shape of the solutions, making them more useful for practical applications. We use the @constraints macro from RxInfer to define approriate variational constraints.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"@constraints function partially_pooled_constraints()\n    # Assume that `μ_α`, `σ_α`, `μ_β`, `σ_β` and `σ` are jointly independent\n    q(μ_α, σ_α, μ_β, σ_β, σ) = q(μ_α)q(σ_α)q(μ_β)q(σ_β)q(σ)\n    # Assume that `μ_α`, `σ_α`, `α` are jointly independent\n    q(μ_α, σ_α, α) = q(μ_α, α)q(σ_α)\n    # Assume that `μ_β`, `σ_β`, `β` are jointly independent\n    q(μ_β, σ_β, β) = q(μ_β, β)q(σ_β)\n    # Assume that `FVC_est`, `σ` are jointly independent\n    q(FVC_est, σ) = q(FVC_est)q(σ) \nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"partially_pooled_constraints (generic function with 1 method)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"These @constraints assume some structural independencies in the resulting variational approximation. For simplicity we can also use constraints = MeanField() in the inference function below. That's all for modelling!","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Inference-in-the-model","page":"Bayesian Linear Regression Tutorial","title":"Inference in the model","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"A significant achievement of Probabilistic Programming Languages, like RxInfer, is the ability to separate model specification and inference. Once I define my generative model with priors, condition statements, and data likelihoods, I can delegate the challenging inference tasks to RxInfer's inference engine.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Calling the inference engine only takes a few lines of code. Before proceeding, let's assign a numerical Patient ID to each patient code, a task that can be easily accomplished using label encoding.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"patient_ids          = dataset[!, \"Patient\"] # get the column of all patients\npatient_code_encoder = Dict(map(((id, patient), ) -> patient => id, enumerate(unique(patient_ids))));\npatient_code_column  = map(patient -> patient_code_encoder[patient], patient_ids)\n\ndataset[!, :PatientCode] = patient_code_column\n\nfirst(patient_code_encoder, 5)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"5-element Vector{Pair{InlineStrings.String31, Int64}}:\n \"ID00197637202246865691526\" => 85\n \"ID00388637202301028491611\" => 160\n \"ID00341637202287410878488\" => 142\n \"ID00020637202178344345685\" => 9\n \"ID00305637202281772703145\" => 127","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"function partially_pooled_inference(dataset)\n\n    patient_codes = values(dataset[!, \"PatientCode\"])\n    weeks = values(dataset[!, \"Weeks\"])\n    FVC_obs = values(dataset[!, \"FVC\"]);\n\n    results = infer(\n        model = partially_pooled(patient_codes, weeks),\n        data = (data = FVC_obs, ),\n        options = (limit_stack_depth = 500, ),\n        constraints = partially_pooled_constraints(),\n        initmessages = (\n            α = vague(NormalMeanVariance),\n            β = vague(NormalMeanVariance),\n        ),\n        initmarginals = (\n            α = vague(NormalMeanVariance),\n            β = vague(NormalMeanVariance),\n            σ = vague(Gamma),\n            σ_α = vague(Gamma),\n            σ_β = vague(Gamma),\n        ),\n        returnvars = KeepLast(),\n        iterations = 100\n    )\n    \nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"partially_pooled_inference (generic function with 1 method)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"We use a hybrid message passing approach combining exact and variational inference. In loopy models, where there are cycles or feedback loops in the graphical model, we need to initialize messages to kick-start the message passing process. Messages are passed between connected nodes in the model to exchange information and update beliefs iteratively. Initializing messages provides a starting point for the iterative process and ensures that the model converges to a meaningful solution.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"In variational inference procedures, we need to initialize marginals because variational methods aim to approximate the true posterior distribution with a simpler, tractable distribution. Initializing marginals involves providing initial estimates for the parameters of this approximating distribution. These initial estimates serve as a starting point for the optimization process, allowing the algorithm to iteratively refine the approximation until it converges to a close approximation of the true posterior distribution. ","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"partially_pooled_inference_results = partially_pooled_inference(dataset)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Inference results:\n  Posteriors       | available for (α, σ_α, σ_β, σ, FVC_est, μ_β, μ_α, β)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Checking-the-model","page":"Bayesian Linear Regression Tutorial","title":"Checking the model","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Inspecting-the-learned-parameters","page":"Bayesian Linear Regression Tutorial","title":"Inspecting the learned parameters","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"# Convert to `Normal` since it supports easy plotting with `StatsPlots`\nlet \n    local μ_α = Normal(mean_std(partially_pooled_inference_results.posteriors[:μ_α])...)\n    local μ_β = Normal(mean_std(partially_pooled_inference_results.posteriors[:μ_β])...)\n    local α = map(d -> Normal(mean_std(d)...), partially_pooled_inference_results.posteriors[:α])\n    local β = map(d -> Normal(mean_std(d)...), partially_pooled_inference_results.posteriors[:β])\n    \n    local p1 = plot(μ_α, title = \"q(μ_α)\", fill = 0, fillalpha = 0.2, label = false)\n    local p2 = plot(μ_β, title = \"q(μ_β)\", fill = 0, fillalpha = 0.2, label = false)\n    \n    local p3 = plot(title = \"q(α)...\", legend = false)\n    local p4 = plot(title = \"q(β)...\", legend = false)\n    \n    foreach(d -> plot!(p3, d), α) # Add each individual `α` on plot `p3`\n    foreach(d -> plot!(p4, d), β) # Add each individual `β` on plot `p4`\n    \n    plot(p1, p2, p3, p4, size = (1200, 400), layout = @layout([ a b; c d ]))\nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Looks like our model learned personalized alphas and betas for each patient!","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Visualizing-FVC-decline-curves-for-some-patients","page":"Bayesian Linear Regression Tutorial","title":"Visualizing FVC decline curves for some patients","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Now, let's visually inspect the FVC decline curves predicted by our model. We will complete the FVC table by predicting all the missing values. To do this, we need to create a table to accommodate the predictions.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"function patientchart_bayesian(results, dataset, encoder, patient_id; kwargs...)\n    info            = patientinfo(dataset, patient_id)\n    patient_code_id = encoder[patient_id]\n\n    patient_α = results.posteriors[:α][patient_code_id]\n    patient_β = results.posteriors[:β][patient_code_id]\n\n    estimated_σ = inv(mean(results.posteriors[:σ]))\n    \n    predict_weeks = range(-12, 134)\n\n    predicted = map(predict_weeks) do week\n        pm = mean(patient_α) + mean(patient_β) * week\n        pv = var(patient_α) + var(patient_β) * week ^ 2 + estimated_σ\n        return pm, sqrt(pv)\n    end\n    \n    p = patientchart(dataset, patient_id; kwargs...)\n    \n    return plot!(p, predict_weeks, getindex.(predicted, 1), ribbon = getindex.(predicted, 2), color = :orange)\nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"patientchart_bayesian (generic function with 1 method)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"p1 = patientchart_bayesian(partially_pooled_inference_results, dataset, patient_code_encoder, \"ID00007637202177411956430\")\np2 = patientchart_bayesian(partially_pooled_inference_results, dataset, patient_code_encoder, \"ID00009637202177434476278\")\np3 = patientchart_bayesian(partially_pooled_inference_results, dataset, patient_code_encoder, \"ID00011637202177653955184\")\n\nplot(p1, p2, p3, layout = @layout([ a b c ]), size = (1200, 400))","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"The results match our expectations perfectly! Let's highlight the observations:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"The model successfully learned Bayesian Linear Regressions! The orange line representing the learned predicted FVC mean closely aligns with the red line representing the deterministic linear regression. More importantly, the model effectively predicts uncertainty, demonstrated by the light orange region surrounding the mean FVC line.\nThe model predicts higher uncertainty in cases where the data points are more dispersed, such as in the 1st and 3rd patients. In contrast, when data points are closely grouped together, as seen in the 2nd patient, the model predicts higher confidence, resulting in a narrower light orange region.\nAdditionally, across all patients, we observe that the uncertainty increases as we look further into the future. The light orange region widens as the number of weeks increases, reflecting the growth of uncertainty over time.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Computing-the-modified-Laplace-Log-Likelihood-and-RMSE","page":"Bayesian Linear Regression Tutorial","title":"Computing the modified Laplace Log Likelihood and RMSE","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"As mentioned earlier, the competition evaluated models using a modified version of the Laplace Log Likelihood, which takes into account both the accuracy and certainty of each prediction—a valuable feature in medical applications.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"To compute the metric, we predicted both the FVC value and its associated confidence measure (standard deviation σ). The metric is given by the formula:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"beginequation\n    beginaligned\n        sigma_mathrmclipped = max(sigma 70) \n        delta = min(vert mathrmFVC_mathrmtrue - mathrmFVC_mathrmpredvert 1000) \n        mathrmmetric = -fracsqrt2deltasigma_mathrmclipped - mathrmln(sqrt2sigma_mathrmclipped) \n    endaligned\nendequation","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"To prevent large errors from disproportionately penalizing results, errors were thresholded at 1000 ml. Additionally, confidence values were clipped at 70 ml to account for the approximate measurement uncertainty in FVC. The final score was determined by averaging the metric across all (Patient, Week) pairs. It is worth noting that metric values will be negative, and a higher score indicates better model performance.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"function FVC_predict(results)\n    return broadcast(results.posteriors[:FVC_est], Ref(results.posteriors[:σ])) do f, s\n        return @call_rule NormalMeanPrecision(:out, Marginalisation) (m_μ = f, q_τ = s)\n    end\nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"FVC_predict (generic function with 1 method)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"function compute_rmse(results, dataset)\n    FVC_predicted = FVC_predict(results)\n    return mean((dataset[!, \"FVC\"] .- mean.(FVC_predicted)) .^ 2) ^ (1/2)\nend\n\nfunction compute_laplace_log_likelihood(results, dataset)\n    FVC_predicted = FVC_predict(results)\n    sigma_c = std.(FVC_predicted)\n    sigma_c[sigma_c .< 70] .= 70\n    delta = abs.(mean.(FVC_predicted) .- dataset[!, \"FVC\"])\n    delta[delta .> 1000] .= 1000\n    return mean(-sqrt(2) .* delta ./ sigma_c .- log.(sqrt(2) .* sigma_c))\nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"compute_laplace_log_likelihood (generic function with 1 method)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"println(\"RMSE: $(compute_rmse(partially_pooled_inference_results, dataset))\")\nprintln(\"Laplace Log Likelihood: $(compute_laplace_log_likelihood(partially_pooled_inference_results, dataset))\")","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"RMSE: 124.0313266260767\nLaplace Log Likelihood: -6.15689374755478","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"What do these numbers signify? They indicate that adopting this approach would lead to outperforming the majority of public solutions in the competition. In several seconds of inference!","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Interestingly, most public solutions rely on a standard deterministic Neural Network and attempt to model uncertainty through a quantile loss, adhering to a frequentist approach. The importance of uncertainty in single predictions is growing in the field of machine learning, becoming a crucial requirement. Especially when the consequences of an inaccurate prediction are significant, knowing the probability distribution of individual predictions becomes essential.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Add-layer-to-model-hierarchy:-Smoking-Status","page":"Bayesian Linear Regression Tutorial","title":"Add layer to model hierarchy: Smoking Status","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"We can enhance the model by incorporating the column \"SmokingStatus\" as a pooling level, where model parameters will be partially pooled within the groups \"Never smoked,\" \"Ex-smoker,\" and \"Currently smokes.\" To achieve this, we need to:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Encode the \"SmokingStatus\" column. Map the patient encoding to the corresponding \"SmokingStatus\" encodings. Refine and retrain the model with the additional hierarchical structure.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"combine(groupby(dataset, \"SmokingStatus\"), nrow)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"3×2 DataFrame\n Row │ SmokingStatus     nrow\n     │ String31          Int64\n─────┼─────────────────────────\n   1 │ Ex-smoker          1038\n   2 │ Never smoked        429\n   3 │ Currently smokes     82","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"smoking_id_mapping   = Dict(map(((code, smoking_status), ) -> smoking_status => code, enumerate(unique(dataset[!, \"SmokingStatus\"]))))\nsmoking_code_encoder = Dict(map(unique(values(patient_ids))) do patient_id\n    smoking_status = first(unique(patientinfo(dataset, patient_id)[!, \"SmokingStatus\"]))\n    return patient_code_encoder[patient_id] => smoking_id_mapping[smoking_status]\nend)\n\nsmoking_status_patient_mapping = map(id -> smoking_code_encoder[id], 1:length(unique(patient_ids)));","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"@model function partially_pooled_with_smoking(patient_codes, smoking_status_patient_mapping, weeks)\n    μ_α_global ~ Normal(mean = 0.0, var = 250000.0) # Prior for the mean of α (intercept)\n    μ_β_global ~ Normal(mean = 0.0, var = 250000.0) # Prior for the mean of β (slope)\n    σ_α_global ~ Gamma(shape = 1.75, scale = 45.54) # Corresponds to half-normal with scale 100.0\n    σ_β_global ~ Gamma(shape = 1.75, scale = 1.36)  # Corresponds to half-normal with scale 3.0\n\n    n_codes = length(patient_codes) # Total number of data points\n    n_smoking_statuses = length(unique(smoking_status_patient_mapping)) # Number of different smoking patterns\n    n_patients = length(unique(patient_codes)) # Number of unique patients in the data\n\n    μ_α_smoking_status = randomvar(n_smoking_statuses) # Individual intercepts for smoking pattern\n    μ_β_smoking_status = randomvar(n_smoking_statuses) # Individual slopes for smoking pattern\n    \n    for i in 1:n_smoking_statuses\n        μ_α_smoking_status[i] ~ Normal(mean = μ_α_global, precision = σ_α_global)\n        μ_β_smoking_status[i] ~ Normal(mean = μ_β_global, precision = σ_β_global)\n    end\n    \n    α = randomvar(n_patients) # Individual intercepts for each patient\n    β = randomvar(n_patients) # Individual slopes for each patient\n\n    for i in 1:n_patients\n        α[i] ~ Normal(mean = μ_α_smoking_status[smoking_status_patient_mapping[i]], precision = σ_α_global)\n        β[i] ~ Normal(mean = μ_β_smoking_status[smoking_status_patient_mapping[i]], precision = σ_β_global)\n    end\n\n    σ ~ Gamma(shape = 1.75, scale = 45.54) # Corresponds to half-normal with scale 100.0\n\n    FVC_est = randomvar(n_codes)\n    data = datavar(Int, n_codes)\n\n    for i in 1:n_codes\n        FVC_est[i] ~ α[patient_codes[i]] + β[patient_codes[i]] * weeks[i] # FVC estimation using patient-specific α and β\n        data[i] ~ Normal(mean = FVC_est[i], precision = σ)              # Likelihood of the observed FVC data\n    end\n    \nend\n\n@constraints function partially_pooled_with_smooking_constraints()\n    q(μ_α_global, σ_α_global, μ_β_global, σ_β_global) = q(μ_α_global)q(σ_α_global)q(μ_β_global)q(σ_β_global)\n    q(μ_α_smoking_status, μ_β_smoking_status, σ_α_global, σ_β_global) = q(μ_α_smoking_status)q(μ_β_smoking_status)q(σ_α_global)q(σ_β_global)\n    q(μ_α_global, σ_α_global, μ_β_global, σ_β_global, σ) = q(μ_α_global)q(σ_α_global)q(μ_β_global)q(σ_β_global)q(σ)\n    q(μ_α_global, σ_α_global, α) = q(μ_α_global, α)q(σ_α_global)\n    q(μ_β_global, σ_β_global, β) = q(μ_β_global, β)q(σ_β_global)\n    q(FVC_est, σ) = q(FVC_est)q(σ) \nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"partially_pooled_with_smooking_constraints (generic function with 1 method)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"function partially_pooled_with_smoking(dataset, smoking_status_patient_mapping)\n    patient_codes = values(dataset[!, \"PatientCode\"])\n    weeks = values(dataset[!, \"Weeks\"])\n    FVC_obs = values(dataset[!, \"FVC\"]);\n    \n    return infer(\n        model = partially_pooled_with_smoking(patient_codes, smoking_status_patient_mapping, weeks),\n        data = (data = FVC_obs, ),\n        options = (limit_stack_depth = 500, ),\n        constraints = partially_pooled_with_smooking_constraints(),\n        initmessages = (\n            α = vague(NormalMeanVariance),\n            β = vague(NormalMeanVariance),\n        ),\n        initmarginals = (\n            σ = Gamma(1.75, 45.54),\n            σ_α_global = Gamma(1.75, 45.54),\n            σ_β_global = Gamma(1.75, 1.36),\n        ),\n        returnvars = KeepLast(),\n        iterations = 100,\n    )\nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"partially_pooled_with_smoking (generic function with 2 methods)","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"partially_pooled_with_smoking_inference_results = partially_pooled_with_smoking(dataset, smoking_status_patient_mapping);","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Inspect-the-learned-parameters","page":"Bayesian Linear Regression Tutorial","title":"Inspect the learned parameters","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"# Convert to `Normal` since it supports easy plotting with `StatsPlots`\nlet \n    local μ_α = Normal(mean_std(partially_pooled_with_smoking_inference_results.posteriors[:μ_α_global])...)\n    local μ_β = Normal(mean_std(partially_pooled_with_smoking_inference_results.posteriors[:μ_β_global])...)\n    local αsmoking = map(d -> Normal(mean_std(d)...), partially_pooled_with_smoking_inference_results.posteriors[:μ_α_smoking_status])\n    local βsmoking = map(d -> Normal(mean_std(d)...), partially_pooled_with_smoking_inference_results.posteriors[:μ_β_smoking_status])\n    local α = map(d -> Normal(mean_std(d)...), partially_pooled_with_smoking_inference_results.posteriors[:α])\n    local β = map(d -> Normal(mean_std(d)...), partially_pooled_with_smoking_inference_results.posteriors[:β])\n    \n    local p1 = plot(μ_α, title = \"q(μ_α_global)\", fill = 0, fillalpha = 0.2, label = false)\n    local p2 = plot(μ_β, title = \"q(μ_β_global)\", fill = 0, fillalpha = 0.2, label = false)\n    \n    local p3 = plot(title = \"q(α)...\", legend = false)\n    local p4 = plot(title = \"q(β)...\", legend = false)\n    \n    foreach(d -> plot!(p3, d), α) # Add each individual `α` on plot `p3`\n    foreach(d -> plot!(p4, d), β) # Add each individual `β` on plot `p4`\n    \n    local p5 = plot(title = \"q(μ_α_smoking_status)...\", legend = false)\n    local p6 = plot(title = \"q(μ_β_smoking_status)...\", legend = false)\n    \n    foreach(d -> plot!(p5, d, fill = 0, fillalpha = 0.2), αsmoking) \n    foreach(d -> plot!(p6, d, fill = 0, fillalpha = 0.2), βsmoking)\n    \n    plot(p1, p2, p3, p4, p5, p6, size = (1200, 600), layout = @layout([ a b; c d; e f ]))\nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Interpret-smoking-status-model-parameters","page":"Bayesian Linear Regression Tutorial","title":"Interpret smoking status model parameters","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"The model parameters for each smoking status reveal intriguing findings, particularly concerning the trend, μ_β_smoking_status. In the summary below, it is evident that the trend for current smokers has a positive mean, while the trend for ex-smokers and those who have never smoked is negative.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"smoking_id_mapping","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Dict{InlineStrings.String31, Int64} with 3 entries:\n  \"Currently smokes\" => 3\n  \"Ex-smoker\"        => 1\n  \"Never smoked\"     => 2","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"posteriors_μ_β_smoking_status = partially_pooled_with_smoking_inference_results.posteriors[:μ_β_smoking_status]\n\nprintln(\"Trend for\")\nforeach(pairs(smoking_id_mapping)) do (key, id)\n    println(\"  $key: $(mean(posteriors_μ_β_smoking_status[id]))\")\nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Trend for\n  Currently smokes: 1.8157457671008241\n  Ex-smoker: -4.56516461765951\n  Never smoked: -4.446729680170274","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Let's look at these curves for individual patients to help interpret these model results.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"# Never smoked\np1 = patientchart_bayesian(partially_pooled_with_smoking_inference_results, dataset, patient_code_encoder, \"ID00007637202177411956430\") \n# Ex-smoker\np2 = patientchart_bayesian(partially_pooled_with_smoking_inference_results, dataset, patient_code_encoder, \"ID00009637202177434476278\") \n# Currently smokes\np3 = patientchart_bayesian(partially_pooled_with_smoking_inference_results, dataset, patient_code_encoder, \"ID00011637202177653955184\") \n\nplot(p1, p2, p3, layout = @layout([ a b c ]), size = (1200, 400))","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Review-patients-that-currently-smoke","page":"Bayesian Linear Regression Tutorial","title":"Review patients that currently smoke","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"When plotting each patient with the smoking status \"Currently smokes,\" we observe different trends. Some patients show a clear positive trend, while others do not exhibit a clear trend or even have a negative trend. Compared to the unpooled trend lines, the trend lines with partial pooling are less prone to overfitting and display greater uncertainty in both slope and intercept.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Depending on the purpose of the model, we can proceed in different ways:","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"If our goal is to gain insights into how different attributes relate to a patient's FVC over time, we can stop here and understand that current smokers might experience an increase in FVC over time when monitored for Pulmonary Fibrosis. We may then formulate hypotheses to explore the reasons behind this observation and design new experiments for further testing.\nHowever, if our aim is to develop a model for generating predictions to treat patients, it becomes crucial to ensure that the model does not overfit and can be trusted with new patients. To achieve this, we could adjust model parameters to shrink the \"Currently smokes\" group's parameters closer to the global parameters, or even consider merging the group with \"Ex-smokers.\" Additionally, collecting more data for current smokers could help in ensuring the model's robustness and preventing overfitting.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"let \n    local plots = []\n\n    for (i, patient) in enumerate(unique(filter(:SmokingStatus => ==(\"Currently smokes\"), dataset)[!, \"Patient\"]))\n        push!(plots, patientchart_bayesian(partially_pooled_with_smoking_inference_results, dataset, patient_code_encoder, patient))\n    end\n\n    plot(plots..., size = (1200, 1200))\nend","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#Modified-Laplace-Log-Likelihood-and-RMSE-for-model-with-Smoking-Status-Level","page":"Bayesian Linear Regression Tutorial","title":"Modified Laplace Log Likelihood and RMSE for model with Smoking Status Level","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"We calculate the metrics for the updated model and compare to the original model.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"println(\"RMSE: $(compute_rmse(partially_pooled_with_smoking_inference_results, dataset))\")\nprintln(\"Laplace Log Likelihood: $(compute_laplace_log_likelihood(partially_pooled_with_smoking_inference_results, dataset))\")","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"RMSE: 124.7845980866563\nLaplace Log Likelihood: -6.165346437287973","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"Both the Laplace Log Likelihood and RMSE indicate slightly worse performance for the smoking status model. Adding this hierarchy level as it is did not improve the model's performance significantly. However, we did discover some interesting results from the smoking status level that might warrant further investigation. Additionally, we could attempt to enhance model performance by adjusting priors or exploring different hierarchy levels, such as gender.","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/#References","page":"Bayesian Linear Regression Tutorial","title":"References","text":"","category":"section"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"[1] Ghahramani, Z. Probabilistic machine learning and artificial intelligence. Nature 521, 452–459 (2015). https://doi.org/10.1038/nature14541","category":"page"},{"location":"examples/basic_examples/Bayesian Linear Regression Tutorial/","page":"Bayesian Linear Regression Tutorial","title":"Bayesian Linear Regression Tutorial","text":"[2] Rainforth, Thomas William Gamlen. Automating Inference, Learning, and Design Using Probabilistic Programming. University of Oxford, 2017.","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/#examples-probit-model-(ep)","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"","category":"section"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"RxInfer comes with support for expectation propagation (EP). In this demo we illustrate EP in the context of state-estimation in a linear state-space model that combines a Gaussian state-evolution model with a discrete observation model. Here, the probit function links continuous variable x_t with the discrete variable y_t. The model is defined as:","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"beginaligned\n    u = 01 \n    x_0 sim mathcalN(0 100) \n    x_t sim mathcalN(x_t-1+ u 001) \n    y_t sim mathrmBer(Phi(x_t))\nendaligned","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/#Import-packages","page":"Probit Model (EP)","title":"Import packages","text":"","category":"section"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"using RxInfer, StableRNGs, Random, Plots\nusing StatsFuns: normcdf","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/#Data-generation","page":"Probit Model (EP)","title":"Data generation","text":"","category":"section"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"function generate_data(nr_samples::Int64; seed = 123)\n    \n    rng = StableRNG(seed)\n    \n    # hyper parameters\n    u = 0.1\n\n    # allocate space for data\n    data_x = zeros(nr_samples + 1)\n    data_y = zeros(nr_samples)\n    \n    # initialize data\n    data_x[1] = -2\n    \n    # generate data\n    for k = 2:nr_samples + 1\n        \n        # calculate new x\n        data_x[k] = data_x[k-1] + u + sqrt(0.01)*randn(rng)\n        \n        # calculate y\n        data_y[k-1] = normcdf(data_x[k]) > rand(rng)\n        \n    end\n    \n    # return data\n    return data_x, data_y\n    \nend;","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"n = 40","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"40","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"data_x, data_y = generate_data(n);","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"p = plot(xlabel = \"t\", ylabel = \"x, y\")\np = scatter!(p, data_y, label = \"y\")\np = plot!(p, data_x[2:end], label = \"x\")","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/#Model-specification","page":"Probit Model (EP)","title":"Model specification","text":"","category":"section"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"@model function probit_model(nr_samples::Int64)\n    \n    # allocate space for variables\n    x = randomvar(nr_samples + 1)\n    y = datavar(Float64, nr_samples)\n    \n    # specify uninformative prior\n    x[1] ~ Normal(mean = 0.0, precision = 0.01)\n    \n    # create model \n    for k = 2:nr_samples + 1\n        x[k] ~ Normal(mean = x[k - 1] + 0.1, precision = 100)\n        y[k - 1] ~ Probit(x[k]) where {\n            # Probit node by default uses RequireMessage pipeline with vague(NormalMeanPrecision) message as initial value for `in` edge\n            # To change initial value use may specify it manually, like. Changes to the initial message may improve stability in some situations\n            pipeline = RequireMessage(in = NormalMeanPrecision(0, 0.01)) \n        }\n    end\n    \nend;","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/#Inference","page":"Probit Model (EP)","title":"Inference","text":"","category":"section"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"result = infer(\n    model = probit_model(length(data_y)), \n    data  = (y = data_y, ), \n    iterations = 5, \n    returnvars = (x = KeepLast(),),\n    free_energy  = true\n)","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"Inference results:\n  Posteriors       | available for (x)\n  Free Energy:     | Real[23.1779, 15.743, 15.6467, 15.6462, 15.6462]","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/#Results","page":"Probit Model (EP)","title":"Results","text":"","category":"section"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"mx = result.posteriors[:x]\n\np = plot(xlabel = \"t\", ylabel = \"x, y\", legend = :bottomright)\np = scatter!(p, data_y, label = \"y\")\np = plot!(p, data_x[2:end], label = \"x\", lw = 2)\np = plot!(mean.(mx)[2:end], ribbon = std.(mx)[2:end], fillalpha = 0.2, label=\"x (inferred mean)\")\n\nf = plot(xlabel = \"t\", ylabel = \"BFE\")\nf = plot!(result.free_energy, label = \"Bethe Free Energy\")\n\nplot(p, f, size = (800, 400))","category":"page"},{"location":"examples/problem_specific/Probit Model (EP)/","page":"Probit Model (EP)","title":"Probit Model (EP)","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#examples-advanced-tutorial","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"using RxInfer, Plots","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"This notebook covers the fundamentals and advanced usage of the RxInfer.jl package.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"This tutorial is also available in the documentation.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#General-model-specification-syntax","page":"Advanced Tutorial","title":"General model specification syntax","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"We use the @model macro from the RxInfer.jl package to create a probabilistic model p(s y) and we also specify extra constraints on the variational family of distributions mathcalQ, used for approximating intractable posterior distributions. Below there is a simple example of the general syntax for model specification. In this tutorial we do not cover all possible ways to create models or advanced features of RxInfer.jl.  Instead we refer the interested reader to the documentation for a more rigorous explanation and illustrative examples.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# the `@model` macro accepts a regular Julia function\n@model function test_model1(s_mean, s_precision)\n    \n    # We use the `randomvar` function to create \n    # a random variable in our model\n    s = randomvar()\n    \n    # the `tilde` operator creates a functional dependency\n    # between variables in our model and can be read as \n    # `sampled from` or `is modeled by`\n    s ~ Normal(mean = s_mean, precision = s_precision)\n    \n    # We use the `datavar` function to create \n    # observed data variables in our models\n    # We also need to specify the type of our data \n    # In this example it is `Float64`\n    y = datavar(Float64)\n    \n    y ~ Normal(mean = s, precision = 1.0)\n    \n    # It is possible to return something from the model specification (including variables and nodes)\n    return \"Hello world\"\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The @model macro creates a function with the same name and with the same set of input arguments as the original function (test_model1(s_mean, s_precision) in this example). The return value is modified in such a way to contain a reference to the model object as the first value and to the user specified variables in the form of a tuple as the second value.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"modelgenerator = test_model1(0.0, 1.0)\n\nmodel, returnval = create_model(modelgenerator)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(FactorGraphModel(), \"Hello world\")","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The benefits of using model generator as a way to create a model is that it allows to change inference constraints and meta specification for nodes. We will talk about factorisation and form constraints and meta specification later on in this demo.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"RxInfer.jl returns a factor graph-based representation of a model. We can examine this factor graph structure with the help of some utility functions such as: ","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getnodes(): returns an array of factor nodes in a corresponding factor graph\ngetrandom(): returns an array of random variables in the model\ngetdata(): returns an array of data inputs in the model\ngetconstant(): returns an array of constant values in the model","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getnodes(model)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"FactorNodesCollection(nodes: 2)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getrandom(model) .|> name","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"1-element Vector{Symbol}:\n :s","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getdata(model) .|> name","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"1-element Vector{Symbol}:\n :y","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getconstant(model) .|> getconst","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"3-element Vector{Float64}:\n 0.0\n 1.0\n 1.0","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"It is also possible to use control flow statements such as if or for blocks in the model specification function. In general, any valid snippet of Julia code can be used inside the @model block. As an example consider the following (valid!) model:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model2(n)\n    \n    if n <= 1\n        error(\"`n` argument must be greater than one.\")\n    end\n    \n    # `randomvar(n)` creates a dense sequence of \n    # random variables\n    s = randomvar(n)\n    \n    # `datavar(Float64, n)` creates a dense sequence of \n    # observed data variables of type `Float64`\n    y = datavar(Float64, n)\n    \n    s[1] ~ Normal(mean = 0.0, precision = 0.1)\n    y[1] ~ Normal(mean = s[1], precision = 1.0)\n    \n    for i in 2:n\n        s[i] ~ Normal(mean = s[i - 1], precision = 1.0)\n        y[i] ~ Normal(mean = s[i], precision = 1.0)\n    end\n    \nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"model, _ = create_model(test_model2(10));","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# An amount of factor nodes in generated Factor Graph\ngetnodes(model) |> length","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"20","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# An amount of random variables\ngetrandom(model) |> length","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"10","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# An amount of data inputs\ngetdata(model) |> length","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"10","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# An amount of constant values\ngetconstant(model) |> length","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"21","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"It is also possible to use complex expressions inside the functional dependency expressions","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"y ~ NormalMeanPrecision(2.0 * (s + 1.0), 1.0)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The ~ operator automatically creates a random variable if none was created before with the same name and throws an error if this name already exists","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# s = randomvar() here is optional\n# `~` creates random variables automatically\ns ~ NormalMeanPrecision(0.0, 1.0)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"An example model which will throw an error:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function error_model1()\n    s = 1.0\n    s ~ NormalMeanPrecision(0.0, 1.0)\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"By default the RxInfer.jl package creates new references for constants (literals like 0.0 or 1.0) in a model. In some situations this may not be efficient, especially when these constants represent large matrices. RxInfer.jl will by default create new copies of some constant (e.g. matrix) in a model every time it uses it. However it is possible to use constvar() function to create and reuse similar constants in the model specification syntax as","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Creates constant reference in a model with a prespecified value\nc = constvar(0.0)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"An example:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model5(dim::Int, n::Int, A::Matrix, P::Matrix, Q::Matrix)\n    \n    s = randomvar(n)\n    \n    y = datavar(Vector{Float64}, n)\n    \n    # Here we create constant references\n    # for constant matrices in our model \n    # to make inference more memory efficient\n    cA = constvar(A)\n    cP = constvar(P)\n    cQ = constvar(Q)\n    \n    s[1] ~ MvNormal(mean = zeros(dim), covariance = cP)\n    y[1] ~ MvNormal(mean = s[1], covariance = cQ)\n    \n    for i in 2:n\n        s[i] ~ MvNormal(mean = cA * s[i - 1], covariance = cP)\n        y[i] ~ MvNormal(mean = s[i], covariance = cQ)\n    end\n    \nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The ~ expression can also return a reference to a newly created node in a corresponding factor graph for convenience in later usage:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model()\n\n    # In this example `ynode` refers to the corresponding \n    # `GaussianMeanVariance` node created in the factor graph\n    ynode, y ~ GaussianMeanVariance(0.0, 1.0)\n    \n    return ynode, y\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#Probabilistic-inference-in-RxInfer.jl","page":"Advanced Tutorial","title":"Probabilistic inference in RxInfer.jl","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"RxInfer.jl uses the Rocket.jl package API for inference routines. Rocket.jl is a reactive programming extension for Julia that is higly inspired by RxJS and similar libraries from the Rx ecosystem. It consists of observables, actors, subscriptions and operators. For more information and rigorous examples see Rocket.jl github page.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#Observables","page":"Advanced Tutorial","title":"Observables","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Observables are lazy push-based collections and they deliver their values over time.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Timer that emits a new value every second and has an initial one second delay \nobservable = timer(300, 300)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"TimerObservable(300, 300)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"A subscription allows us to subscribe on future values of some observable, and actors specify what to do with these new values:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"actor = (value) -> println(value)\nsubscription1 = subscribe!(observable, actor)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"TimerSubscription()","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# We always need to unsubscribe from some observables\nunsubscribe!(subscription1)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# We can modify our observables\nmodified = observable |> filter(d -> rem(d, 2) === 1) |> map(Int, d -> d ^ 2)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"ProxyObservable(Int64, MapProxy(Int64))","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"subscription2 = subscribe!(modified, (value) -> println(value))","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"TimerSubscription()","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"unsubscribe!(subscription2)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function coin_toss_model(n)\n\n    # `datavar` creates data 'inputs' in our model\n    # We will pass data later on to these inputs\n    # In this example we create a sequence of inputs that accepts Float64\n    y = datavar(Float64, n)\n    \n    # We endow θ parameter of our model with some prior\n    θ ~ Beta(2.0, 7.0)\n    \n    # We assume that the outcome of each coin flip \n    # is modeled by a Bernoulli distribution\n    for i in 1:n\n        y[i] ~ Bernoulli(θ)\n    end\n    \n    # We return references to our data inputs and θ parameter\n    # We will use these references later on during the inference step\n    return y, θ\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"We can call the inference function to run inference in such model:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"p = 0.75 # Bias of a coin\n\ndataset = float.(rand(Bernoulli(p), 500));\n\nresult = infer(\n    model = coin_toss_model(length(dataset)),\n    data  = (y = dataset, )\n)\n\nprintln(\"Inferred bias: \", mean_var(result.posteriors[:θ]))","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Inferred bias: (0.7662082514734774, 0.00035124150362241923)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"We can see that the inferred bias is quite close to the actual value we used in the dataset generation.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The RxInfer.jl package's API is more flexible (and reactive!) and can return posterior marginal distributions in our specified model in the form of an observable. It is possible to subscribe on its future updates, but for convenience RxInfer.jl only caches the last obtained values of all marginals in a model. To get a reference for the posterior marginal of some random variable in a model RxInfer.jl exports two functions: ","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getmarginal(x): for a single random variable x\ngetmarginals(xs): for a dense sequence of random variables sx","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Let's see how it works in practice. Here we create a simple coin toss model. We assume that observations are governed by the Bernoulli distribution with unknown bias parameter θ. To have a fully Bayesian treatment of this problem we endow θ with the Beta prior.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"_, (y, θ) = create_model(coin_toss_model(length(dataset)));","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# As soon as we have a new value for the marginal posterior over the `θ` variable\n# we simply print the first two statistics of it\nθ_subscription = subscribe!(getmarginal(θ), (marginal) -> println(\"New update: mean(θ) = \", mean(marginal), \", std(θ) = \", std(marginal)));","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"To pass data to our model we use update! function","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"update!(y, dataset)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"New update: mean(θ) = 0.7662082514734774, std(θ) = 0.01874143814178675","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# It is necessary to always unsubscribe from running observables\nunsubscribe!(θ_subscription)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# The ReactiveMP.jl inference backend is lazy and does not compute posterior marginals if no-one is listening for them\n# At this moment we have already unsubscribed from the new posterior updates so this `update!` does nothing\nupdate!(y, dataset)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Rocket.jl provides some useful built-in actors for obtaining posterior marginals especially with static datasets.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# the `keep` actor simply keeps all incoming updates in an internal storage, ordered\nθvalues = keep(Marginal)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"KeepActor{Marginal}(Marginal[])","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# `getmarginal` always emits last cached value as its first value\nsubscribe!(getmarginal(θ) |> take(1), θvalues);","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getvalues(θvalues)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"1-element Vector{Marginal}:\n Marginal(Beta{Float64}(α=390.0, β=119.0))","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"subscribe!(getmarginal(θ) |> take(1), θvalues);","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getvalues(θvalues)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"2-element Vector{Marginal}:\n Marginal(Beta{Float64}(α=390.0, β=119.0))\n Marginal(Beta{Float64}(α=390.0, β=119.0))","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# the `buffer` actor keeps very last incoming update in an internal storage and can also store \n# an array of updates for a sequence of random variables\nθbuffer = buffer(Marginal, 1)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"BufferActor{Marginal, Vector{Marginal}}(Marginal[#undef])","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"subscribe!(getmarginals([ θ ]) |> take(1), θbuffer);","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getvalues(θbuffer)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"1-element Vector{Marginal}:\n Marginal(Beta{Float64}(α=390.0, β=119.0))","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"subscribe!(getmarginals([ θ ]) |> take(1), θbuffer);","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getvalues(θbuffer)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"1-element Vector{Marginal}:\n Marginal(Beta{Float64}(α=390.0, β=119.0))","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#Reactive-Online-Inference","page":"Advanced Tutorial","title":"Reactive Online Inference","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"RxInfer.jl naturally supports reactive streams of data and it is possible to run reactive inference with some external datasource.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function online_coin_toss_model()\n    \n    # We create datavars for the prior \n    # over `θ` variable\n    θ_a = datavar(Float64)\n    θ_b = datavar(Float64)\n    \n    θ ~ Beta(θ_a, θ_b)\n    \n    y = datavar(Float64)\n    y ~ Bernoulli(θ)\n\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"autoupdates = @autoupdates begin \n    θ_a, θ_b = params(q(θ))\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(θ_a,θ_b = params(q(θ)),)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"rxresult = infer(\n    model = online_coin_toss_model(),\n    data  = (y = dataset, ),\n    autoupdates = autoupdates,\n    historyvars = (θ = KeepLast(), ),\n    keephistory = length(dataset),\n    initmarginals = (\n        θ = vague(Beta),\n    ),\n    autostart = true\n);","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"animation = @animate for i in 1:length(dataset)\n    plot(mean.(rxresult.history[:θ][1:i]), ribbon = std.(rxresult.history[:θ][1:i]), title = \"Online coin bias inference\", label = \"Inferred bias\", legend = :bottomright)\n    hline!([ p ], label = \"Real bias\", size = (600, 200))\nend\n\ngif(animation, \"../pics/online-coin-bias-inference.gif\", fps = 24, show_msg = false);","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In this example we used static dataset and the history field of the reactive inference result, but the rxinference function also supports any real-time reactive stream and can run indefinitely.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"That was an example of exact Bayesian inference with Sum-Product (or Belief Propagation) algorithm. However, RxInfer is not limited to only the sum-product algorithm but it also supports variational message passing with Constrained Bethe Free Energy Minimisation.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#Variational-inference","page":"Advanced Tutorial","title":"Variational inference","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"On a very high-level, ReactiveMP.jl is aimed to solve the Constrained Bethe Free Energy minimisation problem. For this task we approximate our exact posterior marginal distribution by some family of distributions q in mathcalQ. Often this involves assuming some factorization over q. For this purpose the @model macro supports optional where { ... } clauses for every ~ expression in a model specification.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model6_with_manual_constraints(n)\n    τ ~ Gamma(shape = 1.0, rate = 1.0) \n    μ ~ Normal(mean = 0.0, variance = 100.0)\n    \n    y = datavar(Float64, n)\n    \n    for i in 1:n\n        # Here we assume a mean-field assumption on our \n        # variational family of distributions locally for the current node\n        y[i] ~ Normal(mean = μ, precision = τ) where { q = q(y[i])q(μ)q(τ) }\n    end\n\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In this example we specified an extra constraints for q_a for Bethe factorisation:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"beginaligned\nq(s) = prod_a in mathcalV q_a(s_a) prod_i in mathcalE q_i^-1(s_i)\nendaligned","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"There are several options to specify the mean-field factorisation constraint. ","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i])q(μ)q(τ) } # With names from model specification\ny[i] ~ NormalMeanPrecision(μ, τ) where { q = q(out)q(mean)q(precision) } # With names from node specification\ny[i] ~ NormalMeanPrecision(μ, τ) where { q = MeanField() } # With alias name","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"It is also possible to use local structured factorisation:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i], μ)q(τ) } # With names from model specification\ny[i] ~ NormalMeanPrecision(μ, τ) where { q = q(out, mean)q(precision) } # With names from node specification","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#RxInfer.jl-constraints-macro","page":"Advanced Tutorial","title":"RxInfer.jl constraints macro","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"RxInfer.jl package exports @constraints macro to simplify factorisation and form constraints specification. Read more about @constraints macro in the corresponding documentation section, here we show a simple example of the same factorisation constraints specification, but with @constraints macro:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"constraints6 = @constraints begin\n     q(μ, τ) = q(μ)q(τ) # Mean-Field over `μ` and `τ`\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Constraints:\n  marginals form:\n  messages form:\n  factorisation:\n    q(μ, τ) = q(μ)q(τ)\nOptions:\n  warn = true","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Note: where blocks have higher priority over constraints specification","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model6(n)\n    τ ~ Gamma(shape = 1.0, rate = 1.0) \n    μ ~ Normal(mean = 0.0, variance = 100.0)\n    \n    y = datavar(Float64, n)\n    \n    for i in 1:n\n        # Here we assume a mean-field assumption on our \n        # variational family of distributions locally for the current node\n        y[i] ~ Normal(mean = μ, precision = τ)\n    end\n    \n    return μ, τ, y\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#Inference","page":"Advanced Tutorial","title":"Inference","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"To run inference in this model we again need to create a synthetic dataset:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"dataset = rand(Normal(-3.0, inv(sqrt(5.0))), 1000);","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#inference-function","page":"Advanced Tutorial","title":"inference function","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In order to simplify model and inference testing, RxInfer.jl exports pre-written inference function, that is aimed for simple use cases with static datasets:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Use ?inference to quickly check the documentation for the inference function.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"result = infer(\n    model         = test_model6(length(dataset)),\n    data          = (y = dataset, ),\n    constraints   = constraints6, \n    initmarginals = (μ = vague(NormalMeanPrecision), τ = vague(GammaShapeRate)),\n    returnvars    = (μ = KeepLast(), τ = KeepLast()),\n    iterations    = 10,\n    free_energy   = true,\n    showprogress  = true\n)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Inference results:\n  Posteriors       | available for (μ, τ)\n  Free Energy:     | Real[14763.3, 3276.03, 684.158, 646.181, 646.181, 646.\n181, 646.181, 646.181, 646.181, 646.181]","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"println(\"μ: mean = \", mean(result.posteriors[:μ]), \", std = \", std(result.posteriors[:μ]))","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"μ: mean = -3.0056908466725094, std = 0.014489267624899296","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"println(\"τ: mean = \", mean(result.posteriors[:τ]), \", std = \", std(result.posteriors[:τ]))","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"τ: mean = 4.763281190254187, std = 0.2128077093353377","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#Manual-inference","page":"Advanced Tutorial","title":"Manual inference","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"For advanced use cases it is advised to write inference functions manually as it provides more flexibility, here is an example of manual inference specification:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"model, (μ, τ, y) = create_model(test_model6(length(dataset)), constraints = constraints6);","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"For variational inference we also usually need to set initial marginals for our inference procedure. For that purpose ReactiveMP inference engine export the setmarginal! function:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"setmarginal!(μ, vague(NormalMeanPrecision))\nsetmarginal!(τ, vague(GammaShapeRate))","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"μ_values = keep(Marginal)\nτ_values = keep(Marginal)\n\nμ_subscription = subscribe!(getmarginal(μ), μ_values)\nτ_subscription = subscribe!(getmarginal(τ), τ_values)\n\nfor i in 1:10\n    update!(y, dataset)\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getvalues(μ_values)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"10-element Vector{Marginal}:\n Marginal(NormalWeightedMeanPrecision{Float64}(xi=-3.011708551085051e-9, w=\n0.010000001002000566))\n Marginal(NormalWeightedMeanPrecision{Float64}(xi=-27.568551355196337, w=9.\n182098823340285))\n Marginal(NormalWeightedMeanPrecision{Float64}(xi=-9439.094316526523, w=314\n0.4109865644887))\n Marginal(NormalWeightedMeanPrecision{Float64}(xi=-14309.600684674531, w=47\n60.835837794097))\n Marginal(NormalWeightedMeanPrecision{Float64}(xi=-14316.97336150061, w=476\n3.288738549474))\n Marginal(NormalWeightedMeanPrecision{Float64}(xi=-14316.980723228184, w=47\n63.291187807456))\n Marginal(NormalWeightedMeanPrecision{Float64}(xi=-14316.980730575227, w=47\n63.29119025181))\n Marginal(NormalWeightedMeanPrecision{Float64}(xi=-14316.980730582567, w=47\n63.291190254112))\n Marginal(NormalWeightedMeanPrecision{Float64}(xi=-14316.980730582556, w=47\n63.291190254099))\n Marginal(NormalWeightedMeanPrecision{Float64}(xi=-14316.980730582549, w=47\n63.291190254099))","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getvalues(τ_values)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"10-element Vector{Marginal}:\n Marginal(GammaShapeRate{Float64}(a=501.0, b=5.0000000000462156e14))\n Marginal(GammaShapeRate{Float64}(a=501.0, b=54622.176412349254))\n Marginal(GammaShapeRate{Float64}(a=501.0, b=159.5337672301783))\n Marginal(GammaShapeRate{Float64}(a=501.0, b=105.23384325945597))\n Marginal(GammaShapeRate{Float64}(a=501.0, b=105.17965197908343))\n Marginal(GammaShapeRate{Float64}(a=501.0, b=105.17959789617599))\n Marginal(GammaShapeRate{Float64}(a=501.0, b=105.17959784220113))\n Marginal(GammaShapeRate{Float64}(a=501.0, b=105.17959784214716))\n Marginal(GammaShapeRate{Float64}(a=501.0, b=105.17959784214723))\n Marginal(GammaShapeRate{Float64}(a=501.0, b=105.17959784214727))","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"println(\"μ: mean = \", mean(last(μ_values)), \", std = \", std(last(μ_values)))","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"μ: mean = -3.0056908466725094, std = 0.014489267624899296","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"println(\"τ: mean = \", mean(last(τ_values)), \", std = \", std(last(τ_values)))","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"τ: mean = 4.763281190254187, std = 0.2128077093353377","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#Form-constraints","page":"Advanced Tutorial","title":"Form constraints","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In order to support form constraints, the randomvar() function also supports a where { ... } clause with some optional arguments. One of these arguments is form_constraint that allows us to specify a form constraint to the random variables in our model. Another one is prod_constraint that allows to specify an additional constraints during computation of product of two colliding messages. For example we can perform the EM algorithm if we assign a point mass contraint on some variables in our model.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"<img style=\"display: block;   margin-left: auto;   margin-right: auto;   width: 50%;\" src=\"./pics/posterior.png\" />","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model7_with_manual_constraints(n)\n    τ ~ Gamma(shape = 1.0, rate = 1.0) \n    \n    # In case of form constraints `randomvar()` call is necessary\n    μ = randomvar() where { marginal_form_constraint = PointMassFormConstraint() }\n    μ ~ Normal(mean = 0.0, variance = 100.0)\n    \n    y = datavar(Float64, n)\n    \n    for i in 1:n\n        y[i] ~ Normal(mean = μ, precision = τ) where { q = q(y[i])q(μ)q(τ) }\n    end\n    \n    return μ, τ, y\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"As in the previous example we can use @constraints macro to achieve the same goal with a nicer syntax:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"constraints7 = @constraints begin \n    q(μ) :: PointMass\n    \n    q(μ, τ) = q(μ)q(τ) # Mean-Field over `μ` and `τ`\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Constraints:\n  marginals form:\n    q(μ) :: PointMassFormConstraint() [ prod_constraint = GenericProd() ]\n  messages form:\n  factorisation:\n    q(μ, τ) = q(μ)q(τ)\nOptions:\n  warn = true","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In this example we specified an extra constraints for q_i for Bethe factorisation:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"beginaligned\nq(s) = prod_a in mathcalV q_a(s_a) prod_i in mathcalE q_i^-1(s_i)\nendaligned","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model7(n)\n    τ ~ Gamma(shape = 1.0, rate = 1.0) \n    \n    # In case of form constraints `randomvar()` call is necessary\n    μ = randomvar()\n    μ ~ Normal(mean = 0.0, variance = 100.0)\n    \n    y = datavar(Float64, n)\n    \n    for i in 1:n\n        y[i] ~ Normal(mean = μ, precision = τ)\n    end\n    \n    return μ, τ, y\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"model, (μ, τ, y) = create_model(test_model7(length(dataset)), constraints = constraints7);","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"setmarginal!(μ, vague(NormalMeanPrecision))\nsetmarginal!(τ, PointMass(1.0))\n\nμ_values = keep(Marginal)\nτ_values = keep(Marginal)\n\nμ_subscription = subscribe!(getmarginal(μ), μ_values)\nτ_subscription = subscribe!(getmarginal(τ), τ_values)\n\nfor i in 1:10\n    update!(y, dataset)\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getvalues(μ_values) |> last","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Marginal(PointMass{Float64}(-3.0056908529699893))","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"getvalues(τ_values) |> last","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Marginal(GammaShapeRate{Float64}(a=501.0, b=105.07462840395456))","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"By default RxInfer tries to compute an analytical product of two colliding messages and throws an error if no analytical solution is known. However, it is possible to fall back to a generic product that does not require an analytical solution to be known. In this case the inference backend will simply propagate the product of two messages in a form of a tuple. It is not possible to use such a tuple-product during an inference and in this case it is mandatory to use some form constraint to approximate this product.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"μ = randomvar() where { \n    prod_constraint = ProdGeneric(),\n    form_constraint = SampleListFormConstraint() \n}","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Sometimes it is useful to preserve a specific parametrisation of the resulting product later on in an inference procedure. The ReactiveMP inference engine exports a special prod_constraint called ProdPreserveType especially for that purpose:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"μ = randomvar() where { prod_constraint = ProdPreserveType(NormalWeightedMeanPrecision) }","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Note: @constraints macro specifies required prod_constraint automatically.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#Free-Energy","page":"Advanced Tutorial","title":"Free Energy","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"During variational inference the RxInfer optimises a special functional called the Bethe Free Energy functional. It is possible to obtain its values for all VMP iterations with the score function.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"model, (μ, τ, y) = create_model(test_model6(length(dataset)), constraints = constraints6);","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"bfe_observable = score(model, Float64, BetheFreeEnergy())","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"ProxyObservable(Float64, ErrorIfProxy())","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"bfe_subscription = subscribe!(bfe_observable, (fe) -> println(\"Current BFE value: \", fe));","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Reset the model with vague marginals\nsetmarginal!(μ, vague(NormalMeanPrecision))\nsetmarginal!(τ, vague(GammaShapeRate))\n\nfor i in 1:10\n    update!(y, dataset)\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Current BFE value: 658.7153120621533\nCurrent BFE value: 646.1805207523544\nCurrent BFE value: 646.1805205034352\nCurrent BFE value: 646.1805205034316\nCurrent BFE value: 646.1805205034375\nCurrent BFE value: 646.1805205034352\nCurrent BFE value: 646.1805205034343\nCurrent BFE value: 646.1805205034334\nCurrent BFE value: 646.1805205034334\nCurrent BFE value: 646.1805205034334","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# It always necessary to unsubscribe and release computer resources\nunsubscribe!([ μ_subscription, τ_subscription, bfe_subscription ])","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#Meta-data-specification","page":"Advanced Tutorial","title":"Meta data specification","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"During model specification some functional dependencies may accept an optional meta object in the where { ... } clause. The purpose of the meta object is to adjust, modify or supply some extra information to the inference backend during the computations of the messages. The meta object for example may contain an approximation method that needs to be used during various approximations or it may specify the tradeoff between accuracy and performance:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# In this example the `meta` object for the autoregressive `AR` node specifies the variate type of \n# the autoregressive process and its order. In addition it specifies that the message computation rules should\n# respect accuracy over speed with the `ARsafe()` strategy. In contrast, `ARunsafe()` strategy tries to speedup computations\n# by cost of possible numerical instabilities during an inference procedure\ns[i] ~ AR(s[i - 1], θ, γ) where { q = q(s[i - 1], s[i])q(θ)q(γ), meta = ARMeta(Multivariate, order, ARsafe()) }\n...\ns[i] ~ AR(s[i - 1], θ, γ) where { q = q(s[i - 1], s[i])q(θ)q(γ), meta = ARMeta(Univariate, order, ARunsafe()) }","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Another example with GaussianControlledVariance, or simply GCV [see Hierarchical Gaussian Filter], node:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# In this example we specify structured factorisation and flag meta with `GaussHermiteCubature` \n# method with `21` sigma points for approximation of non-lineariety between hierarchy layers\nxt ~ GCV(xt_min, zt, real_k, real_w) where { q = q(xt, xt_min)q(zt)q(κ)q(ω), meta = GCVMetadata(GaussHermiteCubature(21)) }","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The Meta object is useful to pass any extra information to a node that is not a random variable or constant model variable. It may include extra approximation methods, differentiation methods, optional non-linear functions, extra inference parameters etc.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#RxInfer.jl-@meta-macro","page":"Advanced Tutorial","title":"RxInfer.jl @meta macro","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Users can use @meta macro from the RxInfer.jl package to achieve the same goal. Read more about @meta macro in the corresponding documentation section. Here is a simple example of the same meta specification:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@meta begin \n     AR(s, θ, γ) -> ARMeta(Multivariate, 5, ARsafe())\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Meta specification:\n  AR(s, θ, γ) -> ARMeta{Multivariate, ARsafe}(5, ARsafe())\nOptions:\n  warn = true","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#Creating-custom-nodes-and-message-computation-rules","page":"Advanced Tutorial","title":"Creating custom nodes and message computation rules","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/#Custom-nodes","page":"Advanced Tutorial","title":"Custom nodes","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"To create a custom functional form and to make it available during model specification the ReactiveMP inference engine exports the @node macro:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# `@node` macro accepts a name of the functional form, its type, either `Stochastic` or `Deterministic` and an array of interfaces:\n@node NormalMeanVariance Stochastic [ out, μ, v ]\n\n# Interfaces may have aliases for their names that might be convenient for factorisation constraints specification\n@node NormalMeanVariance Stochastic [ out, (μ, aliases = [ mean ]), (v, aliases = [ var ]) ]\n\n# `NormalMeanVariance` structure declaration must exist, otherwise `@node` macro will throw an error\nstruct NormalMeanVariance end \n\n@node NormalMeanVariance Stochastic [ out, μ, v ]\n\n# It is also possible to use function objects as a node functional form\nfunction dot end\n\n# Syntax for functions is a bit differet, as it is necesssary to use `typeof(...)` function for them \n# out = dot(x, a)\n@node typeof(dot) Deterministic [ out, x, a ]","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Note: Deterministic nodes do not support factorisation constraints with the where { q = ... } clause.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"After that it is possible to use the newly created node during model specification:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function test_model()\n    ...\n    y ~ dot(x, a)\n    ...\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#Custom-messages-computation-rules","page":"Advanced Tutorial","title":"Custom messages computation rules","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"RxInfer.jl exports the @rule macro to create custom message computation rules. For example let us create a simple + node to be available for usage in the model specification usage. We refer to A Factor Graph Approach to Signal Modelling , System Identification and Filtering [ Sascha Korl, 2005, page 32 ] for a rigorous explanation of the + node in factor graphs. According to Korl, assuming that inputs are Gaussian Sum-Product message computation rule for + node is the following:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"beginaligned\nmu_z = mu_x + mu_y\nV_z = V_x + V_y\nendaligned","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"To specify this in RxInfer.jl we use the @node and @rule macros:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@node typeof(+) Deterministic  [ z, x, y ]\n\n@rule typeof(+)(:z, Marginalisation) (m_x::UnivariateNormalDistributionsFamily, m_y::UnivariateNormalDistributionsFamily) = begin\n    x_mean, x_var = mean_var(m_x)\n    y_mean, y_var = mean_var(m_y)\n    return NormalMeanVariance(x_mean + y_mean, x_var + y_var)\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In this example, for the @rule macro, we specify a type of our functional form: typeof(+). Next, we specify an edge we are going to compute an outbound message for. Marginalisation indicates that the corresponding message respects the marginalisation constraint for posterior over corresponding edge:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"beginaligned\nq(z) = int q(z x y) mathrmdxmathrmdy\nendaligned","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"If we look on difference between sum-product rules and variational rules with mean-field assumption we notice that they require different local information to compute an outgoing message:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"(Image: ) (Image: )","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"beginaligned\nmu(z) = int f(x y z)mu(x)mu(y)mathrmdxmathrmdy\nendaligned","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"beginaligned\nnu(z) = exp int log f(x y z)q(x)q(y)mathrmdxmathrmdy \nendaligned","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"The @rule macro supports both cases with special prefixes during rule specification:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"m_ prefix corresponds to the incoming message on a specific edge\nq_ prefix corresponds to the posterior marginal of a specific edge","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Example of a Sum-Product rule with m_ messages used:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule NormalMeanPrecision(:μ, Marginalisation) (m_out::UnivariateNormalDistributionsFamily, m_τ::PointMass) = begin \n    m_out_mean, m_out_cov = mean_cov(m_out)\n    return NormalMeanPrecision(m_out_mean, inv(m_out_cov + inv(mean(m_τ))))\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Example of a Variational rule with Mean-Field assumption with q_ posteriors used:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule NormalMeanPrecision(:μ, Marginalisation) (q_out::Any, q_τ::Any) = begin \n    return NormalMeanPrecision(mean(q_out), mean(q_τ))\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"RxInfer.jl also supports structured rules. It is possible to obtain joint marginal over a set of edges:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule NormalMeanPrecision(:τ, Marginalisation) (q_out_μ::Any, ) = begin\n    m, V = mean_cov(q_out_μ)\n    θ = 2 / (V[1,1] - V[1,2] - V[2,1] + V[2,2] + abs2(m[1] - m[2]))\n    α = convert(typeof(θ), 1.5)\n    return Gamma(α, θ)\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"NOTE: In the @rule specification the messages or marginals arguments must be in order with interfaces specification from @node macro:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Inference backend expects arguments in `@rule` macro to be in the same order\n@node NormalMeanPrecision Stochastic [ out, μ, τ ]","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Any rule always has access to the meta information with hidden the meta::Any variable:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any) = begin \n    ...\n    println(meta)\n    ...\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"It is also possible to dispatch on a specific type of a meta object:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any, meta::LaplaceApproximation) = begin \n    ...\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"or","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any, meta::GaussHermiteCubature) = begin \n    ...\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/#Customizing-messages-computational-pipeline","page":"Advanced Tutorial","title":"Customizing messages computational pipeline","text":"","category":"section"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"In certain situations it might be convenient to customize the default message computational pipeline. RxInfer.jl supports the pipeline keyword in the where { ... } clause to add some extra steps after a message has been computed. A use case might be an extra approximation method to preserve conjugacy in the model, debugging or simple printing.","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"<img style=\"display: block;   margin-left: auto;   margin-right: auto;   width: 30%;\" src=\"./pics/pipeline.png\" width=\"20%\" />","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Logs all outbound messages\ny[i] ~ NormalMeanPrecision(x[i], 1.0) where { pipeline = LoggerPipelineStage() }\n# Initialise messages to be vague\ny[i] ~ NormalMeanPrecision(x[i], 1.0) where { pipeline = InitVaguePipelineStage() }\n# In principle, it is possible to approximate outbound messages with Laplace Approximation\ny[i] ~ NormalMeanPrecision(x[i], 1.0) where { pipeline = LaplaceApproximation() }","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"Let us return to the coin toss model, but this time we want to print flowing messages:","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"@model function coin_toss_model_log(n)\n\n    y = datavar(Float64, n)\n\n    θ ~ Beta(2.0, 7.0) where { pipeline = LoggerPipelineStage(\"θ\") }\n\n    for i in 1:n\n        y[i] ~ Bernoulli(θ)  where { pipeline = LoggerPipelineStage(\"y[$i]\") }\n    end\n    \n    return y, θ\nend","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"_, (y, θ) = RxInfer.create_model(coin_toss_model_log(5));","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"θ_subscription = subscribe!(getmarginal(θ), (value) -> println(\"New posterior marginal for θ: \", value));","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"[θ][Beta][out]: VariationalMessage()","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"coinflips = float.(rand(Bernoulli(0.5), 5));","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"update!(y, coinflips)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"[y[1]][Bernoulli][p]: VariationalMessage()\n[y[2]][Bernoulli][p]: VariationalMessage()\n[y[3]][Bernoulli][p]: VariationalMessage()\n[y[4]][Bernoulli][p]: VariationalMessage()\n[y[5]][Bernoulli][p]: VariationalMessage()\nNew posterior marginal for θ: Marginal(Beta{Float64}(α=4.0, β=10.0))","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"unsubscribe!(θ_subscription)","category":"page"},{"location":"examples/advanced_examples/Advanced Tutorial/","page":"Advanced Tutorial","title":"Advanced Tutorial","text":"# Inference is lazy and does not send messages if no one is listening for them\nupdate!(y, coinflips)","category":"page"},{"location":"manuals/custom-node/#create-node","page":"Defining a custom node and rules","title":"Creating your own custom nodes","text":"","category":"section"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Welcome to the RxInfer documentation on creating custom factor graph nodes. In RxInfer, factor nodes represent functional relationships between variables, also known as factors. Together, these factors define your probabilistic model. Quite often these factors represent distributions, denoting how a certain parameter affects another. However, other factors are also possible, such as ones specifying linear or non-linear relationships. RxInfer already supports a lot of factor nodes, however, depending on the problem that you are trying to solve, you may need to create a custom node that better fits the specific requirements of your model. This tutorial will guide you through the process of defining a custom node in RxInfer, step by step. By the end of this tutorial, you will be able to create your own custom node and integrate it into your model.","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"To create a custom node in RxInfer, 4 steps are required:","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Create your custom node in RxInfer using the @node macro.\nDefine the corresponding message passing update rules with the @rule macro. These rules specify how the node processes information in the form of messages, and how it communicates the results to adjacent parts of the model.\nSpecify computations for marginal distributions of the relevant variables with the @marginalrule macro.\nImplement the computation of the Free Energy in a node with the @average_energy macro.","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Throughout this tutorial, we will create a node for the Bernoulli distribution. The Bernoulli distribution is a commonly used distribution in statistical modeling that is often used to model a binary outcome, such as a coin flip. By recreating this node, we will be able to demonstrate the process of creating a custom node, from notifying RxInfer of the nodes existence to implementing the required methods. While this tutorial focuses on the Bernoulli distribution, the principles can be applied to creating custom nodes for other distributions as well. So let's get started!","category":"page"},{"location":"manuals/custom-node/#Problem-statement","page":"Defining a custom node and rules","title":"Problem statement","text":"","category":"section"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Jane wants to determine whether a coin is a fair coin, meaning that is equally likely to land on heads or tails. In order to determine this, she will throw the coin K=20 times and write down how often it lands on heads and tails. The result of this experiment is a realization of the underlying stochastic process. Jane models the outcome of the experiment x_kin01 using the Bernoulli distribution as","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"p(x_k mid pi) = mathrmBer(x_k mid pi) = pi^x_k (1-pi)^1-x_k","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"where pi in01 denotes the probability that she throws heads, also known as the success probability. Jane also has a prior belief (initial guess) about the value of pi which she models using the Beta distribution as","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"p(pi) = mathrmBeta(pi mid 4 8)","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"With this prior belief, the total probabilistic model that she has for this experiment is given by","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"p(x_1K pi) = p(pi) prod_k=1^K p(x_k mid pi)","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Jane is interested in determining the fairness of the coin. Therefore she aims to infer (calculate) the posterior belief of pi, p(pi mid x_1K), denoting how pi is distributed after we have seen the data.","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"","category":"page"},{"location":"manuals/custom-node/#Step-1:-Creating-the-custom-node","page":"Defining a custom node and rules","title":"Step 1: Creating the custom node","text":"","category":"section"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"note: Note\nIn this example we will assume that the Bernoulli node and distribution do not yet exist. The RxInfer already defines the node for the Bernoulli distribution from the Distributions.jl package.","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"First things first, let's import RxInfer:","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"using RxInfer","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"In order to define a custom node using the @node macro from RxInfer, we need the following three arguments:","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"The name of the node. (::Type)\nWhether the node is Deterministic or Stochastic.\nThe interfaces of the node and any potential aliases. (::Vector)","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"For the name of the node we wish to use MyBernoulli in this tutorial (Bernoulli already exists). However, the corresponding distribution does not yet exist. Therefore we need to specify it first as","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"# struct for Bernoulli distribution with success probability π\nstruct MyBernoulli{T <: Real} <: ContinuousUnivariateDistribution\n    π :: T\n    new(π :: Real) = 0 ≤ π ≤ 1 ? MyBernoulli(π) : throw(ArgumentError(\"π must be between 0 and 1\"))\t\nend\n\n# for simplicity, let's also specify the mean of the distribution\nDistributions.mean(d::MyBernoulli) = d.π\n\nnothing # hide","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"In this case the distribution also has an entry in the struct, however, this is not necessary as long as the name of the distribution is a Type. The custom node created with struct NewNode end would also work fine.","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"note: Note\nYou can use regular functions, e.g + as a node type. Their Julia type, however, is written with the typeof(_) specification, e.g. typeof(+)","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"For our node we are dealing with a stochastic node, because the node forms a probabilistic relationship. This means that for a given value of pi, we do know the corresponding value of the output, but we do have some belief about this. Deterministic nodes include for example linear and non-linear transformation.","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"The interfaces specify what variables are connected to the node. The first argument is its output by convention. The ordering is important for both the model specification as the rule definition. As an example consider the NormalMeanVariance factor node. This factor node has interfaces [out, μ, v] and can be called in the model specification language as x ~ NormalMeanVariance(μ, v). It is also possible to use aliases for the interfaces, which can be specified in a tuple as you will see below.","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Concluding, we can create the MyBernoulli factor node as","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"@node MyBernoulli Stochastic [out, (π, aliases = [p])]","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Cool! Step 1 is done, we have created a custom node.","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"","category":"page"},{"location":"manuals/custom-node/#Step-2:-Defining-rules-for-our-node","page":"Defining a custom node and rules","title":"Step 2: Defining rules for our node","text":"","category":"section"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"In order for RxInfer to perform probabilistic inference and compute posterior distributions, such as p(pimid x_1K), we need to tell it how to perform inference locally around our node. This localization is what makes RxInfer achieve high performance. In our message passing-based paradigm, we need to describe how the node processes incoming information in the form of messages (or marginals). Here we will highlight two different message passing strategies: sum-product message passing and variational message passing.","category":"page"},{"location":"manuals/custom-node/#Sum-product-message-passing-update-rules","page":"Defining a custom node and rules","title":"Sum-product message passing update rules","text":"","category":"section"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"In sum-product message passing we compute outgoing messages to our node as","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"vecmu(x) propto int mathrmBer(xmid pi) vecmu(pi) mathrmdx","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"overleftarrowmu(pi) propto sum_x in 01 mathrmBer(xmid pi) overleftarrowmu(x)","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"This integral does not always have nice tractable solutions. However, for some forms of the incoming messages, it does yield a tractable solution.","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"For the case of a Beta message coming into our node, the outgoing message will be the predictive posterior of the Bernoulli distribution with a Beta prior. Here we obtain pi = fracalphaalpha + beta, which coincides with the mean of the Beta distribution. Hence, we can write down the first update rule using the @rule macro as","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"@rule MyBernoulli(:out, Marginalisation) (m_π :: Beta,) = MyBernoulli(mean(m_π))","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Here, :out refers to the interface of the outgoing message. The second argument denotes the incoming messages (which can be typed) as a tuple. Therefore make sure that it has a trailing , when there is a single message coming in. m_π is shorthand for \"the incoming message on interface π\". As we will see later, the structured approximation update rule for incoming message from π will have q_π as parameter.","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"The second rule is also straightforward; if π is a PointMass and therefore fixed, the outgoing message will be MyBernoulli(π):","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"@rule MyBernoulli(:out, Marginalisation) (m_π :: PointMass,) = MyBernoulli(mean(m_π))","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Continuing with the sum-product update rules, we now have to define the update rules towards the π interface. We can only do exact inference if the incoming message is known, which in the case of the Bernoulli distribution, means that the out message is a PointMass distribution that is either 0 or 1. The updated Beta distribution for π will be:","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"overleftarrowmu(π) propto mathrmBeta(1 + x 2 - x)","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Which gives us the following update rule:","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"@rule MyBernoulli(:π, Marginalisation) (m_out :: PointMass,) = begin\n    p = mean(m_out)\n    Beta(one(p) + p, 2one(p) - p)\nend","category":"page"},{"location":"manuals/custom-node/#Variational-message-passing-update-rules","page":"Defining a custom node and rules","title":"Variational message passing update rules","text":"","category":"section"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"We will now cover our second set of update rules. The sum-product messages are not always tractable and therefore we may need to resort to approximations. Here we highlight the variational approximation. In variational message passing we compute outgoing messages to our node as","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"vecnu(x) propto exp int q(pi) ln mathrmBer(xmid pi) mathrmdx","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"overleftarrownu(pi) propto exp sum_x in 01 q(x) ln mathrmBer(xmid pi)","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"These messages depend on the marginals on the adjacent edges and not on the incoming messages as was the case with sum-product message passing. Update rules that operate on the marginals instead of the incoming messages are specified with the q_{interface} argument names. With these update rules, we can often support a wider family of distributions. Below we directly give the variational update rules. Deriving them yourself will be a nice challenge.","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"#rules towards out\n@rule MyBernoulli(:out, Marginalisation) (q_π :: PointMass,) = MyBernoulli(mean(q_π))\n\n@rule Bernoulli(:out, Marginalisation) (q_π::Any,) = begin\n    rho_1 = mean(log, q_π)          # E[ln(x)]\n    rho_2 = mean(mirrorlog, q_π)    # E[log(1-x)]\n    m = max(rho_1, rho_2)\n    tmp = exp(rho_1 - m)\n    p = clamp(tmp / (tmp + exp(rho_2 - m)), tiny, one(m))\n    return Bernoulli(p)\nend\n\n#rules towards π\n@rule MyBernoulli(:π, Marginalisation) (q_out :: Any,) = begin\n    p = mean(q_out)\n    return Beta(one(p) + p, 2one(p) - p)\nend","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"node: Node\nTypically, the type of the variational distributions q_ does not matter in the real computations, but only their statistics, e.g mean or var. Thus, in this case, we may safely use ::Any.","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"In the example that we will show later on, we solely use sum-product message passing. Variational message passing requires us to set the local constraints in our model, something which is out of scope of this tutorial.","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"","category":"page"},{"location":"manuals/custom-node/#Step-3:-Defining-joint-marginals-for-our-node","page":"Defining a custom node and rules","title":"Step 3: Defining joint marginals for our node","text":"","category":"section"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"The entire probabilistic model can be scored using the Bethe free energy, which bounds the log-evidence for acyclic graphs. This Bethe free energy consists out of the sum of node-local entropies, negative node-local average energies and edge specific entropies. Formally we can denote this by","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Fqf = - sum_ainmathcalV mathrmHq_a(s_a) - sum_ainmathcalVmathrmE_q_a(s_a)ln f_a(s_a) + sum_iinmathcalEmathrmHq_i(s_i)","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Here we call q_a(s_a) the joint marginals around a node and -mathrmE_q_a(s_a)ln f_a(s_a) we term the average energy.","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"In order to be able to compute the Bethe free energy, we need to first describe how to compute q_a(s_a), defined in our case as ","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"q(x_k pi) = vecmu(pi) overleftarrowmu(x_k) mathrmBer(x_k mid pi)","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"To calculate the updated posterior marginal for our custom distribution, we need to return joint posterior marginals for the interfaces of our node. In our case, the posterior marginal for the observation is still the same PointMass distribution. However, to calculate the posterior marginal over π, we use RxInfer's built-in prod functionality to multiply the Beta prior with the Beta likelihood. This gives us the updated posterior distribution, which is also a Beta distribution. We use ProdAnalytical() parameter to ensure that we multiply the two distributions analytically. This is done as follows:","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"@marginalrule MyBernoulli(:out_π) (m_out::PointMass, m_π::Beta) = begin\n    r = mean(m_out)\n    p = prod(ProdAnalytical(), Beta(one(r) + r, 2one(r) - r), m_π)\n    return (out = m_out, p = p)\nend","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"In this code :out_π describes the arguments of the joint marginal distribution. The second argument contains the incoming messages. Here we know from the model specification that we observe out and therefore this has to be a PointMass. Because it is a PointMass, the joint marginal automatically factorizes as q(x_k pi) = q(x_k)q(pi). These are the distributions that we return in a form of the NamedTuple. NamedTuple is used only in cases where we know that the joint marginal factorizes further, but typically it should be a full distribution. For computing q(pi) we need to compute the product vecmu(pi)overleftarrowmu(pi). We already know how overleftarrowmu(pi) looks like from the previous step, so we can just use the prod function.","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"","category":"page"},{"location":"manuals/custom-node/#Step-4:-Defining-the-average-energy-for-our-node","page":"Defining a custom node and rules","title":"Step 4: Defining the average energy for our node","text":"","category":"section"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"To complete the computation of the Bethe free energy, we also need to compute the average energy term. The average energy in our MyBernoulli example can be computed as -mathrmE_q(x_k pi)ln p(x_k mid pi), however, because we know that we observe x_k and therefore q(x_k pi) factorizes, we can instead compute beginaligned -mathrmE_q(x_k)q(pi)ln p(x_k mid pi) = -mathrmE_q(x_k)q(pi) ln (pi^x_k (1-pi)^1 - x_k) \n= -mathrmE_q(x_k)q(pi) x_k ln(pi) + (1-x_k) ln(1-pi) \n= -mathrmE_q(x_k)x_k mathrmE_q(pi) ln(pi) - (1-mathrmE_q(x_k)x_k) mathrmE_q(pi)ln(1-pi) endaligned","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Which is what we implemented below. Note that mean(mirrorlog, q(x)) is equal to mathrmE_q(x)1-logx.","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"@average_energy Bernoulli (q_out::Any, q_π::Any) = -mean(q_out) * mean(log, q_π) - (1.0 - mean(q_out)) * mean(mirrorlog, q_π)","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"In the case that the interfaces do not factorize, we would get something like @average_energy MyBernoulli (q_out_π) ....","category":"page"},{"location":"manuals/custom-node/#Using-our-node-in-a-model","page":"Defining a custom node and rules","title":"Using our node in a model","text":"","category":"section"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"With all the necessary functions defined, we can proceed to test our custom node in an experiment. For this experiment, we will generate a dataset from a Bernoulli distribution with a fixed success probability of 0.75. Next, we will define a probabilistic model that has a Beta prior and a MyBernoulli likelihood. The Beta prior will be used to model our prior belief about the probability of success. The MyBernoulli likelihood will be used to model the generative process of the observed data. We start by generating the dataset:","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"using Random\n\nrng = MersenneTwister(42)\nn = 500\nπ_real = 0.75\ndistribution = Bernoulli(π_real)\n\ndataset = float.(rand(rng, distribution, n))\n\nnothing # hide","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Next, we define our model. Note that we use the MyBernoulli node in the model. The model consists of a single latent variable π, which has a Beta prior and is the parameter of the MyBernoulli likelihood. The MyBernoulli node takes the value of π as its parameter and returns a binary observation. We set the hyperparameters of the Beta prior to be 4 and 8, respectively, which correspond to a distribution slightly biased towards higher values of π. The model is defined as follows:","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"@model function coin_model_mybernoulli(n)\n\n    # `datavar` creates data 'inputs' in our model\n    y = datavar(Float64, n)\n\n    # We endow θ parameter of our model with some prior\n    π ~ Beta(4.0, 8.0)\n\n    # We assume that outcome of each coin flip is governed by the MyBernoulli distribution\n    for i in 1:n\n        y[i] ~ MyBernoulli(π)\n    end\n\nend","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Finally, we can run inference with this model and the generated dataset:","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"result_mybernoulli = infer(\n    model = coin_model_mybernoulli(length(dataset)), \n    data  = (y = dataset, ),\n)","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"We have now completed our experiment and obtained the posterior marginal distribution for p through inference. To evaluate the performance of our inference, we can compare the estimated posterior to the true value. In our experiment, the true value for p is 0.75, and we can see that the estimated posterior has a mean of approximately 0.713, which shows that our custom node was able to successfully pass messages towards the π variable in order to learn the true value of the parameter.","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"using Plots\n\nrθ = range(0, 1, length = 1000)\n\np = plot(title = \"Inference results\")\n\nplot!(rθ, (x) -> pdf(result_mybernoulli.posteriors[:π], x), fillalpha=0.3, fillrange = 0, label=\"p(π|x)\", c=3)\nvline!([π_real], label=\"Real π\")","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"As a sanity check, we can create the same model with the RxInfer built-in node Bernoulli and compare the resulting posterior distribution with the one obtained using our custom MyBernoulli node. This will give us confidence that our custom node is working correctly. We use the Bernoulli node with the same Beta prior and the observed data, and then run inference. We can compare the two posterior distributions and observe that they are exactly the same, which indicates that our custom node is performing as expected.","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"@model function coin_model(n)\n    \n    y = datavar(Float64, n)\n    p ~ Beta(4.0, 8.0)\n\n    for i in 1:n\n        y[i] ~ Bernoulli(p)\n    end\n\nend\n\nresult_bernoulli = infer(\n    model = coin_model(length(dataset)), \n    data  = (y = dataset, ),\n)\n\nif !(result_bernoulli.posteriors[:p] == result_mybernoulli.posteriors[:π])\n    error(\"Results are not identical\")\nelse \n    println(\"Results are identical 🎉🎉🎉\")\nend\n\nnothing # hide","category":"page"},{"location":"manuals/custom-node/","page":"Defining a custom node and rules","title":"Defining a custom node and rules","text":"Congratulations! You have successfully implemented your own custom node in RxInfer. We went through the definition of a node to the implementation of the update rules and marginal posterior calculations. Finally we tested our custom node in a model and checked if we implemented everything correctly.","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/#examples-simple-nonlinear-node","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"","category":"section"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"using RxInfer, Random, StableRNGs","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"Here is an example of creating custom node with nonlinear function approximation with samplelist.","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/#Custom-node-creation","page":"Simple Nonlinear Node","title":"Custom node creation","text":"","category":"section"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"struct NonlinearNode end # Dummy structure just to make Julia happy\n\nstruct NonlinearMeta{R, F}\n    rng      :: R\n    fn       :: F   # Nonlinear function, we assume 1 float input - 1 float output\n    nsamples :: Int # Number of samples used in approximation\nend","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"@node NonlinearNode Deterministic [ out, in ]","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"We need to define two Sum-product message computation rules for our new custom node","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"Rule for outbound message on out edge given inbound message on in edge\nRule for outbound message on in edge given inbound message on out edge\nBoth rules accept optional meta object","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"# Rule for outbound message on `out` edge given inbound message on `in` edge\n@rule NonlinearNode(:out, Marginalisation) (m_in::NormalMeanVariance, meta::NonlinearMeta) = begin \n    samples = rand(meta.rng, m_in, meta.nsamples)\n    return SampleList(map(meta.fn, samples))\nend","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"# Rule for outbound message on `in` edge given inbound message on `out` edge\n@rule NonlinearNode(:in, Marginalisation) (m_out::Gamma, meta::NonlinearMeta) = begin     \n    return ContinuousUnivariateLogPdf((x) -> logpdf(m_out, meta.fn(x)))\nend","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/#Model-specification","page":"Simple Nonlinear Node","title":"Model specification","text":"","category":"section"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"After we have defined our custom node with custom rules we may proceed with a model specification:","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"beginaligned\np(theta) = mathcalN(thetamu_theta sigma_theta)\np(m) = mathcalN(thetamu_m sigma_m)\np(w) = f(theta)\np(y_im w) = mathcalN(y_im w)\nendaligned","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"Given this IID model, we aim to estimate the precision of a Gaussian distribution. We pass a random variable theta through a non-linear transformation f to make it positive and suitable for a precision parameter of a Gaussian distribution. We, later on, will estimate the posterior of theta. ","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"@model function nonlinear_estimation(n)\n    \n    θ ~ Normal(mean = 0.0, variance = 100.0)\n    m ~ Normal(mean = 0.0, variance = 1.0)\n    \n    w ~ NonlinearNode(θ)\n    \n    y = datavar(Float64, n)\n    \n    for i in 1:n\n        y[i] ~ Normal(mean = m, precision = w)\n    end\n    \nend","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"@constraints function nconstsraints(nsamples)\n    q(θ) :: SampleList(nsamples, LeftProposal())\n    q(w) :: SampleList(nsamples, RightProposal())\n    \n    q(θ, w, m) = q(θ)q(m)q(w)\nend","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"nconstsraints (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"@meta function nmeta(fn, nsamples)\n    NonlinearNode(θ, w) -> NonlinearMeta(StableRNG(123), fn, nsamples)\nend","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"nmeta (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"Here we generate some data","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"nonlinear_fn(x) = abs(exp(x) * sin(x))","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"nonlinear_fn (generic function with 1 method)","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"seed = 123\nrng  = StableRNG(seed)\n\nniters   = 15 # Number of VMP iterations\nnsamples = 5_000 # Number of samples in approximation\n\nn = 500 # Number of IID samples\nμ = -10.0\nθ = -1.0\nw = nonlinear_fn(θ)\n\ndata = rand(rng, NormalMeanPrecision(μ, w), n);","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"result = infer(\n    model = nonlinear_estimation(n),\n    meta =  nmeta(nonlinear_fn, nsamples),\n    constraints = nconstsraints(nsamples),\n    data = (y = data, ), \n    initmarginals = (m = vague(NormalMeanPrecision), w = vague(Gamma)),\n    returnvars = (θ = KeepLast(), ),\n    iterations = niters,  \n    showprogress = true\n)","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"Inference results:\n  Posteriors       | available for (θ)","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"θposterior = result.posteriors[:θ]","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"SampleList(Univariate, 5000)","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"using Plots, StatsPlots\n\nestimated = Normal(mean_std(θposterior)...)\n\nplot(estimated, title=\"Posterior for θ\", label = \"Estimated\", legend = :bottomright, fill = true, fillopacity = 0.2, xlim = (-3, 3), ylim = (0, 2))\nvline!([ θ ], label = \"Real value of θ\")","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Simple Nonlinear Node/","page":"Simple Nonlinear Node","title":"Simple Nonlinear Node","text":"","category":"page"},{"location":"library/exported-methods/#lib-using-methods","page":"Exported methods","title":"Using methods from RxInfer","text":"","category":"section"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"In the Julia programming language (in contrast to Python for example) the most common way of loading a module is:","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"using RxInfer","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"A nice explanation about how modules/packages work in Julia can be found in the official documentation.","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"In a nutshell, Julia automatically resolves all name collisions and there is no a lot of benefit of importing specific names, e.g.:","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"import RxInfer: mean","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"One of the reasons for that is that Julia uses multiple-dispatch capabilities to merge names automatically and will indicate (with a warning) if something went wrong or names have unresolvable collisions on types. As a small example of this feature consider the following small import example:","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"import RxInfer: mean as mean_from_rxinfer\nimport Distributions: mean as mean_from_distributions\n\nmean_from_rxinfer === mean_from_distributions","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"Even though we import mean function from two different packages they actually refer to the same object. Worth noting that this is not always the case - Julia will print a warning in case it finds unresolvable conflicts and usage of such functions will be disallowed unless user import them specifically. Read more about this in the section of the Julia's documentation.","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"# It is easier to let Julia resolve names automatically\n# Julia will not overwrite `mean` that is coming from both packages\nusing RxInfer, Distributions ","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"mean(Normal(0.0, 1.0)) # `Normal` is an object from `Distributions.jl`","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"mean(NormalMeanVariance(0.0, 1.0)) # `NormalMeanVariance` is an object from `RxInfer.jl`","category":"page"},{"location":"library/exported-methods/#lib-list-methods","page":"Exported methods","title":"List of available methods","text":"","category":"section"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"Below you can find a list of exported methods from RxInfer.jl. All methods (even private) can be always accessed with RxInfer. prefix, e.g RxInfer.mean.","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"note: Note\nSome exported names are (for legacy reasons) intended for private usage only. As a result some of these methods do not have a proper associated documentation with them. We constantly improve RxInfer.jl library and continue to add better documentation for many exported methods, but a small portion of these methods could be removed from this list in the future.","category":"page"},{"location":"library/exported-methods/","page":"Exported methods","title":"Exported methods","text":"using RxInfer #hide\nforeach(println, names(RxInfer))","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/#examples-nonlinear-sensor-fusion","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"","category":"section"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"using RxInfer, Random, LinearAlgebra, Distributions, Plots, StatsPlots, Optimisers\nusing DataFrames, DelimitedFiles, StableRNGs","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"In a secret ongoing mission to Mars, NASA has deployed its custom lunar roving vehicle, called WALL-E, to explore the area and to discover hidden minerals. During one of the solar storm, WALL-E's GPS unit got damaged, preventing it from accurately locating itself. The engineers at NASA were devastated as they developed the project over the past couple of years and spend most of their funding on it. Without being able to locate WALL-E, they were unable to complete their mission.","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"A smart group of engineers came up with a solution to locate WALL-E. They decided to repurpose 3 nearby satelites as beacons for WALL-E, allowing it to detect its relative location to these beacons. However, these satelites were old and therefore WALL-E was only able to obtain noisy estimates of its distance to these beacons. These distances were communicated back to earth, where the engineers tried to figure our WALL-E's location. Luckily they knew the locations of these satelites and together with the noisy estimates of the distance to WALL-E they can infer the exact location of the moving WALL-E.","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"To illustrate these noisy measurements, the engineers decided to plot them:","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"# fetch measurements\nbeacon_locations = readdlm(\"../data/sensor_fusion/beacons.txt\")\ndistances = readdlm(\"../data/sensor_fusion/distances.txt\")\nposition = readdlm(\"../data/sensor_fusion/position.txt\")\nnr_observations = size(distances, 1);","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"# plot beacon and actual location of WALL-E\np1 = scatter(beacon_locations[:,1], beacon_locations[:,2], markershape=:utriangle, markersize=10, legend=:topleft, label=\"beacon locations\")\nplot!(position[1,:], position[2,:], label=\"actual location\", linewidth=3, linestyle=:dash, arrow=(:closed, 2.0), aspect_ratio=1.0)\nxlabel!(\"longitude [m]\"), ylabel!(\"latitude [m]\")\n\n# plot noisy distance measurements\np2 = plot(distances, legend=:topleft, linewidth=3, label=[\"distance to beacon 1\" \"distance to beacon 2\" \"distance to beacon 3\"])\nxlabel!(\"time [sec]\"), ylabel!(\"distance [m]\")\n\nplot(p1, p2, size=(1200, 500))","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"In order to track the location of WALL-E based on the noisy distance measurements to the beacon, the engineers developed a probabilistic model for the movements for WALL-E and the distance measurements that followed from this. The engineers assumed that the position of WALL-E at time t, denoted by z_t, follows a 2-dimensional normal random walk:","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"beginaligned\n  p(z_t mid z_t - 1) = mathcalN(z_t mid z_t-1mathrmI_2)\nendaligned","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"where mathrmI_2 denotes the 2-dimensional identity matrix. From the current position of WALL-E, we specify our noisy distance measurements y_t as a noisy set of the distances between WALL-E and the beacons, specified by s_i:","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"beginaligned\n  p(y_t mid z_t)  = mathcalN left (y_t left vert beginbmatrix  z_t - s_1  z_t - s_2  z_t - s_3endbmatrixmathrmI_3 right  right)\nendaligned","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"The engineers are smart enough to automate the probabilistic inference procedure using RxInfer.jl. They specify the probabilistic model as:","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"# function to compute distance to beacons\nfunction compute_distances(z)    \n    distance1 = norm(z - beacon_locations[1,:])\n    distance2 = norm(z - beacon_locations[2,:])\n    distance3 = norm(z - beacon_locations[3,:])\n    distances = [distance1, distance2, distance3]\nend;","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"@model function random_walk_model(nr_observations)\n    \n    # specify precision matrices\n    W = diageye(2)\n    R = diageye(3)\n\n    # allocate locations and observations\n    z = randomvar(nr_observations)\n    y = datavar(Vector{Float64}, nr_observations)\n\n    # specify initial estimates of the location\n    z[1] ~ MvNormalMeanCovariance(zeros(2), diageye(2)) \n    y[1] ~ MvNormalMeanCovariance(compute_distances(z[1]), diageye(3))\n\n    # loop over time steps\n    for t in 2:nr_observations\n\n        # specify random walk state transition model\n        z[t] ~ MvNormalMeanPrecision(z[t-1], W)\n\n        # specify non-linear distance observations model\n        y[t] ~ MvNormalMeanPrecision(compute_distances(z[t]), R)\n        \n    end\n\nend;","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"Due to non-linearity, exact probabilistic inference is intractable in this model. Therefore we resort to Conjugate-Computational Variational Inference (CVI) following the paper Probabilistic programming with stochastic variational message passing. This requires setting the @meta macro in RxInfer.jl.","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"Please note that we permit improper messages within the CVI procedure in this example by providing Val(false) to CVI constructor:","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"    compute_distances(z) -> CVI(..., Val(false), ...)","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"This move may lead to numerical instabilities in other scenarios, however dissallowing improper messages in this case can lead to a biased estimates of posterior distribution. So, as a rule of thumb, you should try the default setting, and if it fails to find an unbiased result, enable improper messages.","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"@meta function random_walk_model_meta(nr_samples, nr_iterations, rng)\n    compute_distances(z) -> CVI(rng, nr_samples, nr_iterations, Optimisers.Descent(0.1), ForwardDiffGrad(), 1, Val(false), false)\nend;","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"NOTE: You can try out different meta for approximating the nonlinearity, e.g.","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"@meta function random_walk_linear_meta()\n    compute_distances(z) -> Linearization()\nend;","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"@meta function random_walk_unscented_meta()\n    compute_distances(z) -> Unscented()\nend;","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"results_fast = infer(\n    model = random_walk_model(nr_observations),\n    meta = random_walk_model_meta(1, 3, StableRNG(42)), # or random_walk_unscented_meta()\n    data = (y = [distances[t,:] for t in 1:nr_observations],),\n    iterations = 100,\n    free_energy = false,\n    returnvars = (z = KeepLast(),),\n    initmessages = (z = MvNormalMeanPrecision(ones(2), 0.1 * diageye(2)),),\n);","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"results_accuracy = infer(\n    model = random_walk_model(nr_observations),\n    meta = random_walk_model_meta(1000, 100, StableRNG(42)),\n    data = (y = [distances[t,:] for t in 1:nr_observations],),\n    iterations = 100,\n    free_energy = false,\n    returnvars = (z = KeepLast(),),\n    initmessages = (z = MvNormalMeanPrecision(zeros(2), 0.1 * diageye(2)),),\n);","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"After running this fast inference procedure, the engineers plot the results and evaluate the performance:","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"# plot beacon and actual and estimated location of WALL-E (fast inference)\np1 = scatter(beacon_locations[:,1], beacon_locations[:,2], markershape=:utriangle, markersize=10, legend=:topleft, label=\"beacon locations\")\nplot!(position[1,:], position[2,:], label=\"actual location\", linewidth=3, linestyle=:dash, arrow=(:closed, 2.0), aspect_ratio=1.0)\nmap(posterior -> covellipse!(mean(posterior), cov(posterior), color=\"red\", label=\"\", n_std=2), results_fast.posteriors[:z])\nxlabel!(\"longitude [m]\"), ylabel!(\"latitude [m]\"), title!(\"Fast (1 sample, 3 iterations)\"); p1.series_list[end][:label] = \"estimated location ±2σ\"\n\n# plot beacon and actual and estimated location of WALL-E (accurate inference)\np2 = scatter(beacon_locations[:,1], beacon_locations[:,2], markershape=:utriangle, markersize=10, legend=:topleft, label=\"beacon locations\")\nplot!(position[1,:], position[2,:], label=\"actual location\", linewidth=3, linestyle=:dash, arrow=(:closed, 2.0), aspect_ratio=1.0)\nmap(posterior -> covellipse!(mean(posterior), cov(posterior), color=\"red\", label=\"\", n_std=2), results_accuracy.posteriors[:z])\nxlabel!(\"longitude [m]\"), ylabel!(\"latitude [m]\"), title!(\"Accurate (1000 samples, 100 iterations)\"); p2.series_list[end][:label] = \"estimated location ±2σ\"\n\nplot(p1, p2, size=(1200, 500))","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"The engineers were very happy with the solution, as it meant that the Mars mission could continue. However, they noted that the estimates began to deviate after WALL-E moved further away from the beacons. They deemed this was likely due to the noise in the distance measurements. Therefore, the engineers decided to adapt the model, such that they would also infer the process and observation noise precision matrices, Q and R respectively. They did this by adding Wishart priors to those matrices:","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"beginaligned\n  p(Q) = mathcalW(Q mid 3 mathrmI_2) \n  p(R) = mathcalW(R mid 4 mathrmI_3) \n  p(z_t mid z_t - 1 Q) = mathcalN(z_t mid z_t-1 Q^-1)\n  p(y_t mid z_t R)  = mathcalN left (y_t left vert beginbmatrix  z_t - s_1  z_t - s_2  z_t - s_3endbmatrixR^-1 right  right)\nendaligned","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"@model function random_walk_model_wishart(nr_observations)\n\n    # allocate locations and observations\n    z = randomvar(nr_observations)\n    y = datavar(Vector{Float64}, nr_observations)\n\n    # set priors on precision matrices\n    Q ~ Wishart(3, diageye(2))\n    R ~ Wishart(4, diageye(3))\n\n    # specify initial estimates of the location\n    z[1] ~ MvNormalMeanCovariance(zeros(2), diageye(2)) \n    y[1] ~ MvNormalMeanCovariance(compute_distances(z[1]), diageye(3))\n\n    # loop over time steps\n    for t in 2:nr_observations\n\n        # specify random walk state transition model\n        z[t] ~ MvNormalMeanPrecision(z[t-1], Q)\n\n        # specify non-linear distance observations model\n        y[t] ~ MvNormalMeanPrecision(compute_distances(z[t]), R)\n        \n    end\n\nend;","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"meta = @meta begin \n    compute_distances(z) -> CVI(StableRNG(42), 1000, 100, Optimisers.Descent(0.01), ForwardDiffGrad(), 1, Val(false), false)\nend;","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"Because of the added complexity with the Wishart distributions, the engineers simplify the problem by employing a structured mean-field factorization:","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"constraints = @constraints begin\n    q(z, Q, R) = q(z)q(Q)q(R)\nend;","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"The engineers run the inference procedure again and decide to track the inference performance using the Bethe free energy.","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"results_wishart = infer(\n    model = random_walk_model_wishart(nr_observations),\n    data = (y = [distances[t,:] for t in 1:nr_observations],),\n    iterations = 100,\n    free_energy = true,\n    returnvars = (z = KeepLast(),),\n    constraints = constraints,\n    meta = meta,\n    initmessages = (z = MvNormalMeanPrecision(zeros(2), 0.01 * diageye(2)),),\n    initmarginals = (R = Wishart(4, diageye(3)), Q = Wishart(3, diageye(2)))\n);","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"They plot the new estimates and the performance over time, and luckily WALL-E is found!","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"# plot beacon and actual and estimated location of WALL-E (fast inference)\np1 = scatter(beacon_locations[:,1], beacon_locations[:,2], markershape=:utriangle, markersize=10, legend=:topleft, label=\"beacon locations\")\nplot!(position[1,:], position[2,:], label=\"actual location\", linewidth=3, linestyle=:dash, arrow=(:closed, 2.0), aspect_ratio=1.0)\nmap(posterior -> covellipse!(mean(posterior), cov(posterior), color=\"red\", label=\"\", n_std=2), results_wishart.posteriors[:z])\nxlabel!(\"longitude [m]\"), ylabel!(\"latitude [m]\"); p1.series_list[end][:label] = \"estimated location ±2σ\"\n\n# plot bethe free energy performance\np2 = plot(results_wishart.free_energy, label = \"\")\nxlabel!(\"iteration\"), ylabel!(\"Bethe free energy [nats]\")\n\nplot(p1, p2, size=(1200, 500))","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Nonlinear Sensor Fusion/","page":"Nonlinear Sensor Fusion","title":"Nonlinear Sensor Fusion","text":"","category":"page"},{"location":"examples/problem_specific/overview/#examples-problem_specific-overview","page":"Overview","title":"Problem specific","text":"","category":"section"},{"location":"examples/problem_specific/overview/","page":"Overview","title":"Overview","text":"This section contains a set of examples for Bayesian Inference with RxInfer package in various probabilistic models.","category":"page"},{"location":"examples/problem_specific/overview/","page":"Overview","title":"Overview","text":"note: Note\nAll examples have been pre-generated automatically from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/problem_specific/overview/","page":"Overview","title":"Overview","text":"Problem specific examples contain specialized models and inference for various domains.","category":"page"},{"location":"examples/problem_specific/overview/","page":"Overview","title":"Overview","text":"Autoregressive Models: An example of Bayesian treatment of latent AR and ARMA models. Reference: Albert Podusenko, Message Passing-Based Inference for Time-Varying Autoregressive Models.\nGamma Mixture Model: This example implements one of the Gamma mixture experiments outlined in https://biaslab.github.io/publication/mp-based-inference-in-gmm/ .\nGaussian Mixture: This example implements variational Bayesian inference in univariate and multivariate Gaussian mixture models with mean-field assumption.\nHierarchical Gaussian Filter: An example of online inference procedure for Hierarchical Gaussian Filter with univariate noisy observations using Variational Message Passing algorithm. Reference: Ismail Senoz, Online Message Passing-based Inference in the Hierarchical Gaussian Filter.\nInvertible neural networks: a tutorial: An example of variational Bayesian Inference with invertible neural networks. Reference: Bart van Erp, Hybrid Inference with Invertible Neural Networks in Factor Graphs.\nProbit Model (EP): In this demo we illustrate EP in the context of state-estimation in a linear state-space model that combines a Gaussian state-evolution model with a discrete observation model.\nRTS vs BIFM Smoothing: This example performs BIFM Kalman smoother on a factor graph using message passing and compares it with the RTS implementation.\nSimple Nonlinear Node: In this example we create a non-conjugate model and use a nonlinear link function between variables. We show how to extend the functionality of RxInfer and to create a custom factor node with arbitrary message passing update rules.\nUniversal Mixtures: Universal mixture modeling.","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/#examples-coin-toss-model-(beta-bernoulli)","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"","category":"section"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"In this example, we are going to perform an exact inference for a coin toss model that can be represented as:","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"beginaligned\np(theta) = mathrmBeta(thetaa b)\np(y_itheta) = mathrmBer(y_itheta)\nendaligned","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"where y_i in 0 1 is a binary observation induced by Bernoulli likelihood while theta is a Beta prior distribution on the parameter of Bernoulli. We are interested in inferring the posterior distribution of theta.","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"We start with importing all needed packages:","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"using RxInfer, Random","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"Let's generate some synthetic dataset with IID observations from Bernoulli distribution, that represents our coin tosses. We also assume that our coin is biased:","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"rng = MersenneTwister(42)\nn = 500\nθ_real = 0.75\ndistribution = Bernoulli(θ_real)\n\ndataset = float.(rand(rng, Bernoulli(θ_real), n));","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"# GraphPPL.jl export `@model` macro for model specification\n# It accepts a regular Julia function and builds an FFG under the hood\n@model function coin_model(n)\n\n    # `datavar` creates data 'inputs' in our model\n    # We will pass data later on to these inputs\n    # In this example we create a sequence of inputs that accepts Float64\n    y = datavar(Float64, n)\n\n    # We endow θ parameter of our model with some prior\n    θ ~ Beta(4.0, 8.0)\n    # or, in this particular case, the `Uniform(0.0, 1.0)` prior also works:\n    # θ ~ Uniform(0.0, 1.0)\n\n    # We assume that outcome of each coin flip is governed by the Bernoulli distribution\n    for i in 1:n\n        y[i] ~ Bernoulli(θ)\n    end\n\nend","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"result = infer(\n    model = coin_model(length(dataset)), \n    data  = (y = dataset, )\n)","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"Inference results:\n  Posteriors       | available for (θ)","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"θestimated = result.posteriors[:θ]","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"Beta{Float64}(α=365.0, β=147.0)","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"using Plots\n\nrθ = range(0, 1, length = 1000)\n\np = plot(title = \"Inference results\")\n\nplot!(rθ, (x) -> pdf(Beta(2.0, 7.0), x), fillalpha=0.3, fillrange = 0, label=\"P(θ)\", c=1,)\nplot!(rθ, (x) -> pdf(θestimated, x), fillalpha=0.3, fillrange = 0, label=\"P(θ|y)\", c=3)\nvline!([θ_real], label=\"Real θ\")","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"(Image: )","category":"page"},{"location":"examples/basic_examples/Coin Toss Model/","page":"Coin toss model (Beta-Bernoulli)","title":"Coin toss model (Beta-Bernoulli)","text":"","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/#examples-invertible-neural-networks:-a-tutorial","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"","category":"section"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"# Activate local environment, see `Project.toml`\nimport Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Table of contents","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Introduction\nModel specification\nModel compilation\nProbabilistic inference\nParameter estimation","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/#Introduction","page":"Invertible neural networks: a tutorial","title":"Introduction","text":"","category":"section"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/#Load-required-packages","page":"Invertible neural networks: a tutorial","title":"Load required packages","text":"","category":"section"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Before we can start, we need to import some packages:","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"using RxInfer\nusing Random\nusing StableRNGs\n\nusing ReactiveMP        # ReactiveMP is included in RxInfer, but we explicitly use some of its functionality\nusing LinearAlgebra     # only used for some matrix specifics\nusing Plots             # only used for visualisation\nusing Distributions     # only used for sampling from multivariate distributions\nusing Optim             # only used for parameter optimisation","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/#Model-specification","page":"Invertible neural networks: a tutorial","title":"Model specification","text":"","category":"section"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Specifying an invertible neural network model is easy. The general recipe looks like follows: model = FlowModel(input_dim, (layer1(options), layer2(options), ...)). Here the first argument corresponds to the input dimension of the model and the second argument is a tuple of layers. An example model can be defined as ","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"model = FlowModel(2,\n    (\n        AdditiveCouplingLayer(PlanarFlow()),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false)\n    )\n);","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Alternatively, the input_dim can also be passed as an InputLayer layer as ","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"model = FlowModel(\n    (\n        InputLayer(2),\n        AdditiveCouplingLayer(PlanarFlow()),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false)\n    )\n);","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"In the above AdditiveCouplingLayer layers the input bfx = x_1 x_2 ldots x_N is partitioned into chunks of unit length. These partitions are additively coupled to an output bfy = y_1 y_2 ldots y_N as ","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"beginaligned\n    y_1 = x_1 \n    y_2 = x_2 + f_1(x_1) \n    vdots \n    y_N = x_N + f_N-1(x_N-1)\nendaligned","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Importantly, this structure can easily be converted as ","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"beginaligned\n    x_1 = y_1 \n    x_2 = y_2 - f_1(x_1) \n    vdots \n    x_N = y_N - f_N-1(x_N-1)\nendaligned","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"f_n","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"is an arbitrarily complex function, here chosen to be a PlanarFlow, but this can be interchanged for any function or neural network. The permute keyword argument (which defaults to true) specifies whether the output of this layer should be randomly permuted or shuffled. This makes sure that the first element is also transformed in consecutive layers.","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"A permutation layer can also be added by itself as a PermutationLayer layer with a custom permutation matrix if desired.","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"model = FlowModel(\n    (\n        InputLayer(2),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false),\n        PermutationLayer(PermutationMatrix(2)),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false)\n    )\n);","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/#Model-compilation","page":"Invertible neural networks: a tutorial","title":"Model compilation","text":"","category":"section"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"In the current models, the layers are setup to work with the passed input dimension. This means that the function f_n is repeated input_dim-1 times for each of the partitions. Furthermore the permutation layers are set up with proper permutation matrices. If we print the model we get","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"model","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"FlowModel{3, Tuple{ReactiveMP.AdditiveCouplingLayerEmpty{Tuple{ReactiveMP.P\nlanarFlowEmpty{1}}}, PermutationLayer{Int64}, ReactiveMP.AdditiveCouplingLa\nyerEmpty{Tuple{ReactiveMP.PlanarFlowEmpty{1}}}}}(2, (ReactiveMP.AdditiveCou\nplingLayerEmpty{Tuple{ReactiveMP.PlanarFlowEmpty{1}}}(2, (ReactiveMP.Planar\nFlowEmpty{1}(),), 1), PermutationLayer{Int64}(2, [0 1; 1 0]), ReactiveMP.Ad\nditiveCouplingLayerEmpty{Tuple{ReactiveMP.PlanarFlowEmpty{1}}}(2, (Reactive\nMP.PlanarFlowEmpty{1}(),), 1)))","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"The text below describes the terms above. Please note the distinction in typing and elements, i.e. FlowModel{types}(elements):","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"FlowModel - specifies that we are dealing with a flow model.\n3 - Number of layers.\nTuple{AdditiveCouplingLayerEmpty{...},PermutationLayer{Int64},AdditiveCouplingLayerEmpty{...}} - tuple of layer types.\nTuple{ReactiveMP.PlanarFlowEmpty{1},ReactiveMP.PlanarFlowEmpty{1}} - tuple of functions f_n.\nPermutationLayer{Int64}(2, [0 1; 1 0]) - permutation layer with input dimension 2 and permutation matrix [0 1; 1 0].","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"From inspection we can see that the AdditiveCouplingLayerEmpty and PlanarFlowEmpty objects are different than before. They are initialized for the correct dimension, but they do not have any parameters registered to them. This is by design to allow for separating the model specification from potential optimization procedures. Before we perform inference in this model, the parameters should be initialized. We can randomly initialize the parameters as","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"compiled_model = compile(model)","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"CompiledFlowModel{3, Tuple{AdditiveCouplingLayer{Tuple{PlanarFlow{Float64, \nFloat64}}}, PermutationLayer{Int64}, AdditiveCouplingLayer{Tuple{PlanarFlow\n{Float64, Float64}}}}}(2, (AdditiveCouplingLayer{Tuple{PlanarFlow{Float64, \nFloat64}}}(2, (PlanarFlow{Float64, Float64}(0.24357081234214367, -0.7202599\n849473186, 0.7012262639170591),), 1), PermutationLayer{Int64}(2, [0 1; 1 0]\n), AdditiveCouplingLayer{Tuple{PlanarFlow{Float64, Float64}}}(2, (PlanarFlo\nw{Float64, Float64}(-0.0928216039987209, -2.329353058120145, 0.570152767728\n4544),), 1)))","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Now we can see that random parameters have been assigned to the individual functions inside of our model. Alternatively if we would like to pass our own parameters, then this is also possible. You can easily find the required number of parameters using the nr_params(model) function.","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"compiled_model = compile(model, randn(StableRNG(321), nr_params(model)))","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"CompiledFlowModel{3, Tuple{AdditiveCouplingLayer{Tuple{PlanarFlow{Float64, \nFloat64}}}, PermutationLayer{Int64}, AdditiveCouplingLayer{Tuple{PlanarFlow\n{Float64, Float64}}}}}(2, (AdditiveCouplingLayer{Tuple{PlanarFlow{Float64, \nFloat64}}}(2, (PlanarFlow{Float64, Float64}(0.7296412319250487, -0.97673361\n28037319, -0.4749869451771002),), 1), PermutationLayer{Int64}(2, [0 1; 1 0]\n), AdditiveCouplingLayer{Tuple{PlanarFlow{Float64, Float64}}}(2, (PlanarFlo\nw{Float64, Float64}(0.3490911082645933, -0.8184067956921087, -1.45782147323\n52386),), 1)))","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/#Probabilistic-inference","page":"Invertible neural networks: a tutorial","title":"Probabilistic inference","text":"","category":"section"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"We can perform inference in our compiled model through standard usage of RxInfer and its underlying ReactiveMP inference engine. Let's first generate some random 2D data which has been sampled from a standard normal distribution and is consecutively passed through an invertible neural network. Using the forward(model, data) function we can propagate data in the forward direction.","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"function generate_data(nr_samples::Int64, model::CompiledFlowModel; seed = 123)\n\n    rng = StableRNG(seed)\n    \n    # specify latent sampling distribution\n    dist = MvNormal([1.5, 0.5], I)\n\n    # sample from the latent distribution\n    x = rand(rng, dist, nr_samples)\n\n    # transform data\n    y = zeros(Float64, size(x))\n    for k = 1:nr_samples\n        y[:,k] .= ReactiveMP.forward(model, x[:,k])\n    end\n\n    # return data\n    return y, x\n\nend;","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"# generate data\ny, x = generate_data(1000, compiled_model)\n\n# plot generated data\np1 = scatter(x[1,:], x[2,:], alpha=0.3, title=\"Original data\", size=(800,400))\np2 = scatter(y[1,:], y[2,:], alpha=0.3, title=\"Transformed data\", size=(800,400))\nplot(p1, p2, legend = false)","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"The probabilistic model for doing inference can be described as ","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"@model function invertible_neural_network(nr_samples::Int64)\n    \n    # initialize variables\n    z_μ   = randomvar()\n    z_Λ   = randomvar()\n    x     = randomvar(nr_samples)\n    y_lat = randomvar(nr_samples)\n    y     = datavar(Vector{Float64}, nr_samples)\n\n    # specify prior\n    z_μ ~ MvNormalMeanCovariance(zeros(2), huge*diagm(ones(2)))\n    z_Λ ~ Wishart(2.0, tiny*diagm(ones(2)))\n\n    # specify observations\n    for k = 1:nr_samples\n\n        # specify latent state\n        x[k] ~ MvNormalMeanPrecision(z_μ, z_Λ)\n\n        # specify transformed latent value\n        y_lat[k] ~ Flow(x[k])\n\n        # specify observations\n        y[k] ~ MvNormalMeanCovariance(y_lat[k], tiny*diagm(ones(2)))\n\n    end\n\n    # return variables\n    return z_μ, z_Λ, x, y_lat, y\n\nend;","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Here the model is passed inside a meta data object of the flow node. Inference then resorts to","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"observations = [y[:,k] for k=1:size(y,2)]\n\nfmodel         = invertible_neural_network(length(observations))\ndata          = (y = observations, )\ninitmarginals = (z_μ = MvNormalMeanCovariance(zeros(2), huge*diagm(ones(2))), z_Λ = Wishart(2.0, tiny*diagm(ones(2))))\nreturnvars    = (z_μ = KeepLast(), z_Λ = KeepLast(), x = KeepLast(), y_lat = KeepLast())\n\nconstraints = @constraints begin\n    q(z_μ, x, z_Λ) = q(z_μ)q(z_Λ)q(x)\nend\n\n@meta function fmeta(model)\n    compiled_model = compile(model, randn(StableRNG(321), nr_params(model)))\n    Flow(y_lat, x) -> FlowMeta(compiled_model) # defaults to FlowMeta(compiled_model; approximation=Linearization()). \n                                               # other approximation methods can be e.g. FlowMeta(compiled_model; approximation=Unscented(input_dim))\nend\n\n# First execution is slow due to Julia's initial compilation \nresult = infer(\n    model = fmodel, \n    data  = data,\n    constraints   = constraints,\n    meta          = fmeta(model),\n    initmarginals = initmarginals,\n    returnvars    = returnvars,\n    free_energy   = true,\n    iterations    = 10, \n    showprogress  = false\n)","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Inference results:\n  Posteriors       | available for (z_μ, z_Λ, y_lat, x)\n  Free Energy:     | Real[29485.3, 23762.9, 23570.6, 23570.6, 23570.6, 2357\n0.6, 23570.6, 23570.6, 23570.6, 23570.6]","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"fe_flow = result.free_energy\nzμ_flow = result.posteriors[:z_μ]\nzΛ_flow = result.posteriors[:z_Λ]\nx_flow  = result.posteriors[:x]\ny_flow  = result.posteriors[:y_lat];","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"As we can see, the variational free energy decreases inside of our model.","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"plot(1:10, fe_flow/size(y,2), xlabel=\"iteration\", ylabel=\"normalized variational free energy [nats/sample]\", legend=false)","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"If we plot a random noisy observation and its approximated transformed uncertainty we obtain:","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"# pick a random observation\nid = rand(StableRNG(321), 1:size(y,2))\nrand_observation = MvNormal(y[:,id], 5e-1*diagm(ones(2)))\nwarped_observation = MvNormal(ReactiveMP.backward(compiled_model, y[:,id]), ReactiveMP.inv_jacobian(compiled_model, y[:,id])*5e-1*diagm(ones(2))*ReactiveMP.inv_jacobian(compiled_model, y[:,id])');\n\np1 = scatter(x[1,:], x[2,:], alpha=0.1, title=\"Latent distribution\", size=(1200,500), label=\"generated data\")\ncontour!(-5:0.1:5, -5:0.1:5, (x, y) -> pdf(MvNormal([1.5, 0.5], I), [x, y]), c=:viridis, colorbar=false, linewidth=2)\nscatter!([mean(zμ_flow)[1]], [mean(zμ_flow)[2]], color=\"red\", markershape=:x, markersize=5, label=\"inferred mean\")\ncontour!(-5:0.01:5, -5:0.01:5, (x, y) -> pdf(warped_observation, [x, y]), colors=\"red\", levels=1, linewidth=2, colorbar=false)\nscatter!([mean(warped_observation)[1]], [mean(warped_observation)[2]], color=\"red\", label=\"transformed noisy observation\")\np2 = scatter(y[1,:], y[2,:], alpha=0.1, label=\"generated data\")\nscatter!([ReactiveMP.forward(compiled_model, mean(zμ_flow))[1]], [ReactiveMP.forward(compiled_model, mean(zμ_flow))[2]], color=\"red\", marker=:x, label=\"inferred mean\")\ncontour!(-10:0.1:10, -10:0.1:10, (x, y) -> pdf(MvNormal([1.5, 0.5], I), ReactiveMP.backward(compiled_model, [x, y])), c=:viridis, colorbar=false, linewidth=2)\ncontour!(-10:0.1:10, -10:0.1:10, (x, y) -> pdf(rand_observation, [x, y]), colors=\"red\", levels=1, linewidth=2, label=\"random noisy observation\", colorba=false)\nscatter!([mean(rand_observation)[1]], [mean(rand_observation)[2]], color=\"red\", label=\"random noisy observation\")\nplot(p1, p2, legend = true)","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/#Parameter-estimation","page":"Invertible neural networks: a tutorial","title":"Parameter estimation","text":"","category":"section"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"The flow model is often used to learn unknown probabilistic mappings. Here we will demonstrate it as follows for a binary classification task with the following data:","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"function generate_data(nr_samples::Int64; seed = 123)\n    \n    rng = StableRNG(seed)\n\n    # sample weights\n    w = rand(rng, nr_samples, 2)\n\n    # sample appraisal\n    y = zeros(Float64, nr_samples)\n    for k = 1:nr_samples\n        y[k] = 1.0*(w[k,1] > 0.5)*(w[k,2] < 0.5)\n    end\n\n    # return data\n    return y, w\n\nend;","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"data_y, data_x = generate_data(50);\nscatter(data_x[:,1], data_x[:,2], marker_z=data_y, xlabel=\"w1\", ylabel=\"w2\", colorbar=false, legend=false)","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"We will then specify a possible model as","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"# specify flow model\nmodel = FlowModel(2,\n    (\n        AdditiveCouplingLayer(PlanarFlow()), # defaults to AdditiveCouplingLayer(PlanarFlow(); permute=true)\n        AdditiveCouplingLayer(PlanarFlow()),\n        AdditiveCouplingLayer(PlanarFlow()),\n        AdditiveCouplingLayer(PlanarFlow(); permute=false)\n    )\n);","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"The corresponding probabilistic model for the binary classification task can be created as","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"@model function invertible_neural_network_classifier(nr_samples::Int64)\n    \n    # initialize variables\n    x_lat  = randomvar(nr_samples)\n    y_lat1 = randomvar(nr_samples)\n    y_lat2 = randomvar(nr_samples)\n    y      = datavar(Float64, nr_samples)\n    x      = datavar(Vector{Float64}, nr_samples)\n\n    # specify observations\n    for k = 1:nr_samples\n\n        # specify latent state\n        x_lat[k] ~ MvNormalMeanPrecision(x[k], 1e3*diagm(ones(2)))\n\n        # specify transformed latent value\n        y_lat1[k] ~ Flow(x_lat[k])\n        y_lat2[k] ~ dot(y_lat1[k], [1, 1])\n\n        # specify observations\n        y[k] ~ Probit(y_lat2[k]) # default: where { pipeline = RequireMessage(in = NormalMeanPrecision(0, 1.0)) }\n\n    end\n\n    # return variables\n    return x_lat, x, y_lat1, y_lat2, y\n\nend;","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"fcmodel       = invertible_neural_network_classifier(length(data_y))\ndata          = (y = data_y, x = [data_x[k,:] for k=1:size(data_x,1)], )\n\n@meta function fmeta(model, params)\n    compiled_model = compile(model, params)\n    Flow(y_lat1, x_lat) -> FlowMeta(compiled_model)\nend","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"fmeta (generic function with 2 methods)","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Here we see that the compilation occurs inside of our probabilistic model. As a result we can pass parameters (and a model) to this function which we wish to opmize for some criterium, such as the variational free energy. Inference can be described as","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"For the optimization procedure, we will simplify our inference loop, such that it only accepts parameters as an argument (which is wishes to optimize) and outputs a performance metric.","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"function f(params)\n    Random.seed!(123) # Flow uses random permutation matrices, which is not good for the optimisation procedure\n    result = infer(\n        model                   = fcmodel, \n        data                    = data,\n        meta                    = fmeta(model, params),\n        free_energy             = true,\n        free_energy_diagnostics = nothing, # Free Energy can be set to NaN due to optimization procedure\n        iterations              = 10, \n        showprogress            = false\n    );\n    \n    result.free_energy[end]\nend;","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Optimization can be performed using the Optim package. Alternatively, other (custom) optimizers can be implemented, such as:","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"res = optimize(f, randn(StableRNG(42), nr_params(model)), GradientDescent(), Optim.Options(store_trace = true, show_trace = true, show_every = 50), autodiff=:forward)","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"uses finitediff and is slower/less accurate.","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"or","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"# create gradient function\ng = (x) -> ForwardDiff.gradient(f, x);\n\n# specify initial params\nparams = randn(nr_params(model))\n\n# create custom optimizer (here Adam)\noptimizer = Adam(params; λ=1e-1)\n\n# allocate space for gradient\n∇ = zeros(nr_params(model))\n\n# perform optimization\nfor it = 1:10000\n\n    # backward pass\n    ∇ .= ForwardDiff.gradient(f, optimizer.x)\n\n    # gradient update\n    ReactiveMP.update!(optimizer, ∇)\n\nend\n","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"res = optimize(f, randn(StableRNG(42), nr_params(model)), GradientDescent(), Optim.Options(f_tol = 1e-3, store_trace = true, show_trace = true, show_every = 100), autodiff=:forward)","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"Iter     Function value   Gradient norm \n     0     5.888958e+02     8.943663e+02\n * time: 5.91278076171875e-5\n   100     1.059826e+01     4.120090e+00\n * time: 11.980152130126953\n * Status: success\n\n * Candidate solution\n    Final objective value:     1.010332e+01\n\n * Found with\n    Algorithm:     Gradient Descent\n\n * Convergence measures\n    |x - x'|               = 1.21e-03 ≰ 0.0e+00\n    |x - x'|/|x'|          = 5.80e-04 ≰ 0.0e+00\n    |f(x) - f(x')|         = 6.28e-03 ≰ 0.0e+00\n    |f(x) - f(x')|/|f(x')| = 6.22e-04 ≤ 1.0e-03\n    |g(x)|                 = 2.07e+00 ≰ 1.0e-08\n\n * Work counters\n    Seconds run:   14  (vs limit Inf)\n    Iterations:    113\n    f(x) calls:    301\n    ∇f(x) calls:   301","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"optimization results are then given as","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"params = Optim.minimizer(res)\ninferred_model = compile(model, params)\ntrans_data_x_1 = hcat(map((x) -> ReactiveMP.forward(inferred_model, x), [data_x[k,:] for k=1:size(data_x,1)])...)'\ntrans_data_x_2 = map((x) -> dot([1, 1], x), [trans_data_x_1[k,:] for k=1:size(data_x,1)])\ntrans_data_x_2_split = [trans_data_x_2[data_y .== 1.0], trans_data_x_2[data_y .== 0.0]]\np1 = scatter(data_x[:,1], data_x[:,2], marker_z = data_y, size=(1200,400), c=:viridis, colorbar=false, title=\"original data\")\np2 = scatter(trans_data_x_1[:,1], trans_data_x_1[:,2], marker_z = data_y, c=:viridis, size=(1200,400), colorbar=false, title=\"|> warp\")\np3 = histogram(trans_data_x_2_split; stacked=true, bins=50, size=(1200,400), title=\"|> dot\")\nplot(p1, p2, p3, layout=(1,3), legend=false)","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"(Image: )","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"using StatsFuns: normcdf\np1 = scatter(data_x[:,1], data_x[:,2], marker_z = data_y, title=\"original labels\", xlabel=\"weight 1\", ylabel=\"weight 2\", size=(1200,400), c=:viridis)\np2 = scatter(data_x[:,1], data_x[:,2], marker_z = normcdf.(trans_data_x_2), title=\"predicted labels\", xlabel=\"weight 1\", ylabel=\"weight 2\", size=(1200,400), c=:viridis)\np3 = contour(0:0.01:1, 0:0.01:1, (x, y) -> normcdf(dot([1,1], ReactiveMP.forward(inferred_model, [x,y]))), title=\"Classification map\", xlabel=\"weight 1\", ylabel=\"weight 2\", size=(1200,400), c=:viridis)\nplot(p1, p2, p3, layout=(1,3), legend=false)","category":"page"},{"location":"examples/problem_specific/Invertible Neural Network Tutorial/","page":"Invertible neural networks: a tutorial","title":"Invertible neural networks: a tutorial","text":"(Image: )","category":"page"},{"location":"contributing/new-example/#contributing-new-example","page":"Contributing to the examples","title":"Contributing to the examples","text":"","category":"section"},{"location":"contributing/new-example/","page":"Contributing to the examples","title":"Contributing to the examples","text":"We welcome all possible contributors. This page details some of the guidelines that should be followed when adding a new example (in the examples/ folder) to this package.","category":"page"},{"location":"contributing/new-example/","page":"Contributing to the examples","title":"Contributing to the examples","text":"In order to add a new example simply create a new Jupyter notebook with your experiments in the examples/\"subcategory\" folder. When creating a new example add a descriptive explanation of your experiments, model specification, inference constraints decisions and add appropriate results analysis. We expect examples to be readable for the general public and therefore highly value descriptive comments. If a submitted example only contains code, we will kindly request some changes to improve the readability.","category":"page"},{"location":"contributing/new-example/","page":"Contributing to the examples","title":"Contributing to the examples","text":"After preparing the new example it is necessary to modify the examples/.meta.jl file.","category":"page"},{"location":"contributing/new-example/","page":"Contributing to the examples","title":"Contributing to the examples","text":"Make sure that the very first cell of the notebook contains ONLY # <title> in it and has the markdown cell type. This is important for generating links in our documentation.\nThe path option must be set to a local path in a category sub-folder.\nThe text in the description option will be used on the Examples page in the documentation.\nSet category = :hidden_examples to hide a certain example in the documentation (the example will be executed to ensure it runs without errors).\nPlease do no use Overview as a name for the new example, the title Overview is reserved.\nUse the following template for equations, note that $$ and both \\begin and \\end commands are on the same line (check other examples if you are not sure). This is important, because otherwise formulas may not render correctly. Inline equations may use $...$ template.","category":"page"},{"location":"contributing/new-example/","page":"Contributing to the examples","title":"Contributing to the examples","text":"$$\\begin{aligned}\n      <latex equations here>\n\\end{aligned}$$","category":"page"},{"location":"contributing/new-example/","page":"Contributing to the examples","title":"Contributing to the examples","text":"When using equations, make sure not to follow the left-hand $$ or $ with a space, but instead directly start the equation, e.g. not $$ a + b $$, but $$a + b$$. For equations that are supposed to be on a separate line, make sure $$...$$ is preceded and followed by an empty line.\nNotebooks and plain Julia have different scoping rules for global variables. It may happen that the generation of your example fails due to an UndefVarError or other scoping issues. In these cases we recommend using let ... end blocks to enforce local scoping or use the global keyword to disambiguate the scoping rules, e.g.","category":"page"},{"location":"contributing/new-example/","page":"Contributing to the examples","title":"Contributing to the examples","text":"variable = 0\nfor i in 1:10\n    global variable = variable + i\nend","category":"page"},{"location":"contributing/new-example/","page":"Contributing to the examples","title":"Contributing to the examples","text":"All examples must use and activate the local environment specified by Project.toml in the second cell (see 1.). Please have a look at the existing notebooks for an example on how to activate this local environment. If you need additional packages, you can add then to the (examples) project.\nAll plots should be displayed automatically. In special cases, if needed, save figures in the ../pics/figure-name.ext format. Might be useful for saving gifs. Use ![](../pics/figure-name.ext) to display a static image.","category":"page"},{"location":"contributing/new-example/","page":"Contributing to the examples","title":"Contributing to the examples","text":"note: Note\nPlease avoid adding PyPlot in the (examples) project. Installing and building PyPlot dependencies takes several minutes on every CI run. Use Plots instead.","category":"page"},{"location":"contributing/new-example/","page":"Contributing to the examples","title":"Contributing to the examples","text":"note: Note\nUse make examples to run all examples or make examples specific=MyNewCoolNotebook to run any notebook that includes MyNewCoolNotebook in its file name.","category":"page"},{"location":"library/model-specification/#lib-model-specification","page":"Model specification","title":"Model specification in RxInfer","text":"","category":"section"},{"location":"library/model-specification/","page":"Model specification","title":"Model specification","text":"RxInfer.@model\nRxInfer.ModelGenerator\nRxInfer.create_model\nRxInfer.ModelInferenceOptions","category":"page"},{"location":"library/model-specification/#RxInfer.@model","page":"Model specification","title":"RxInfer.@model","text":"@model function model_name(model_arguments...; model_keyword_arguments...)\n    # model description\nend\n\n@model macro generates a function that returns an equivalent graph-representation of the given probabilistic model description.\n\nSupported alias in the model specification\n\na || b: alias for OR(a, b) node (operator precedence between ||, &&, -> and ! is the same as in Julia).\na && b: alias for AND(a, b) node (operator precedence ||, &&, -> and ! is the same as in Julia).\na -> b: alias for IMPLY(a, b) node (operator precedence ||, &&, -> and ! is the same as in Julia).\n¬a and !a: alias for NOT(a) node (Unicode \\neg, operator precedence ||, &&, -> and ! is the same as in Julia).\na + b + c: alias for (a + b) + c\na * b * c: alias for (a * b) * c\nNormal(μ|m|mean = ..., σ²|τ⁻¹|v|var|variance = ...) alias for NormalMeanVariance(..., ...) node. Gaussian could be used instead Normal too.\nNormal(μ|m|mean = ..., τ|γ|σ⁻²|w|p|prec|precision = ...) alias for NormalMeanPrecision(..., ...) node. Gaussian could be used instead Normal too.\nMvNormal(μ|m|mean = ..., Σ|V|Λ⁻¹|cov|covariance = ...) alias for MvNormalMeanCovariance(..., ...) node. MvGaussian could be used instead MvNormal too.\nMvNormal(μ|m|mean = ..., Λ|W|Σ⁻¹|prec|precision = ...) alias for MvNormalMeanPrecision(..., ...) node. MvGaussian could be used instead MvNormal too.\nMvNormal(μ|m|mean = ..., τ|γ|σ⁻²|scale_diag_prec|scale_diag_precision = ...) alias for MvNormalMeanScalePrecision(..., ...) node. MvGaussian could be used instead MvNormal too.\nGamma(α|a|shape = ..., θ|β⁻¹|scale = ...) alias for GammaShapeScale(..., ...) node.\nGamma(α|a|shape = ..., β|θ⁻¹|rate = ...) alias for GammaShapeRate(..., ...) node.\n\n\n\n\n\n","category":"macro"},{"location":"library/model-specification/#RxInfer.ModelGenerator","page":"Model specification","title":"RxInfer.ModelGenerator","text":"ModelGenerator\n\nModelGenerator is a special object that is used in the infer function to lazily create model later on given constraints, meta and options.\n\nSee also: infer\n\n\n\n\n\n","category":"type"},{"location":"library/model-specification/#RxInfer.create_model","page":"Model specification","title":"RxInfer.create_model","text":"create_model(::ModelGenerator, constraints = nothing, meta = nothing, options = nothing)\n\nCreates an instance of FactorGraphModel from the given model specification as well as optional constraints, meta and options.\n\nReturns a tuple of 2 values:\n\nan instance of FactorGraphModel\nreturn value from the @model macro function definition\n\n\n\n\n\n","category":"function"},{"location":"library/model-specification/#RxInfer.ModelInferenceOptions","page":"Model specification","title":"RxInfer.ModelInferenceOptions","text":"ModelInferenceOptions(; kwargs...)\n\nCreates model inference options object. The list of available options is present below.\n\nOptions\n\nlimit_stack_depth: limits the stack depth for computing messages, helps with StackOverflowError for some huge models, but reduces the performance of inference backend. Accepts integer as an argument that specifies the maximum number of recursive depth. Lower is better for stack overflow error, but worse for performance.\nwarn: (optional) flag to suppress warnings. Warnings are not displayed if set to false. Defaults to true.\n\nAdvanced options\n\npipeline: changes the default pipeline for each factor node in the graph\nglobal_reactive_scheduler: changes the scheduler of reactive streams, see Rocket.jl for more info, defaults to no scheduler\n\nSee also: infer\n\n\n\n\n\n","category":"type"},{"location":"manuals/getting-started/#user-guide-getting-started","page":"Getting started","title":"Getting started","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"RxInfer.jl is a Julia package for Bayesian Inference on Factor Graphs by Message Passing. It supports both exact and variational inference algorithms.","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"RxInfer.jl package forms an ecosystem around three main packages: ReactiveMP.jl exports a reactive message passing based Bayesian inference engine, Rocket.jl is the core library that enables reactivity and GraphPPL.jl library simplifies model and constraints specification. ReactiveMP.jl engine is a successor of the ForneyLab package. It follows the same ideas and concepts for message-passing based inference, but uses new reactive and efficient message passing implementation under the hood. The API between two packages is different due to a better flexibility, performance and new reactive approach for solving inference problems.","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"This page provides the necessary information you need to get started with Rxinfer. We will show the general approach to solving inference problems with RxInfer by means of a running example: inferring the bias of a coin.","category":"page"},{"location":"manuals/getting-started/#Installation","page":"Getting started","title":"Installation","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Install RxInfer through the Julia package manager:","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"] add RxInfer","category":"page"},{"location":"manuals/getting-started/#Importing-RxInfer","page":"Getting started","title":"Importing RxInfer","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"To add RxInfer package (and all associated packages) into a running Julia session simply run:","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"using RxInfer","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Read more about about using in the Using methods from RxInfer section of the documentation.","category":"page"},{"location":"manuals/getting-started/#Example:-Inferring-the-bias-of-a-coin","page":"Getting started","title":"Example: Inferring the bias of a coin","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"The RxInfer approach to solving inference problems consists of three phases:","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Model specification: RxInfer uses GraphPPL package for model specification part. It offers a domain-specific language to specify your probabilistic model.\nInference specification: RxInfer inference API uses ReactiveMP inference engine under the hood and has been designed to be as flexible as possible. It is compatible both with asynchronous infinite data streams and with static datasets. For most of the use cases it consists of the same simple building blocks. In this example we will show one of the many possible ways to infer your quantities of interest.\nInference execution: Given model specification and inference procedure it is pretty straightforward to use reactive API from Rocket to pass data to the inference backend and to run actual inference.","category":"page"},{"location":"manuals/getting-started/#Coin-flip-simulation","page":"Getting started","title":"Coin flip simulation","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Let's start by creating some dataset. One approach could be flipping a coin N times and recording each outcome. For simplicity in this example we will use static pre-generated dataset. Each sample can be thought of as the outcome of single flip which is either heads or tails (1 or 0). We will assume that our virtual coin is biased, and lands heads up on 75% of the trials (on average).","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"First let's setup our environment by importing all needed packages:","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"using RxInfer, Distributions, Random","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Next, let's define our dataset:","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"rng = MersenneTwister(42)\nn = 10\np = 0.75\ndistribution = Bernoulli(p)\n\ndataset = float.(rand(rng, Bernoulli(p), n))","category":"page"},{"location":"manuals/getting-started/#getting-started-model-specification","page":"Getting started","title":"Model specification","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"In a Bayesian setting, the next step is to specify our probabilistic model. This amounts to specifying the joint probability of the random variables of the system.","category":"page"},{"location":"manuals/getting-started/#Likelihood","page":"Getting started","title":"Likelihood","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"We will assume that the outcome of each coin flip is governed by the Bernoulli distribution, i.e.","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"y_i sim mathrmBernoulli(theta)","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"where y_i = 1 represents \"heads\", y_i = 0 represents \"tails\". The underlying probability of the coin landing heads up for a single coin flip is theta in 01.","category":"page"},{"location":"manuals/getting-started/#Prior","page":"Getting started","title":"Prior","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"We will choose the conjugate prior of the Bernoulli likelihood function defined above, namely the beta distribution, i.e.","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"theta sim Beta(a b)","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"where a and b are the hyperparameters that encode our prior beliefs about the possible values of theta. We will assign values to the hyperparameters in a later step.   ","category":"page"},{"location":"manuals/getting-started/#Joint-probability","page":"Getting started","title":"Joint probability","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"The joint probability is given by the multiplication of the likelihood and the prior, i.e.","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"P(y_1N θ) = P(θ) prod_i=1^N P(y_i  θ)","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Now let's see how to specify this model using GraphPPL's package syntax.","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"\n# GraphPPL.jl export `@model` macro for model specification\n# It accepts a regular Julia function and builds an FFG under the hood\n@model function coin_model(n)\n\n    # `datavar` creates data 'inputs' in our model\n    # We will pass data later on to these inputs\n    # In this example we create a sequence of inputs that accepts Float64\n    y = datavar(Float64, n)\n    \n    # We endow θ parameter of our model with some prior\n    θ ~ Beta(2.0, 7.0)\n    # or, in this particular case, the `Uniform(0.0, 1.0)` prior also works:\n    # θ ~ Uniform(0.0, 1.0)\n    \n    # We assume that outcome of each coin flip is governed by the Bernoulli distribution\n    for i in 1:n\n        y[i] ~ Bernoulli(θ)\n    end\n    \n    # We return references to our data inputs and θ parameter\n    # We will use these references later on during inference step\n    return y, θ\nend\n","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"As you can see, RxInfer offers a model specification syntax that resembles closely to the mathematical equations defined above. We use datavar function to create \"clamped\" variables that take specific values at a later date. θ ~ Beta(2.0, 7.0) expression creates random variable θ and assigns it as an output of Beta node in the corresponding FFG. ","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"note: Note\nTo quickly check the list of all available factor nodes that can be used in the model specification language call ?make_node or Base.doc(make_node).","category":"page"},{"location":"manuals/getting-started/#getting-started-inference-specification","page":"Getting started","title":"Inference specification","text":"","category":"section"},{"location":"manuals/getting-started/#Automatic-inference-specification","page":"Getting started","title":"Automatic inference specification","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Once we have defined our model, the next step is to use RxInfer API to infer quantities of interests. To do this we can use a generic inference function that supports static datasets.","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"result = infer(\n    model = coin_model(length(dataset)),\n    data  = (y = dataset, )\n)","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"θestimated = result.posteriors[:θ]","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"println(\"mean: \", mean(θestimated))\nprintln(\"std:  \", std(θestimated))\nnothing #hide","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Read more information about the inference function in the Static Inference documentation section.","category":"page"},{"location":"manuals/getting-started/#Manual-inference-specification","page":"Getting started","title":"Manual inference specification","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"There is a way to manually specify an inference procedure for advanced use-cases. RxInfer API is flexible in terms of inference specification and is compatible both with real-time inference processing and with static datasets. In most of the cases for static datasets, as in our example, it consists of same basic building blocks:","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Return variables of interests from model specification\nSubscribe on variables of interests posterior marginal updates\nPass data to the model\nUnsubscribe ","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Here is an example of inference procedure:","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"function custom_inference(data)\n    n = length(data)\n\n    # `coin_model` function from `@model` macro returns a reference to the model generator object\n    # we need to use the `create_model` function to get actual model object\n    model, (y, θ) = create_model(coin_model(n))\n    \n    # Reference for future posterior marginal \n    mθ = nothing\n\n    # `getmarginal` function returns an observable of future posterior marginal updates\n    # We use `Rocket.jl` API to subscribe on this observable\n    # As soon as posterior marginal update is available we just save it in `mθ`\n    subscription = subscribe!(getmarginal(θ), (m) -> mθ = m)\n    \n    # `update!` function passes data to our data inputs\n    update!(y, data)\n    \n    # It is always a good practice to unsubscribe and to \n    # free computer resources held by the subscription\n    unsubscribe!(subscription)\n    \n    # Here we return our resulting posterior marginal\n    return mθ\nend","category":"page"},{"location":"manuals/getting-started/#getting-started-inference-execution","page":"Getting started","title":"Inference execution","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"Here after everything is ready we just call our inference function to get a posterior marginal distribution over θ parameter in the model.","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"θestimated = custom_inference(dataset)","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"println(\"mean: \", mean(θestimated))\nprintln(\"std:  \", std(θestimated))\nnothing #hide","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"using Plots\n\nrθ = range(0, 1, length = 1000)\n\np1 = plot(rθ, (x) -> pdf(Beta(2.0, 7.0), x), title=\"Prior\", fillalpha=0.3, fillrange = 0, label=\"P(θ)\", c=1,)\np2 = plot(rθ, (x) -> pdf(θestimated, x), title=\"Posterior\", fillalpha=0.3, fillrange = 0, label=\"P(θ|y)\", c=3)\n\nplot(p1, p2, layout = @layout([ a; b ]))","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"In our dataset we used 10 coin flips to estimate the bias of a coin. It resulted in a vague posterior distribution, however RxInfer scales very well for large models and factor graphs. We may use more coin flips in our dataset for better posterior distribution estimates:","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"dataset_100   = float.(rand(rng, Bernoulli(p), 100))\ndataset_1000  = float.(rand(rng, Bernoulli(p), 1000))\ndataset_10000 = float.(rand(rng, Bernoulli(p), 10000))\nnothing # hide","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"θestimated_100   = custom_inference(dataset_100)\nθestimated_1000  = custom_inference(dataset_1000)\nθestimated_10000 = custom_inference(dataset_10000)\nnothing #hide","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"p3 = plot(title = \"Posterior\", legend = :topleft)\n\np3 = plot!(p3, rθ, (x) -> pdf(θestimated_100, x), fillalpha = 0.3, fillrange = 0, label = \"P(θ|y_100)\", c = 4)\np3 = plot!(p3, rθ, (x) -> pdf(θestimated_1000, x), fillalpha = 0.3, fillrange = 0, label = \"P(θ|y_1000)\", c = 5)\np3 = plot!(p3, rθ, (x) -> pdf(θestimated_10000, x), fillalpha = 0.3, fillrange = 0, label = \"P(θ|y_10000)\", c = 6)\n\nplot(p1, p3, layout = @layout([ a; b ]))","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"With larger dataset our posterior marginal estimate becomes more and more accurate and represents real value of the bias of a coin.","category":"page"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"println(\"mean: \", mean(θestimated_10000))\nprintln(\"std:  \", std(θestimated_10000))\nnothing #hide","category":"page"},{"location":"manuals/getting-started/#Where-to-go-next?","page":"Getting started","title":"Where to go next?","text":"","category":"section"},{"location":"manuals/getting-started/","page":"Getting started","title":"Getting started","text":"There are a set of examples available in RxInfer repository that demonstrate the more advanced features of the package and also Examples section in the documentation. Alternatively, you can head to the Model specification which provides more detailed information of how to use RxInfer to specify probabilistic models. Inference execution section provides a documentation about RxInfer API for running reactive Bayesian inference.","category":"page"},{"location":"manuals/debugging/#user-guide-debugging","page":"Debugging","title":"Debugging","text":"","category":"section"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"Debugging RxInfer can be quite challenging, mostly due to custom typing, the use of observables and Julia's stack tracing in general. Below we discuss ways to help you find problems in your model that prevents you from getting the results you want. ","category":"page"},{"location":"manuals/debugging/#Requesting-a-trace-of-messages","page":"Debugging","title":"Requesting a trace of messages","text":"","category":"section"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"We have developed a way that allows us to save the history of the computations leading up to the computed messages and marginals in an inference procedure. This history is added on top of messages and marginals and is referred to as a \"Memory Addon\". Below is an example explaining how you can extract this history and use it to fix a bug.","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"Consider the coin toss example from earlier in the documentation. We model the binary outcome x (heads or tails) using a Bernoulli distribution, with a parameter 𝜃 that represents the probability of landing on heads. We have a Beta prior distribution for the theta parameter, with a known shape 𝑎 and rate 𝑏 parameter.","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"x_i sim mathrmBernoulli(theta)","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"theta sim mathrmBeta(a b)","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"where x_i in 0 1 are the binary observations (heads = 1, tails = 0). This is the corresponding RxInfer model:","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"using RxInfer, Random, Plots\n\nn = 4\nθ_real = 0.3\ndataset = convert.(Int64, rand(Bernoulli(θ_real), n))\n\n@model function coin_model(n)\n    \n    # Observations\n    x = datavar(Int64, n)\n    \n    # Prior distribution\n    θ ~ Beta(4, huge)\n\n    # Likelihood for each input\n    for i in 1:n\n        x[i] ~ Bernoulli(θ)\n    end\n\nend\n\nresult = infer(\n    model = coin_model(length(dataset)), \n    data  = (x = dataset, ),\n);\n","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"The model will run without errors. But when we plot the posterior distribution for theta, something's wrong. The posterior seems to be a flat distribution:","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"\nrθ = range(0, 1, length = 1000)\n\nplot(rθ, (rvar) -> pdf(result.posteriors[:θ], rvar), label=\"Infered posterior\")\nvline!([θ_real], label=\"Real θ\", title = \"Inference results\")","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"We can figure out what's wrong by looking at the Memory Addon. To obtain the trace, we have to add addons = (AddonMemory(),) as an argument to the inference function.","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"result = infer(\n    model = coin_model(length(dataset)), \n    data  = (x = dataset, ),\n    addons = (AddonMemory(),)\n)","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"Now we have access to the messages that led to the marginal posterior:","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"(Image: Addons_messages)","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"The messages in the factor graph are marked in color. If you're interested in the mathematics behind these results, consider verifying them manually using the general equation for sum-product messages:","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"underbraceoverrightarrowmu_θ(θ)_substack textoutgoing textmessage = sum_x_1ldotsx_n underbraceoverrightarrowmu_X_1(x_1)cdots overrightarrowmu_X_n(x_n)_substacktextincoming  textmessages cdot underbracef(θx_1ldotsx_n)_substacktextnode textfunction","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"(Image: Graph)","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"Note that the posterior (yellow) has a rate parameter on the order of 1e12. Our plot failed because a Beta distribution with such a rate parameter cannot be accurately depicted using the range of theta we used in the code block above. So why does the posterior have this rate parameter?","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"All the observations (purple, green, pink, blue) have much smaller rate parameters. It seems the prior distribution (red) has an unusual rate parameter, namely 1e12. If we look back at the model, the parameter was set to huge (which is a reserved keyword meaning 1e12). Reducing the prior rate parameter will ensure the posterior has a reasonable rate parameter as well.","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"@model function coin_model(n)\n    \n    # Observations\n    x = datavar(Int64, n)\n    \n    # Prior distribution\n    θ ~ Beta(4, 100)\n\n    # Likelihood for each input\n    for i in 1:n\n        x[i] ~ Bernoulli(θ)\n    end\n\nend\n\nresult = infer(\n    model = coin_model(length(dataset)), \n    data  = (x = dataset, ),\n);","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"rθ = range(0, 1, length = 1000)\n\nplot(rθ, (rvar) -> pdf(result.posteriors[:θ], rvar), label=\"Infered posterior\")\nvline!([θ_real], label=\"Real θ\", title = \"Inference results\")","category":"page"},{"location":"manuals/debugging/","page":"Debugging","title":"Debugging","text":"Now the posterior is visible in the plot.","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"This example has been auto-generated from the examples/ folder at GitHub repository.","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/#examples-active-inference-mountain-car","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"","category":"section"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"import Pkg; Pkg.activate(\"..\"); Pkg.instantiate();","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"using RxInfer, Plots","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"A group of friends is going to a camping site that is located on the biggest mountain in the Netherlands. They use an electric car for the trip. When they are almost there, the car's battery is almost empty and is therefore limiting the engine force. Unfortunately, they are in the middle of a valley and don't have enough power to reach the camping site. Night is falling and they still need to reach the top of the mountain. As rescuers, let us develop an Active Inference (AI) agent that can get them up the hill with the limited engine power.","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/#The-environmental-process-of-the-mountain","page":"Active Inference Mountain car","title":"The environmental process of the mountain","text":"","category":"section"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"Firstly, we specify the environmental process according to Ueltzhoeffer (2017) \"Deep active inference\". This process shows how the environment evolves after interacting with the agent.","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"Particularly, let's denote z_t = (phi_t dotphi_t) as the environmental state depending on the position phi_t and velocity dotphi_t of the car; a_t as the action of the environment on the car. Then the evolution of the state is described as follows  ","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"beginaligned \ndotphi_t = dotphi_t-1 + F_g(phi_t-1) + F_f(dotphi_t-1) + F_a(a_t)\nphi_t = phi_t-1 + dotphi_t \nendaligned","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"where F_g(phi_t-1) is the gravitational force of the hill landscape that depends on the car's position","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"F_g(phi) = begincases\n        -005(2phi + 1)    mathrmif  phi  0 \n        -005 left(1 + 5phi^2)^-frac12 + phi^2 (1 + 5phi^2)^-frac32 + frac116phi^4 right   mathrmotherwise\nendcases","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"F_f(dotphi)","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"is the friction on the car defined through the car's velocity F_f(dotphi)  = -01  dotphi and F_a(a) is the engine force F_a(a) = 004 tanh(a) Since the car is on low battery, we use the tanh(cdot) function to limit the engine force to the interval [-0.04, 0.04].","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"In the cell below, the create_physics function defines forces F_g F_f F_a; and the create_world function defines the environmental process of the mountain.","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"import HypergeometricFunctions: _₂F₁\n\nfunction create_physics(; engine_force_limit = 0.04, friction_coefficient = 0.1)\n    # Engine force as function of action\n    Fa = (a::Real) -> engine_force_limit * tanh(a) \n\n    # Friction force as function of velocity\n    Ff = (y_dot::Real) -> -friction_coefficient * y_dot \n    \n    # Gravitational force (horizontal component) as function of position\n    Fg = (y::Real) -> begin\n        if y < 0\n            0.05*(-2*y - 1)\n        else\n            0.05*(-(1 + 5*y^2)^(-0.5) - (y^2)*(1 + 5*y^2)^(-3/2) - (y^4)/16)\n        end\n    end\n    \n    # The height of the landscape as a function of the horizontal coordinate\n    height = (x::Float64) -> begin\n        if x < 0\n            h = x^2 + x\n        else\n            h = x * _₂F₁(0.5,0.5,1.5, -5*x^2) + x^3 * _₂F₁(1.5, 1.5, 2.5, -5*x^2) / 3 + x^5 / 80\n        end\n        return 0.05*h\n    end\n\n    return (Fa, Ff, Fg,height)\nend;\n\nfunction create_world(; Fg, Ff, Fa, initial_position = -0.5, initial_velocity = 0.0)\n\n    y_t_min = initial_position\n    y_dot_t_min = initial_velocity\n    \n    y_t = y_t_min\n    y_dot_t = y_dot_t_min\n    \n    execute = (a_t::Float64) -> begin\n        # Compute next state\n        y_dot_t = y_dot_t_min + Fg(y_t_min) + Ff(y_dot_t_min) + Fa(a_t)\n        y_t = y_t_min + y_dot_t\n    \n        # Reset state for next step\n        y_t_min = y_t\n        y_dot_t_min = y_dot_t\n    end\n    \n    observe = () -> begin \n        return [y_t, y_dot_t]\n    end\n        \n    return (execute, observe)\nend","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"create_world (generic function with 1 method)","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"Let's visualize the mountain landscape and the situation of the car. ","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"engine_force_limit   = 0.04\nfriction_coefficient = 0.1\n\nFa, Ff, Fg, height = create_physics(\n    engine_force_limit = engine_force_limit,\n    friction_coefficient = friction_coefficient\n);\ninitial_position = -0.5\ninitial_velocity = 0.0\n\nx_target = [0.5, 0.0] \n\nvalley_x = range(-2, 2, length=400)\nvalley_y = [ height(xs) for xs in valley_x ]\nplot(valley_x, valley_y, title = \"Mountain valley\", label = \"Landscape\", color = \"black\")\nscatter!([ initial_position ], [ height(initial_position) ], label=\"initial car position\")   \nscatter!([x_target[1]], [height(x_target[1])], label=\"camping site\")","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/#Naive-approach","page":"Active Inference Mountain car","title":"Naive approach","text":"","category":"section"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"Well, let's see how our friends were struggling with the low-battery car when they tried to get it to the camping site before we come to help. They basically used the brute-force method, i.e. just pushing the gas pedal for full power.","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"N_naive  = 100 # Total simulation time\npi_naive = 100.0 * ones(N_naive) # Naive policy for right full-power only\n\n# Let there be a world\n(execute_naive, observe_naive) = create_world(; \n    Fg = Fg, Ff = Ff, Fa = Fa, \n    initial_position = initial_position, \n    initial_velocity = initial_velocity\n);\n\ny_naive = Vector{Vector{Float64}}(undef, N_naive)\nfor t = 1:N_naive\n    execute_naive(pi_naive[t]) # Execute environmental process\n    y_naive[t] = observe_naive() # Observe external states\nend\n\nanimation_naive = @animate for i in 1:N_naive\n    plot(valley_x, valley_y, title = \"Naive policy\", label = \"Landscape\", color = \"black\", size = (800, 400))\n    scatter!([y_naive[i][1]], [height(y_naive[i][1])], label=\"car\")\n    scatter!([x_target[1]], [height(x_target[1])], label=\"goal\")   \nend\n\n# The animation is saved and displayed as markdown picture for the automatic HTML generation\ngif(animation_naive, \"../pics/ai-mountain-car-naive.gif\", fps = 24, show_msg = false);","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"They failed as expected since the car doesn't have enough power. This helps to understand that the brute-force approach is not the most efficient one in this case and hopefully a bit of swinging is necessary to achieve the goal.","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/#Active-inference-approach","page":"Active Inference Mountain car","title":"Active inference approach","text":"","category":"section"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"Now let's help them solve the problem with an active inference approach. Particularly, we create an agent that predicts the future car position as well as the best possible actions in a probabilistic manner.","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"We start by specifying a probabilistic model for the agent that describes the agent's internal beliefs over the external dynamics of the environment. The generative model is defined as follows","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"beginaligned\np_t(xsu) propto p(s_t-1) prod_k=t^t+T p(x_k mid s_k)  p(s_k mid s_k-1u_k)  p(u_k)  p(x_k) nonumber\nendaligned","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"where the factors are defined as","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"p(x_k) = mathcalN(x_k mid x_goalV_goal)  quad (mathrmtarget)","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"p(s_k mid s_k-1u_k) = mathcalN(s_k mid tildeg(s_k-1)+h(u_k)gamma^-1)  quad (mathrmstate  transition)","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"p(x_k mid s_k) = mathcalN(x_k mid s_ktheta) quad (mathrmobservation)","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"p(u_k) = mathcalN(u_k mid m_uV_u) quad (mathrmcontrol)","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"p(s_t-1) = mathcalN(s_t-1 mid m_t-1V_t-1) quad (mathrmprevious  state)","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"where ","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"x\ndenotes observations of the agent after interacting with the environment; \ns_t = (s_tdots_t)\nis the state of the car embodying its position and velocity; \nu_t\ndenotes the control state of the agent; \nh(cdot)\nis the tanh(cdot) function modeling engine control; \ntildeg(cdot)\nexecutes a linear approximation of equations (1) and (2): ","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"beginaligned \ndots_t = dots_t-1 + F_g(s_t-1) + F_f(dots_t-1)\ns_t = s_t-1 + dots_t\nendaligned","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"In the cell below, the @model macro and the meta blocks are used to define the probabilistic model and the approximation methods for the nonlinear state-transition functions, respectively. In addition, the beliefs over the future states (up to T steps ahead) of the agent is included.","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"@model function mountain_car(; T, Fg, Fa, Ff, engine_force_limit)\n    \n    # Transition function modeling transition due to gravity and friction\n    g = (s_t_min::AbstractVector) -> begin \n        s_t = similar(s_t_min) # Next state\n        s_t[2] = s_t_min[2] + Fg(s_t_min[1]) + Ff(s_t_min[2]) # Update velocity\n        s_t[1] = s_t_min[1] + s_t[2] # Update position\n        return s_t\n    end\n    \n    # Function for modeling engine control\n    h = (u::AbstractVector) -> [0.0, Fa(u[1])] \n    \n    # Inverse engine force, from change in state to corresponding engine force\n    h_inv = (delta_s_dot::AbstractVector) -> [atanh(clamp(delta_s_dot[2], -engine_force_limit+1e-3, engine_force_limit-1e-3)/engine_force_limit)] \n    \n    # Internal model perameters\n    Gamma = 1e4*diageye(2) # Transition precision\n    Theta = 1e-4*diageye(2) # Observation variance\n    \n    m_s_t_min = datavar(Vector{Float64})\n    V_s_t_min = datavar(Matrix{Float64})\n\n    s_t_min ~ MvNormal(mean = m_s_t_min, cov = V_s_t_min)\n    s_k_min = s_t_min\n    \n    m_u = datavar(Vector{Float64}, T)\n    V_u = datavar(Matrix{Float64}, T)\n    \n    m_x = datavar(Vector{Float64}, T)\n    V_x = datavar(Matrix{Float64}, T)\n    \n    u = randomvar(T)\n    s = randomvar(T)\n    x = randomvar(T)\n    \n    u_h_k = randomvar(T)\n    s_g_k = randomvar(T)\n    u_s_sum = randomvar(T)\n    \n    for k in 1:T\n        u[k] ~ MvNormal(mean = m_u[k], cov = V_u[k])\n        u_h_k[k] ~ h(u[k]) where { meta = DeltaMeta(method = Linearization(), inverse = h_inv) }\n        s_g_k[k] ~ g(s_k_min) where { meta = DeltaMeta(method = Linearization()) }\n        u_s_sum[k] ~ s_g_k[k] + u_h_k[k]\n        s[k] ~ MvNormal(mean = u_s_sum[k], precision = Gamma)\n        x[k] ~ MvNormal(mean = s[k], cov = Theta)\n        x[k] ~ MvNormal(mean = m_x[k], cov = V_x[k]) # goal\n        s_k_min = s[k]\n    end\n    \n    return (s, )\nend","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"After specifying the generative model, let's create an Active Inference(AI) agent for the car.  Technically, the agent goes through three phases: Act-Execute-Observe, Infer and Slide.","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"Act-Execute-Observe:   In this phase, the agent performs an action onto the environment at time t and gets T observations in exchange. These observations are basically the prediction of the agent on how the environment evolves over the next T time step. \nInfer:  After receiving observations, the agent starts updating its internal probabilistic model by doing inference. Particularly, it finds the posterior distributions over the state s_t and control u_t, i.e. p(s_tmid x_t) and p(u_tmid x_t).\nSlide:  After updating its internal belief, the agent moves to the next time step and uses the inferred action u_t in the previous time step to interact with the environment.  ","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"In the cell below, we create the agent through the create_agent function, which includes compute, act, slide and future functions:","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"The act function selects the next action based on the inferred policy. On the other hand, the future function predicts the next T positions based on the current action. These two function implement the Act-Execute-Observe phase.\nThe compute function infers the policy (which is a set of actions for the next T time steps) and the agent's state using the agent internal model. This function implements the Infer phase. We call it compute to avoid the clash with the infer function of RxInfer.jl.\nThe slide function implements the Slide phase, which moves the agent internal model to the next time step.","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"# We are going to use some private functionality from ReactiveMP, \n# in the future we should expose a proper API for this\nimport RxInfer.ReactiveMP: getrecent, messageout\n\nfunction create_agent(; T = 20, Fg, Fa, Ff, engine_force_limit, x_target, initial_position, initial_velocity)\n    Epsilon = fill(huge, 1, 1)                # Control prior variance\n    m_u = Vector{Float64}[ [ 0.0] for k=1:T ] # Set control priors\n    V_u = Matrix{Float64}[ Epsilon for k=1:T ]\n\n    Sigma    = 1e-4*diageye(2) # Goal prior variance\n    m_x      = [zeros(2) for k=1:T]\n    V_x      = [huge*diageye(2) for k=1:T]\n    V_x[end] = Sigma # Set prior to reach goal at t=T\n\n    # Set initial brain state prior\n    m_s_t_min = [initial_position, initial_velocity] \n    V_s_t_min = tiny * diageye(2)\n    \n    # Set current inference results\n    result = nothing\n\n    # The `infer` function is the heart of the agent\n    # It calls the `RxInfer.inference` function to perform Bayesian inference by message passing\n    compute = (upsilon_t::Float64, y_hat_t::Vector{Float64}) -> begin\n        m_u[1] = [ upsilon_t ] # Register action with the generative model\n        V_u[1] = fill(tiny, 1, 1) # Clamp control prior to performed action\n\n        m_x[1] = y_hat_t # Register observation with the generative model\n        V_x[1] = tiny*diageye(2) # Clamp goal prior to observation\n\n        data = Dict(:m_u       => m_u, \n                    :V_u       => V_u, \n                    :m_x       => m_x, \n                    :V_x       => V_x,\n                    :m_s_t_min => m_s_t_min,\n                    :V_s_t_min => V_s_t_min)\n        \n        model  = mountain_car(; T = T, Fg = Fg, Fa = Fa, Ff = Ff, engine_force_limit = engine_force_limit) \n        result = infer(model = model, data = data)\n    end\n    \n    # The `act` function returns the inferred best possible action\n    act = () -> begin\n        if result !== nothing\n            return mode(result.posteriors[:u][2])[1]\n        else\n            return 0.0 # Without inference result we return some 'random' action\n        end\n    end\n    \n    # The `future` function returns the inferred future states\n    future = () -> begin \n        if result !== nothing \n            return getindex.(mode.(result.posteriors[:s]), 1)\n        else\n            return zeros(T)\n        end\n    end\n\n    # The `slide` function modifies the `(m_s_t_min, V_s_t_min)` for the next step\n    # and shifts (or slides) the array of future goals `(m_x, V_x)` and inferred actions `(m_u, V_u)`\n    slide = () -> begin\n        (s, ) = result.returnval\n        \n        slide_msg_idx = 3 # This index is model dependend\n        (m_s_t_min, V_s_t_min) = mean_cov(getrecent(messageout(s[2], slide_msg_idx)))\n\n        m_u = circshift(m_u, -1)\n        m_u[end] = [0.0]\n        V_u = circshift(V_u, -1)\n        V_u[end] = Epsilon\n\n        m_x = circshift(m_x, -1)\n        m_x[end] = x_target\n        V_x = circshift(V_x, -1)\n        V_x[end] = Sigma\n    end\n\n    return (compute, act, slide, future)    \nend","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"create_agent (generic function with 1 method)","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"Now it's time to see if we can help our friends arrive at the camping site by midnight?","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"(execute_ai, observe_ai) = create_world(\n    Fg = Fg, Ff = Ff, Fa = Fa, \n    initial_position = initial_position, \n    initial_velocity = initial_velocity\n) # Let there be a world\n\nT_ai = 50\n\n(compute_ai, act_ai, slide_ai, future_ai) = create_agent(; # Let there be an agent\n    T  = T_ai, \n    Fa = Fa,\n    Fg = Fg, \n    Ff = Ff, \n    engine_force_limit = engine_force_limit,\n    x_target = x_target,\n    initial_position = initial_position,\n    initial_velocity = initial_velocity\n) \n\nN_ai = 100\n\n# Step through experimental protocol\nagent_a = Vector{Float64}(undef, N_ai) # Actions\nagent_f = Vector{Vector{Float64}}(undef, N_ai) # Predicted future\nagent_x = Vector{Vector{Float64}}(undef, N_ai) # Observations\n\nfor t=1:N_ai\n    agent_a[t] = act_ai()               # Invoke an action from the agent\n    agent_f[t] = future_ai()            # Fetch the predicted future states\n    execute_ai(agent_a[t])              # The action influences hidden external states\n    agent_x[t] = observe_ai()           # Observe the current environmental outcome (update p)\n    compute_ai(agent_a[t], agent_x[t]) # Infer beliefs from current model state (update q)\n    slide_ai()                          # Prepare for next iteration\nend\n\nanimation_ai = @animate for i in 1:N_ai\n    # pls - plot landscape\n    pls = plot(valley_x, valley_y, title = \"Active inference results\", label = \"Landscape\", color = \"black\")\n    pls = scatter!(pls, [agent_x[i][1]], [height(agent_x[i][1])], label=\"car\")\n    pls = scatter!(pls, [x_target[1]], [height(x_target[1])], label=\"goal\")   \n    pls = scatter!(pls, agent_f[i], height.(agent_f[i]), label = \"Predicted future\", alpha = map(i -> 0.5 / i, 1:T_ai))\n    \n    # pef - plot engine force\n    pef = plot(Fa.(agent_a[1:i]), title = \"Engine force (agents actions)\", xlim = (0, N_ai), ylim = (-0.05, 0.05))\n    \n    plot(pls, pef, size = (800, 400))\nend\n    \n# The animation is saved and displayed as markdown picture for the automatic HTML generation\ngif(animation_ai, \"../pics/ai-mountain-car-ai.gif\", fps = 24, show_msg = false);","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"(Image: )","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"Voila! The car now is able to reach the camping site with a smart strategy.","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"The left figure shows the agent reached its goal by swinging and the right one shows the corresponding engine force. As we can see, at the beginning the agent tried to reach the goal directly (with full engine force) but after some trials it realized that's not possible. Since the agent looks ahead for 50 time steps, it has enough time to explore other policies, helping it learn to move back to get more momentum to reach the goal.","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"Now our friends can enjoy their trip at the camping site!. ","category":"page"},{"location":"examples/advanced_examples/Active Inference Mountain car/#Reference","page":"Active Inference Mountain car","title":"Reference","text":"","category":"section"},{"location":"examples/advanced_examples/Active Inference Mountain car/","page":"Active Inference Mountain car","title":"Active Inference Mountain car","text":"We refer reader to the Thijs van de Laar (2019) \"Simulating active inference processes by message passing\" original paper with more in-depth overview and explanation of the active inference agent implementation by message passing. The original environment/task description is from Ueltzhoeffer (2017) \"Deep active inference\".","category":"page"},{"location":"library/bethe-free-energy/#lib-bethe-free-energy","page":"Bethe Free Energy","title":"Bethe Free Energy implementation in RxInfer","text":"","category":"section"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"The following text introduces the Bethe Free Energy. We start be defining a factorized model and move from the Variational Free Energy to a definition of the Bethe Free Energy.","category":"page"},{"location":"library/bethe-free-energy/#lib-bethe-factorized-model","page":"Bethe Free Energy","title":"Factorized model","text":"","category":"section"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"Before we can define a model, we must identify all variables that are relevant to the problem at hand. We distinguish between variables that can be directly observed, y = (y_1 dots y_j dots y_m) and variables that can not be observed directly, also known as latent variables, x = (x_1 dots x_i dots x_n) We then define a model that factorizes over consituent smaller factors (functions), as f(yx) = prod_a f_a(y_ax_a)","category":"page"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"Individual factors may represent stochastic functions, such as conditional or prior distributions, but also potential functions or deterministic relationships. A factor may depend on multiple observed and/or latent variables (or none).","category":"page"},{"location":"library/bethe-free-energy/#lib-bethe-vfe","page":"Bethe Free Energy","title":"Variational Free Energy","text":"","category":"section"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"The Variational Free Energy (VFE) then defines a functional objective that includes the model and a variational distribution over the latent variables, Fq(haty) = mathbbE_q(x)leftlog fracq(x)f(y=haty x) right A functional defines a function of a function that returns a scalar. Here, the VFE is a function of the variational distribution (as indicated by square brackets) and returns a number.","category":"page"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"The VFE is also a function of the observed data, as indicated by round brackets, where the data are substituted in the factorized model.","category":"page"},{"location":"library/bethe-free-energy/#lib-bethe-variational-inference","page":"Bethe Free Energy","title":"Variational inference","text":"","category":"section"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"The goal of variational inference is to find a variational distibution that minimizes the VFE, q^*(x) = argmin_qinmathcalQ Fq(haty) This objective can be optimized (under specific constraints) with the use of variational calculus. Constraints are implied by the domain over which the variational distribution is optimized, and can be enforced by Lagrange multipliers.","category":"page"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"For the VFE, constraints enforce e.g. the normalization of the variational distribution. The variational distribution that minimizes the VFE then approximates the true (but often unobtainable) posterior distribution.","category":"page"},{"location":"library/bethe-free-energy/#lib-bethe-approximation","page":"Bethe Free Energy","title":"Bethe approximation","text":"","category":"section"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"Optimization of the VFE is still a daunting task, because the variational distribution is a joint distribution over possibly many latent variables. Instead of optimizing the joint variational distribution directly, a factorized variational distribution is often chosen. The factorized variational distribution is then optimized for its constituent factors.","category":"page"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"A popular choice of factorization is the Bethe approximation, which is constructed from the factorization of the model itself, q(x) triangleq fracprod_a q_a(x_a)prod_i q_i(x_i)^d_i - 1 The numerator iterates over the factors in the model, and carves the joint variational distribution in smaller variational distributions that are more manageable to optimize.","category":"page"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"The denominator of the Bethe approximation iterates over all individual latent variables and discounts them. The discounting factor is chosen as the degree of the variable minus one, where the degree counts the number of factors in which the variable appears.","category":"page"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"The Bethe approximation thus constrains the variational distribution to a factorized form. However, the true posterior distribution might not factorize in this way, e.g. if the grapical representation of the model contains cycles. In these cases the Bethe approximation trades the exact solution for computational tractability.","category":"page"},{"location":"library/bethe-free-energy/#lib-bethe-bfe","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"","category":"section"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"The Bethe Free Energy (BFE) substitutes the Bethe approximation in the VFE, which then fragments over factors and variables, as F_Bq(haty) = sum_a U_aq_a(haty_a) - sum_a Hq_a + sum_i (d_i - 1) Hq_i The first term of the BFE specifies an average energy,  U_aq_a(haty_a) = -mathbbE_q_a(x_a)leftlog f_a(y_a=haty_a x_a)right which internalizes the factors of the  model. The last two terms specify entropies.","category":"page"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"Crucially, the BFE can be iteratively optimized for each individual variational distribution in turn. Optimization of the BFE is thus more manageable than direct optimization of the VFE.","category":"page"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"For iterative optimization of the BFE, the variational distributions must first be initialized. The initmarginals keyword argument to the infer function initializes the variational distributions of the BFE.","category":"page"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"For disambiguation, note that the initialization of the variational distribution is a different design consideration than the choice of priors. A prior specifies a factor in the model definition, while initialization concerns factors in the variational distribution.","category":"page"},{"location":"library/bethe-free-energy/#lib-bethe-further-reading","page":"Bethe Free Energy","title":"Further reading","text":"","category":"section"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"Pearl (1986) on the original foundations of Bayesian networks and belief propagation;\nYedidia et al. (2005) on the connections between belief propagation and regional approximations to the VFE;\nDauwels (2007) on variational message passing on Forney-style factor graphs (FFGs);\nSenoz et al. (2021) on constraint manipulation and message passing on FFGs.","category":"page"},{"location":"library/bethe-free-energy/","page":"Bethe Free Energy","title":"Bethe Free Energy","text":"RxInfer.AbstractScoreObjective\nRxInfer.BetheFreeEnergy\nRxInfer.apply_diagnostic_check\nRxInfer.BetheFreeEnergyCheckNaNs\nRxInfer.BetheFreeEnergyCheckInfs","category":"page"},{"location":"library/bethe-free-energy/#RxInfer.AbstractScoreObjective","page":"Bethe Free Energy","title":"RxInfer.AbstractScoreObjective","text":"AbstractScoreObjective\n\nAbstract type for functional objectives that can be used in the score function. \n\nnote: Note\n\n\nscore is defined in the ReactiveMP package.\n\n\n\n\n\n","category":"type"},{"location":"library/bethe-free-energy/#RxInfer.BetheFreeEnergy","page":"Bethe Free Energy","title":"RxInfer.BetheFreeEnergy","text":"BetheFreeEnergy(marginal_skip_strategy, scheduler, diagnostic_checks)\n\nCreates Bethe Free Energy values stream when passed to the score function. \n\n\n\n\n\n","category":"type"},{"location":"library/bethe-free-energy/#RxInfer.apply_diagnostic_check","page":"Bethe Free Energy","title":"RxInfer.apply_diagnostic_check","text":"apply_diagnostic_check(check, context, stream)\n\nThis function applies a check to the stream. Accepts optional context object for custom error messages.\n\n\n\n\n\n","category":"function"},{"location":"library/bethe-free-energy/#RxInfer.BetheFreeEnergyCheckNaNs","page":"Bethe Free Energy","title":"RxInfer.BetheFreeEnergyCheckNaNs","text":"BetheFreeEnergyCheckNaNs\n\nIf enabled checks that both variable and factor bound score functions in Bethe Free Energy computation do not return NaNs.  Throws an error if finds NaN. \n\nSee also: BetheFreeEnergyCheckInfs\n\n\n\n\n\n","category":"type"},{"location":"library/bethe-free-energy/#RxInfer.BetheFreeEnergyCheckInfs","page":"Bethe Free Energy","title":"RxInfer.BetheFreeEnergyCheckInfs","text":"BetheFreeEnergyCheckInfs\n\nIf enabled checks that both variable and factor bound score functions in Bethe Free Energy computation do not return Infs.  Throws an error if finds Inf. \n\nSee also: BetheFreeEnergyCheckNaNs\n\n\n\n\n\n","category":"type"},{"location":"manuals/background/#intro-background-variational-inference","page":"Background: variational inference","title":"Background: variational inference","text":"","category":"section"}]
}

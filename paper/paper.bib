
@book{gelman_bayesian_2015,
	address = {New York},
	edition = {3},
	title = {Bayesian {Data} {Analysis}},
	isbn = {978-0-429-11307-9},
	abstract = {Winner of the 2016 De Groot Prize from the International Society for Bayesian AnalysisNow in its third edition, this classic book is widely considered the leading text on Bayesian methods, lauded for its accessible, practical approach to analyzing data and solving research problems. Bayesian Data Analysis, Third Edition continues to take an applied},
	publisher = {Chapman and Hall/CRC},
	author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
	month = jul,
	year = {2015},
	doi = {10.1201/b16018},
}

@article{salimans_markov_nodate,
	title = {Markov {Chain} {Monte} {Carlo} and {Variational} {Inference}:{Bridging} the {Gap}},
	abstract = {Recent advances in stochastic gradient variational inference have made it possible to perform variational Bayesian inference with posterior approximations containing auxiliary random variables. This enables us to explore a new synthesis of variational inference and Monte Carlo methods where we incorporate one or more steps of MCMC into our variational approximation. By doing so we obtain a rich class of inference algorithms bridging the gap between variational methods and MCMC, and offering the best of both worlds: fast posterior approximation through the maximization of an explicit objective, with the option of trading off additional computation for additional accuracy. We describe the theoretical foundations that make this possible and show some promising ﬁrst results.},
	language = {en},
	journal = {Bridging the Gap},
	author = {Salimans, Tim and Kingma, Diederik P and Welling, Max},
	pages = {9},
	file = {Salimans et al. - Markov Chain Monte Carlo and Variational Inference.pdf:/Users/apodusenko/Zotero/storage/5L2I7QSF/Salimans et al. - Markov Chain Monte Carlo and Variational Inference.pdf:application/pdf},
}

@article{blei_variational_2017,
	title = {Variational {Inference}: {A} {Review} for {Statisticians}},
	volume = {112},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Variational {Inference}},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1285773},
	doi = {10.1080/01621459.2017.1285773},
	language = {en},
	number = {518},
	urldate = {2018-06-22},
	journal = {Journal of the American Statistical Association},
	author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	month = apr,
	year = {2017},
	keywords = {Algorithms, Computationally intensive methods, Statistical computing},
	pages = {859--877},
	file = {Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf:/Users/apodusenko/Zotero/storage/9P6W2JF6/Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf:application/pdf;Full Text PDF:/Users/apodusenko/Zotero/storage/Z9UY3NBS/Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf:application/pdf},
}

@article{kucukelbir_automatic_2017,
	title = {Automatic {Differentiation} {Variational} {Inference}},
	volume = {18},
	url = {http://www.jmlr.org/papers/volume18/16-107/16-107.pdf},
	number = {1},
	journal = {Journal of Machine Learning Research},
	author = {Kucukelbir, Alp and Tran, Dustin and Ranganath, Rajesh and Gelman, Andrew and Blei, David M.},
	year = {2017},
	pages = {430--474},
	file = {Kucukelbir et al. - 2017 - Automatic differentiation variational inference.pdf:/Users/apodusenko/Zotero/storage/UIHNPM6P/Kucukelbir et al. - 2017 - Automatic differentiation variational inference.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/DJMG2NFZ/Kucukelbir e.a. - 2017 - Automatic differentiation variational inference.pdf:application/pdf},
}

@article{bamler_structured_2017,
	title = {Structured {Black} {Box} {Variational} {Inference} for {Latent} {Time} {Series} {Models}},
	url = {http://arxiv.org/abs/1707.01069},
	abstract = {Continuous latent time series models are prevalent in Bayesian modeling; examples include the Kalman filter, dynamic collaborative filtering, or dynamic topic models. These models often benefit from structured, non mean field variational approximations that capture correlations between time steps. Black box variational inference with reparameterization gradients (BBVI) allows us to explore a rich new class of Bayesian non-conjugate latent time series models; however, a naive application of BBVI to such structured variational models would scale quadratically in the number of time steps. We describe a BBVI algorithm analogous to the forward-backward algorithm which instead scales linearly in time. It allows us to efficiently sample from the variational distribution and estimate the gradients of the ELBO. Finally, we show results on the recently proposed dynamic word embedding model, which was trained using our method.},
	urldate = {2018-11-29},
	journal = {arXiv:1707.01069 [cs, stat]},
	author = {Bamler, Robert and Mandt, Stephan},
	month = jul,
	year = {2017},
	note = {arXiv: 1707.01069},
	keywords = {Computer Science - Learning, Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/apodusenko/Zotero/storage/QKRLUYQ7/1707.html:text/html;arXiv.org Snapshot:/Users/apodusenko/Zotero/storage/6GVGQXI5/1707.html:text/html;Bamler and Mandt - 2017 - Structured Black Box Variational Inference for Lat.pdf:/Users/apodusenko/Zotero/storage/JZQ9Z3UM/Bamler and Mandt - 2017 - Structured Black Box Variational Inference for Lat.pdf:application/pdf;Bamler and Mandt - 2017 - Structured Black Box Variational Inference for Lat.pdf:/Users/apodusenko/Zotero/storage/YUYTYFJK/Bamler and Mandt - 2017 - Structured Black Box Variational Inference for Lat.pdf:application/pdf},
}

@article{bezanson_julia_2017,
	title = {Julia: {A} {Fresh} {Approach} to {Numerical} {Computing}},
	volume = {59},
	issn = {0036-1445},
	shorttitle = {Julia},
	url = {https://epubs.siam.org/doi/abs/10.1137/141000671},
	doi = {10.1137/141000671},
	abstract = {Bridging cultures that have often been distant, Julia combines expertise from the diverse fields of computer science and computational science to create a new approach to numerical  computing. Julia is  designed to be easy and fast and questions notions generally held to be “laws of nature"  by practitioners of numerical computing: {\textbackslash}beginlist {\textbackslash}item  High-level dynamic programs have to be slow. {\textbackslash}item  One must prototype in one language and then rewrite in another language for speed or deployment. {\textbackslash}item There are parts of a system appropriate for the programmer, and other parts that are best left untouched as they have been built by the experts. {\textbackslash}endlist We introduce the  Julia programming language and its design---a  dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch,  a  technique from computer science, picks  the right algorithm for the right circumstance. Abstraction, which is what good computation is really about, recognizes what remains the same after differences are stripped away. Abstractions in mathematics are captured as code through another technique from computer science, generic programming. Julia shows that  one can achieve machine performance without sacrificing human convenience.},
	number = {1},
	urldate = {2018-04-10},
	journal = {SIAM Review},
	author = {Bezanson, J. and Edelman, A. and Karpinski, S. and Shah, V.},
	month = jan,
	year = {2017},
	pages = {65--98},
	file = {Bezanson et al. - 2017 - Julia A Fresh Approach to Numerical Computing.pdf:/Users/apodusenko/Zotero/storage/GS7LQE5E/Bezanson et al. - 2017 - Julia A Fresh Approach to Numerical Computing.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/EI9VN2TA/141000671.html:text/html},
}

@article{bezanson_julia_2012,
	title = {Julia: {A} {Fast} {Dynamic} {Language} for {Technical} {Computing}},
	shorttitle = {Julia},
	url = {http://arxiv.org/abs/1209.5145},
	abstract = {Dynamic languages have become popular for scientific computing. They are generally considered highly productive, but lacking in performance. This paper presents Julia, a new dynamic language for technical computing, designed for performance from the beginning by adapting and extending modern programming language techniques. A design based on generic functions and a rich type system simultaneously enables an expressive programming model and successful type inference, leading to good performance for a wide range of programs. This makes it possible for much of the Julia library to be written in Julia itself, while also incorporating best-of-breed C and Fortran libraries.},
	urldate = {2018-11-27},
	journal = {arXiv:1209.5145 [cs]},
	author = {Bezanson, Jeff and Karpinski, Stefan and Shah, Viral B. and Edelman, Alan},
	month = sep,
	year = {2012},
	note = {arXiv: 1209.5145},
	keywords = {Computer Science - Programming Languages, Computer Science - Computational Engineering, Finance, and Science, D.3.2},
	file = {arXiv.org Snapshot:/Users/apodusenko/Zotero/storage/V6P4IU54/1209.html:text/html;Bezanson et al. - 2012 - Julia A Fast Dynamic Language for Technical Compu.pdf:/Users/apodusenko/Zotero/storage/4Q9THC8V/Bezanson et al. - 2012 - Julia A Fast Dynamic Language for Technical Compu.pdf:application/pdf},
}

@article{bagaev_reactive_2021,
	title = {Reactive {Message} {Passing} for {Scalable} {Bayesian} {Inference}},
	url = {http://arxiv.org/abs/2112.13251},
	abstract = {We introduce Reactive Message Passing (RMP) as a framework for executing schedule-free, robust and scalable message passing-based inference in a factor graph representation of a probabilistic model. RMP is based on the reactive programming style that only describes how nodes in a factor graph react to changes in connected nodes. The absence of a fixed message passing schedule improves robustness, scalability and execution time of the inference procedure. We also present ReactiveMP.jl, which is a Julia package for realizing RMP through minimization of a constrained Bethe free energy. By user-defined specification of local form and factorization constraints on the variational posterior distribution, ReactiveMP.jl executes hybrid message passing algorithms including belief propagation, variational message passing, expectation propagation, and expectation maximisation update rules. Experimental results demonstrate the improved performance of ReactiveMP-based RMP in comparison to other Julia packages for Bayesian inference across a range of probabilistic models. In particular, we show that the RMP framework is able to run Bayesian inference for large-scale probabilistic state space models with hundreds of thousands of random variables on a standard laptop computer.},
	urldate = {2022-01-04},
	journal = {arXiv:2112.13251 [cs]},
	author = {Bagaev, Dmitry and de Vries, Bert},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.13251},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/UKKDFCW5/Bagaev and de Vries - 2021 - Reactive Message Passing for Scalable Bayesian Inf.pdf:application/pdf;arXiv.org Snapshot:/Users/apodusenko/Zotero/storage/V3SNIXX9/2112.html:text/html},
}

@article{podusenko_message_2021-1,
	title = {Message {Passing}-{Based} {Inference} for {Time}-{Varying} {Autoregressive} {Models}},
	volume = {23},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1099-4300/23/6/683},
	doi = {10.3390/e23060683},
	abstract = {Time-varying autoregressive (TVAR) models are widely used for modeling of non-stationary signals. Unfortunately, online joint adaptation of both states and parameters in these models remains a challenge. In this paper, we represent the TVAR model by a factor graph and solve the inference problem by automated message passing-based inference for states and parameters. We derive structured variational update rules for a composite “AR node” with probabilistic observations that can be used as a plug-in module in hierarchical models, for example, to model the time-varying behavior of the hyper-parameters of a time-varying AR model. Our method includes tracking of variational free energy (FE) as a Bayesian measure of TVAR model performance. The proposed methods are verified on a synthetic data set and validated on real-world data from temperature modeling and speech enhancement tasks.},
	language = {en},
	number = {6},
	urldate = {2021-07-07},
	journal = {Entropy},
	author = {Podusenko, Albert and Kouw, Wouter M. and de Vries, Bert},
	month = jun,
	year = {2021},
	note = {Number: 6
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Bayesian inference, free energy, factor graph, model selection, hybrid message passing, non-stationary systems, probabilistic graphical models},
	pages = {683},
	file = {Podusenko et al. - 2021 - Message Passing-Based Inference for Time-Varying A.html:/Users/apodusenko/Zotero/storage/DSEAPYBR/Podusenko et al. - 2021 - Message Passing-Based Inference for Time-Varying A.html:text/html;Podusenko et al. - 2021 - Message Passing-Based Inference for Time-Varying A.pdf:/Users/apodusenko/Zotero/storage/R74TNLN2/Podusenko et al. - 2021 - Message Passing-Based Inference for Time-Varying A.pdf:application/pdf},
}

@article{senoz_variational_2021,
	title = {Variational {Message} {Passing} and {Local} {Constraint} {Manipulation} in {Factor} {Graphs}},
	volume = {23},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1099-4300},
	url = {https://www.mdpi.com/1099-4300/23/7/807},
	doi = {10.3390/e23070807},
	abstract = {Accurate evaluation of Bayesian model evidence for a given data set is a fundamental problem in model development. Since evidence evaluations are usually intractable, in practice variational free energy (VFE) minimization provides an attractive alternative, as the VFE is an upper bound on negative model log-evidence (NLE). In order to improve tractability of the VFE, it is common to manipulate the constraints in the search space for the posterior distribution of the latent variables. Unfortunately, constraint manipulation may also lead to a less accurate estimate of the NLE. Thus, constraint manipulation implies an engineering trade-off between tractability and accuracy of model evidence estimation. In this paper, we develop a unifying account of constraint manipulation for variational inference in models that can be represented by a (Forney-style) factor graph, for which we identify the Bethe Free Energy as an approximation to the VFE. We derive well-known message passing algorithms from first principles, as the result of minimizing the constrained Bethe Free Energy (BFE). The proposed method supports evaluation of the BFE in factor graphs for model scoring and development of new message passing-based inference algorithms that potentially improve evidence estimation accuracy.},
	language = {en},
	number = {7},
	urldate = {2022-03-15},
	journal = {Entropy},
	author = {Şenöz, İsmail and van de Laar, Thijs and Bagaev, Dmitry and de Vries, Bert},
	month = jul,
	year = {2021},
	note = {Number: 7
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {Bayesian inference, Bethe free energy, factor graphs, message passing, variational inference, variational message passing, variational free energy},
	pages = {807},
	file = {Full Text PDF:/Users/apodusenko/Zotero/storage/LA33ZE54/Şenöz et al. - 2021 - Variational Message Passing and Local Constraint M.pdf:application/pdf;Snapshot:/Users/apodusenko/Zotero/storage/GHY4TMAQ/807.html:text/html},
}

@article{podusenko_aida_2021,
	title = {{AIDA}: {An} {Active} {Inference}-based {Design} {Agent} for {Audio} {Processing} {Algorithms}},
	shorttitle = {{AIDA}},
	url = {http://arxiv.org/abs/2112.13366},
	abstract = {In this paper we present AIDA, which is an active inference-based agent that iteratively designs a personalized audio processing algorithm through situated interactions with a human client. The target application of AIDA is to propose on-the-spot the most interesting alternative values for the tuning parameters of a hearing aid (HA) algorithm, whenever a HA client is not satisfied with their HA performance. AIDA interprets searching for the "most interesting alternative" as an issue of optimal (acoustic) context-aware Bayesian trial design. In computational terms, AIDA is realized as an active inference-based agent with an Expected Free Energy criterion for trial design. This type of architecture is inspired by neuro-economic models on efficient (Bayesian) trial design in brains and implies that AIDA comprises generative probabilistic models for acoustic signals and user responses. We propose a novel generative model for acoustic signals as a sum of time-varying auto-regressive filters and a user response model based on a Gaussian Process Classifier. The full AIDA agent has been implemented in a factor graph for the generative model and all tasks (parameter learning, acoustic context classification, trial design, etc.) are realized by variational message passing on the factor graph. All verification and validation experiments and demonstrations are freely accessible at our GitHub repository.},
	urldate = {2022-01-04},
	journal = {arXiv:2112.13366 [cs, eess, stat]},
	author = {Podusenko, Albert and van Erp, Bart and Koudahl, Magnus and de Vries, Bert},
	month = dec,
	year = {2021},
	note = {arXiv: 2112.13366},
	keywords = {Computer Science - Sound, Statistics - Machine Learning, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/apodusenko/Zotero/storage/37WCIF5W/Podusenko et al. - 2021 - AIDA An Active Inference-based Design Agent for A.pdf:application/pdf;arXiv.org Snapshot:/Users/apodusenko/Zotero/storage/KAHLFMQ2/2112.html:text/html;Full Text PDF:/Users/apodusenko/Zotero/storage/ZNXL3Q74/Podusenko et al. - 2022 - AIDA An Active Inference-Based Design Agent for A.pdf:application/pdf},
}

@inproceedings{podusenko_message_2021,
	address = {Gold Coast, Australia},
	title = {Message {Passing}-{Based} {Inference} in the {Gamma} {Mixture} {Model}},
	isbn = {978-1-72816-338-3},
	url = {https://ieeexplore.ieee.org/document/9596329/},
	doi = {10.1109/MLSP52302.2021.9596329},
	urldate = {2021-11-29},
	booktitle = {2021 {IEEE} 31st {International} {Workshop} on {Machine} {Learning} for {Signal} {Processing} ({MLSP})},
	publisher = {IEEE},
	author = {Podusenko, Albert and van Erp, Bart and Bagaev, Dmitry and {Şenöz, İsmail} and de Vries, Bert},
	month = oct,
	year = {2021},
	keywords = {Machine learning, Probability distribution, Conferences, Probabilistic logic, Signal processing, Mathematical models, Factor Graphs, Expectation-Maximization, Gamma Mixture Model, Message Passing, Mixture models, Moment Matching, Probabilistic Inference},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/Users/apodusenko/Zotero/storage/YLX9ABX2/9596329.html:text/html;IEEE Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/4IBDHHV7/Podusenko et al. - 2021 - Message Passing-Based Inference in the Gamma Mixtu.pdf:application/pdf},
}

@inproceedings{podusenko_message_2022,
	address = {Belgrade, Serbia},
	title = {Message {Passing}-based {Inference} in {Switching} {Autoregressive} {Models}},
	abstract = {The switching autoregressive model is a flexible model for signals generated by non-stationary processes. Unfortunately, evaluation of the exact posterior distributions of the latent variables for a switching autoregressive model is analytically intractable, and this limits the applicability of switching autoregressive models in practical signal processing tasks. In this paper we present a message passing-based approach for computing approximate posterior distributions in the switching autoregressive model. Our solution tracks approximate posterior distributions in a modular way and easily extends to more complicated model variations. The proposed message passing algorithm is verified and validated on synthetic and acoustic data sets respectively.},
	booktitle = {2022 30th {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
	author = {Podusenko, Albert and van Erp, Bart and Bagaev, Dmitry and şenöz, Ïsmail and de Vries, Bert},
	month = aug,
	year = {2022},
	note = {ISSN: 2076-1465},
	keywords = {Europe, Message passing, Signal processing algorithms, Switches, Signal processing, Computational modeling, Analytical models, Message Passing, Variational Inference, State Estimation, Switching Autoregressive Models},
	pages = {1497--1501},
	file = {IEEE Xplore Abstract Record:/Users/apodusenko/Zotero/storage/P6M3S9I8/9909828.html:text/html;IEEE Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/LJVREVG4/Podusenko et al. - 2022 - Message Passing-based Inference in Switching Autor.pdf:application/pdf},
}

@inproceedings{van_erp_hybrid_2022,
	address = {Belgrade, Serbia},
	title = {Hybrid {Inference} with {Invertible} {Neural} {Networks} in {Factor} {Graphs}},
	abstract = {This paper bridges the gap in the literature between neural networks and probabilistic graphical models. Invertible neural networks are incorporated in factor graphs and inference in this model is described by linearization of the network. Consequently, hybrid probabilistic inference in the model is realized through message passing with local constraints on the Bethe free energy. We provide the local Bethe free energy for the invertible neural network node, which allows for evaluation of the performance of the entire probabilistic model. Experimental results show effective hybrid inference in a neural network-based probabilistic model for a binary classification task, paving the way towards a novel class of machine learning models.},
	booktitle = {2022 30th {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
	publisher = {IEEE},
	author = {van Erp, Bart and de Vries, Bert},
	month = aug,
	year = {2022},
	note = {ISSN: 2076-1465},
	keywords = {Machine learning, Machine learning algorithms, Europe, Message passing, Graphical models, Neural networks, Signal processing, Factor Graphs, Message Passing, Bethe Free Energy, Hybrid Inference, Invertible Neural Networks},
	pages = {1397--1401},
	file = {IEEE Xplore Abstract Record:/Users/apodusenko/Zotero/storage/RB8RP3NS/9909873.html:text/html;IEEE Xplore Full Text PDF:/Users/apodusenko/Zotero/storage/PCZPVBKB/van Erp and de Vries - 2022 - Hybrid Inference with Invertible Neural Networks i.pdf:application/pdf},
}

@inproceedings{nguyen_efficient_2022,
	address = {Rennes, France},
	title = {Efficient {Model} {Evidence} {Computation} in {Tree}-structured {Factor} {Graphs}},
	abstract = {Model evidence is a fundamental performance measure in Bayesian machine learning as it represents how well a model fits an observed data set. Since model evidence is often an intractable quantity, the literature often resorts to computing instead the Bethe Free Energy (BFE), which for cyclefree models is a tractable upper bound on the (negative log-) model evidence. In this paper, we propose a different and faster evidence computation approach by tracking local normalization constants of sum-product messages, termed scale factors. We tabulate scale factor update rules for various elementary factor nodes and by experimental validation we verify the correctness of these update rules for models involving both discrete and continuous variables. We show how tracking scale factors leads to performance improvements compared to the traditional BFE computation approach.},
	language = {en},
	booktitle = {2022 {IEEE} {Workshop} on {Signal} {Processing} {Systems} ({SiPS})},
	author = {Nguyen, Hoang M H and van Erp, Bart and Senoz, Ismail and de Vries, Bert},
	year = {2022},
	note = {in press},
	pages = {6},
	file = {Nguyen et al. - 2022 - Efficient Model Evidence Computation in Tree-struc.pdf:/Users/apodusenko/Zotero/storage/ANGCDZER/Nguyen et al. - 2022 - Efficient Model Evidence Computation in Tree-struc.pdf:application/pdf},
}

@misc{bagaev_dmitry_reactivempjl_2021,
	title = {{ReactiveMP}.jl: a {Julia} package for automatic {Bayesian} inference on a factor graph with reactive message passing.},
	copyright = {MIT License, Open Access},
	shorttitle = {{ReactiveMP}.jl},
	url = {https://zenodo.org/record/6365000},
	abstract = {ReactiveMP v1.3.2 Diff since v1.3.1 {\textless}strong{\textgreater}Closed issues:{\textless}/strong{\textgreater} {\textless}code{\textgreater}+{\textless}/code{\textgreater} node should use {\textless}code{\textgreater}convolve{\textless}/code{\textgreater} from Distributions.jl (\#72) Use safe domains for point mass constraints optimisations (\#83) {\textless}strong{\textgreater}Merged pull requests:{\textless}/strong{\textgreater} feat(): Add boundaries specification to point mass form constraint (\#85) (@bvdmitri) Add mean-field rules for AR node (\#86) (@albertpod) fix(): fix AbstractFloat constructors for tiny and huge (\#87) (@bvdmitri) Fix Flow tutorial and demo in the documentation (2prev PR) (\#88) (@bvdmitri) fix(): fix equality node cache computation bug (\#89) (@bvdmitri)},
	urldate = {2022-04-05},
	publisher = {Zenodo},
	author = {Bagaev, Dmitry},
	month = jan,
	year = {2021},
	doi = {10.5281/ZENODO.6365000},
	keywords = {Factor Graphs, Message Passing, Bayesian Inference, Reactive Programming, Variational Inference},
}

@book{sarkka_bayesian_2013,
	address = {London ; New York},
	title = {Bayesian {Filtering} and {Smoothing}},
	isbn = {978-0-415-55809-9},
	abstract = {Filtering and smoothing methods are used to produce an accurate estimate of the state of a time-varying system based on multiple observational inputs (data). Interest in these methods has exploded in recent years, with numerous applications emerging in fields such as navigation, aerospace engineering, telecommunications and medicine.   This compact, informal introduction for graduate students and advanced undergraduates presents the current state-of-the-art filtering and smoothing methods in a unified Bayesian framework. Readers learn what non-linear Kalman filters and particle filters are, how they are related, and their relative advantages and disadvantages. They also discover how state-of-the-art Bayesian parameter estimation methods can be combined with state-of-the-art filtering and smoothing algorithms.   The book's practical and algorithmic approach assumes only modest mathematical prerequisites. Examples include MATLAB computations, and the numerous end-of-chapter exercises include computational assignments. MATLAB/GNU Octave source code is available for download at www.cambridge.org/sarkka, promoting hands-on work with the methods.},
	language = {English},
	publisher = {Cambridge University Press},
	author = {Särkkä, Simo},
	month = oct,
	year = {2013},
	file = {Särkkä - 2013 - Bayesian Filtering and Smoothing.pdf:/Users/apodusenko/Zotero/storage/7TREJ8JV/Särkkä - 2013 - Bayesian Filtering and Smoothing.pdf:application/pdf;Särkkä - 2013 - Bayesian Filtering and Smoothing.pdf:/Users/apodusenko/Zotero/storage/Y3KKQKCV/Särkkä - 2013 - Bayesian Filtering and Smoothing.pdf:application/pdf;Särkkä - 2013 - Bayesian Filtering and Smoothing.pdf:/Users/apodusenko/Zotero/storage/MZEVBGYI/Särkkä - 2013 - Bayesian Filtering and Smoothing.pdf:application/pdf},
}

@article{AKBAYRAK2022235,
    title = {Probabilistic programming with stochastic variational message passing},
    journal = {International Journal of Approximate Reasoning},
    volume = {148},
    pages = {235-252},
    year = {2022},
    issn = {0888-613X},
    doi = {https://doi.org/10.1016/j.ijar.2022.06.006},
    url = {https://www.sciencedirect.com/science/article/pii/S0888613X22000950},
    author = {Semih Akbayrak and İsmail Şenöz and Alp Sarı and Bert {de Vries}},
    keywords = {Factor graphs, Message passing, Natural gradient descent, Probabilistic programming, Variational inference},
    abstract = {Stochastic approximation methods for variational inference have recently gained popularity in the probabilistic programming community since these methods are amenable to automation and allow online, scalable, and universal approximate Bayesian inference. Unfortunately, common Probabilistic Programming Languages (PPLs) with stochastic approximation engines lack the efficiency of message passing-based inference algorithms with deterministic update rules such as Belief Propagation (BP) and Variational Message Passing (VMP). Still, Stochastic Variational Inference (SVI) and Conjugate-Computation Variational Inference (CVI) provide principled methods to integrate fast deterministic inference techniques with broadly applicable stochastic approximate inference. Unfortunately, implementation of SVI and CVI necessitates manually driven variational update rules, which does not yet exist in most PPLs. In this paper, we cast SVI and CVI explicitly in a message passing-based inference context. We provide an implementation for SVI and CVI in ForneyLab, which is an automated message passing-based probabilistic programming package in the open source Julia language. Through a number of experiments, we demonstrate how SVI and CVI extends the automated inference capabilities of message passing-based probabilistic programming.}
}

@article{revels_forward-mode_2016,
	title = {Forward-{Mode} {Automatic} {Differentiation} in {Julia}},
	url = {http://arxiv.org/abs/1607.07892},
	abstract = {We present ForwardDiff, a Julia package for forward-mode automatic differentiation (AD) featuring performance competitive with low-level languages like C++. Unlike recently developed AD tools in other popular high-level languages such as Python and MATLAB, ForwardDiff takes advantage of just-in-time (JIT) compilation to transparently recompile AD-unaware user code, enabling efficient support for higher-order differentiation and differentiation using custom number types (including complex numbers). For gradient and Jacobian calculations, ForwardDiff provides a variant of vector-forward mode that avoids expensive heap allocation and makes better use of memory bandwidth than traditional vector mode. In our numerical experiments, we demonstrate that for nontrivially large dimensions, ForwardDiff's gradient computations can be faster than a reverse-mode implementation from the Python-based autograd package. We also illustrate how ForwardDiff is used effectively within JuMP, a modeling language for optimization. According to our usage statistics, 41 unique repositories on GitHub depend on ForwardDiff, with users from diverse fields such as astronomy, optimization, finite element analysis, and statistics. This document is an extended abstract that has been accepted for presentation at the AD2016 7th International Conference on Algorithmic Differentiation.},
	urldate = {2018-07-22},
	journal = {arXiv:1607.07892 [cs]},
	author = {Revels, Jarrett and Lubin, Miles and Papamarkou, Theodore},
	month = jul,
	year = {2016},
	note = {arXiv: 1607.07892},
	keywords = {Computer Science - Mathematical Software},
	file = {arXiv.org Snapshot:/Users/apodusenko/Zotero/storage/VU44UU6E/1607.html:text/html;Revels et al. - 2016 - Forward-Mode Automatic Differentiation in Julia.pdf:/Users/apodusenko/Zotero/storage/5NLMWUVP/Revels et al. - 2016 - Forward-Mode Automatic Differentiation in Julia.pdf:application/pdf},
}

@misc{hoffman_nuts,
  doi = {10.48550/ARXIV.1111.4246},
  url = {https://arxiv.org/abs/1111.4246},
  author = {Hoffman, Matthew D. and Gelman, Andrew},
  keywords = {Computation (stat.CO), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo},
  publisher = {arXiv},
  year = {2011},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@book{hmc_ref_2011,
	doi = {10.1201/b10905},
	url = {https://doi.org/10.1201%2Fb10905},
	year = 2011,
	month = {may},
	publisher = {Chapman and Hall/{CRC}},
	editor = {Steve Brooks and Andrew Gelman and Galin Jones and Xiao-Li Meng},
	title = {Handbook of Markov Chain Monte Carlo}
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41e47440",
   "metadata": {},
   "source": [
    "# Active Inference Mountain car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbc7485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg; Pkg.activate(\"..\"); Pkg.instantiate();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3fda93",
   "metadata": {},
   "outputs": [],
   "source": [
    "using RxInfer, Plots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cd8fd30",
   "metadata": {},
   "source": [
    "A group of friends is going to a camping site that is located on the biggest mountain in the Netherlands. They use an electric car for the trip. When they are almost there, the car's battery is almost empty and is therefore limiting the engine force. Unfortunately, they are in the middle of a valley and don't have enough power to reach the camping site. Night is falling and they still need to reach the top of the mountain. As rescuers, let us develop an Active Inference (AI) agent that can get them up the hill with the limited engine power."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a3237b9",
   "metadata": {},
   "source": [
    "## The environmental process of the mountain\n",
    "Firstly, we specify the environmental process according to Ueltzhoeffer (2017) \"Deep active inference\". This process shows how the environment evolves after interacting with the agent.\n",
    "\n",
    "Particularly, let's denote $z_t = (\\phi_t, \\,\\,\\dot{\\phi_t})$ as the environmental state depending on the position $\\phi_t$ and velocity $\\dot{\\phi_t}$ of the car; $a_t$ as the action of the environment on the car. Then the evolution of the state is described as follows  \n",
    "\n",
    "$$\\begin{aligned} \n",
    "\\dot{\\phi_t} &= \\dot{\\phi}_{t-1} + F_g(\\phi_{t-1}) + F_f(\\dot{\\phi}_{t-1}) + F_a(a_t)\\\\\n",
    "\\phi_t &= \\phi_{t-1} + \\dot{\\phi_t} \n",
    "\\end{aligned}$$\n",
    "\n",
    "where $F_g(\\phi_{t-1})$ is the gravitational force of the hill landscape that depends on the car's position\n",
    "\n",
    "$$F_g(\\phi) = \\begin{cases}\n",
    "        -0.05(2\\phi + 1) , \\, & \\mathrm{if} \\, \\phi < 0 \\\\\n",
    "        -0.05 \\left[(1 + 5\\phi^2)^{-\\frac{1}{2}} + \\phi^2 (1 + 5\\phi^2)^{-\\frac{3}{2}} + \\frac{1}{16}\\phi^4 \\right], \\, & \\mathrm{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "$F_f(\\dot{\\phi})$ is the friction on the car defined through the car's velocity $$F_f(\\dot{\\phi})  = -0.1 \\, \\dot{\\phi}\\,$$ and $F_a(a)$ is the engine force $$F_a(a) = 0.04 \\,\\tanh(a).$$\n",
    "Since the car is on low battery, we use the $\\tanh(\\cdot)$ function to limit the engine force to the interval [-0.04, 0.04].\n",
    "\n",
    "In the cell below, the ```create_physics``` function defines forces $F_g,\\, F_f,\\, F_a\\,$; and the ```create_world``` function defines the environmental process of the mountain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49986f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import HypergeometricFunctions: _₂F₁\n",
    "\n",
    "function create_physics(; engine_force_limit = 0.04, friction_coefficient = 0.1)\n",
    "    # Engine force as function of action\n",
    "    Fa = (a::Real) -> engine_force_limit * tanh(a) \n",
    "\n",
    "    # Friction force as function of velocity\n",
    "    Ff = (y_dot::Real) -> -friction_coefficient * y_dot \n",
    "    \n",
    "    # Gravitational force (horizontal component) as function of position\n",
    "    Fg = (y::Real) -> begin\n",
    "        if y < 0\n",
    "            0.05*(-2*y - 1)\n",
    "        else\n",
    "            0.05*(-(1 + 5*y^2)^(-0.5) - (y^2)*(1 + 5*y^2)^(-3/2) - (y^4)/16)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # The height of the landscape as a function of the horizontal coordinate\n",
    "    height = (x::Float64) -> begin\n",
    "        if x < 0\n",
    "            h = x^2 + x\n",
    "        else\n",
    "            h = x * _₂F₁(0.5,0.5,1.5, -5*x^2) + x^3 * _₂F₁(1.5, 1.5, 2.5, -5*x^2) / 3 + x^5 / 80\n",
    "        end\n",
    "        return 0.05*h\n",
    "    end\n",
    "\n",
    "    return (Fa, Ff, Fg,height)\n",
    "end;\n",
    "\n",
    "function create_world(; Fg, Ff, Fa, initial_position = -0.5, initial_velocity = 0.0)\n",
    "\n",
    "    y_t_min = initial_position\n",
    "    y_dot_t_min = initial_velocity\n",
    "    \n",
    "    y_t = y_t_min\n",
    "    y_dot_t = y_dot_t_min\n",
    "    \n",
    "    execute = (a_t::Float64) -> begin\n",
    "        # Compute next state\n",
    "        y_dot_t = y_dot_t_min + Fg(y_t_min) + Ff(y_dot_t_min) + Fa(a_t)\n",
    "        y_t = y_t_min + y_dot_t\n",
    "    \n",
    "        # Reset state for next step\n",
    "        y_t_min = y_t\n",
    "        y_dot_t_min = y_dot_t\n",
    "    end\n",
    "    \n",
    "    observe = () -> begin \n",
    "        return [y_t, y_dot_t]\n",
    "    end\n",
    "        \n",
    "    return (execute, observe)\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82005540",
   "metadata": {},
   "source": [
    "Let's visualize the mountain landscape and the situation of the car. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a3026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_force_limit   = 0.04\n",
    "friction_coefficient = 0.1\n",
    "\n",
    "Fa, Ff, Fg, height = create_physics(\n",
    "    engine_force_limit = engine_force_limit,\n",
    "    friction_coefficient = friction_coefficient\n",
    ");\n",
    "initial_position = -0.5\n",
    "initial_velocity = 0.0\n",
    "\n",
    "x_target = [0.5, 0.0] \n",
    "\n",
    "valley_x = range(-2, 2, length=400)\n",
    "valley_y = [ height(xs) for xs in valley_x ]\n",
    "plot(valley_x, valley_y, title = \"Mountain valley\", label = \"Landscape\", color = \"black\")\n",
    "scatter!([ initial_position ], [ height(initial_position) ], label=\"initial car position\")   \n",
    "scatter!([x_target[1]], [height(x_target[1])], label=\"camping site\")   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "760c136a",
   "metadata": {},
   "source": [
    "## Naive approach\n",
    "\n",
    "Well, let's see how our friends were struggling with the low-battery car when they tried to get it to the camping site before we come to help. They basically used the brute-force method, i.e. just pushing the gas pedal for full power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e7940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_naive  = 100 # Total simulation time\n",
    "pi_naive = 100.0 * ones(N_naive) # Naive policy for right full-power only\n",
    "\n",
    "# Let there be a world\n",
    "(execute_naive, observe_naive) = create_world(; \n",
    "    Fg = Fg, Ff = Ff, Fa = Fa, \n",
    "    initial_position = initial_position, \n",
    "    initial_velocity = initial_velocity\n",
    ");\n",
    "\n",
    "y_naive = Vector{Vector{Float64}}(undef, N_naive)\n",
    "for t = 1:N_naive\n",
    "    execute_naive(pi_naive[t]) # Execute environmental process\n",
    "    y_naive[t] = observe_naive() # Observe external states\n",
    "end\n",
    "\n",
    "animation_naive = @animate for i in 1:N_naive\n",
    "    plot(valley_x, valley_y, title = \"Naive policy\", label = \"Landscape\", color = \"black\", size = (800, 400))\n",
    "    scatter!([y_naive[i][1]], [height(y_naive[i][1])], label=\"car\")\n",
    "    scatter!([x_target[1]], [height(x_target[1])], label=\"goal\")   \n",
    "end\n",
    "\n",
    "# The animation is saved and displayed as markdown picture for the automatic HTML generation\n",
    "gif(animation_naive, \"../pics/ai-mountain-car-naive.gif\", fps = 24, show_msg = false);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8289a8e8",
   "metadata": {},
   "source": [
    "![](../pics/ai-mountain-car-naive.gif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e698ce2e",
   "metadata": {},
   "source": [
    "They failed as expected since the car doesn't have enough power. This helps to understand that the brute-force approach is not the most efficient one in this case and hopefully a bit of swinging is necessary to achieve the goal."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e8b32e6",
   "metadata": {},
   "source": [
    "# Active inference approach\n",
    "\n",
    "Now let's help them solve the problem with an active inference approach. Particularly, we create an agent that predicts the future car position as well as the best possible actions in a probabilistic manner.\n",
    "\n",
    "We start by specifying a probabilistic model for the agent that describes the agent's internal beliefs over the external dynamics of the environment. The generative model is defined as follows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2490683f",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}\n",
    "p_t(x,s,u) \\propto p(s_{t-1}) \\prod_{k=t}^{t+T} p(x_k \\mid s_k) \\, p(s_k \\mid s_{k-1},u_k) \\, p(u_k) \\, p'(x_k) \\nonumber\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4e6a501",
   "metadata": {},
   "source": [
    "where the factors are defined as"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3cf28237",
   "metadata": {},
   "source": [
    "$$p'(x_k) = \\mathcal{N}(x_k \\mid x_{goal},\\,V_{goal}) , \\quad (\\mathrm{target})$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d95e84be",
   "metadata": {},
   "source": [
    "$$p(s_k \\mid s_{k-1},u_k) = \\mathcal{N}(s_k \\mid \\tilde{g}(s_{k-1})+h(u_k),\\,\\gamma^{-1}) , \\quad (\\mathrm{state \\,\\, transition})$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6cdddf6c",
   "metadata": {},
   "source": [
    "$$p(x_k \\mid s_k) = \\mathcal{N}(x_k \\mid s_k,\\,\\theta), \\quad (\\mathrm{observation})$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7810eb6f",
   "metadata": {},
   "source": [
    "$$p(u_k) = \\mathcal{N}(u_k \\mid m_u,\\,V_u), \\quad (\\mathrm{control})$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98849452",
   "metadata": {},
   "source": [
    "$$p(s_{t-1}) = \\mathcal{N}(s_{t-1} \\mid m_{t-1},\\,V_{t-1}), \\quad (\\mathrm{previous \\,\\, state})$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f0f5017",
   "metadata": {},
   "source": [
    "where \n",
    "- $x$ denotes observations of the agent after interacting with the environment; \n",
    "- $s_t = (s_t,\\dot{s_t})$ is the state of the car embodying its position and velocity; \n",
    "- $u_t$ denotes the control state of the agent; \n",
    "- $h(\\cdot)$ is the $\\tanh(\\cdot)$ function modeling engine control; \n",
    "- $\\tilde{g}(\\cdot)$ executes a linear approximation of equations (1) and (2): \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63fae093",
   "metadata": {},
   "source": [
    "$$\\begin{aligned} \n",
    "\\dot{s_t} &= \\dot{s}_{t-1} + F_g(s_{t-1}) + F_f(\\dot{s}_{t-1})\\\\\n",
    "s_t &= s_{t-1} + \\dot{s_t}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f35e516",
   "metadata": {},
   "source": [
    "In the cell below, the ```@model``` macro and the `meta` blocks are used to define the probabilistic model and the approximation methods for the nonlinear state-transition functions, respectively. In addition, the beliefs over the future states (up to T steps ahead) of the agent is included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037c0d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@model function mountain_car(; T, Fg, Fa, Ff, engine_force_limit)\n",
    "    \n",
    "    # Transition function modeling transition due to gravity and friction\n",
    "    g = (s_t_min::AbstractVector) -> begin \n",
    "        s_t = similar(s_t_min) # Next state\n",
    "        s_t[2] = s_t_min[2] + Fg(s_t_min[1]) + Ff(s_t_min[2]) # Update velocity\n",
    "        s_t[1] = s_t_min[1] + s_t[2] # Update position\n",
    "        return s_t\n",
    "    end\n",
    "    \n",
    "    # Function for modeling engine control\n",
    "    h = (u::AbstractVector) -> [0.0, Fa(u[1])] \n",
    "    \n",
    "    # Inverse engine force, from change in state to corresponding engine force\n",
    "    h_inv = (delta_s_dot::AbstractVector) -> [atanh(clamp(delta_s_dot[2], -engine_force_limit+1e-3, engine_force_limit-1e-3)/engine_force_limit)] \n",
    "    \n",
    "    # Internal model perameters\n",
    "    Gamma = 1e4*diageye(2) # Transition precision\n",
    "    Theta = 1e-4*diageye(2) # Observation variance\n",
    "    \n",
    "    m_s_t_min = datavar(Vector{Float64})\n",
    "    V_s_t_min = datavar(Matrix{Float64})\n",
    "\n",
    "    s_t_min ~ MvNormal(mean = m_s_t_min, cov = V_s_t_min)\n",
    "    s_k_min = s_t_min\n",
    "    \n",
    "    m_u = datavar(Vector{Float64}, T)\n",
    "    V_u = datavar(Matrix{Float64}, T)\n",
    "    \n",
    "    m_x = datavar(Vector{Float64}, T)\n",
    "    V_x = datavar(Matrix{Float64}, T)\n",
    "    \n",
    "    u = randomvar(T)\n",
    "    s = randomvar(T)\n",
    "    x = randomvar(T)\n",
    "    \n",
    "    u_h_k = randomvar(T)\n",
    "    s_g_k = randomvar(T)\n",
    "    u_s_sum = randomvar(T)\n",
    "    \n",
    "    for k in 1:T\n",
    "        u[k] ~ MvNormal(mean = m_u[k], cov = V_u[k])\n",
    "        u_h_k[k] ~ h(u[k]) where { meta = DeltaMeta(method = Linearization(), inverse = h_inv) }\n",
    "        s_g_k[k] ~ g(s_k_min) where { meta = DeltaMeta(method = Linearization()) }\n",
    "        u_s_sum[k] ~ s_g_k[k] + u_h_k[k]\n",
    "        s[k] ~ MvNormal(mean = u_s_sum[k], precision = Gamma)\n",
    "        x[k] ~ MvNormal(mean = s[k], cov = Theta)\n",
    "        x[k] ~ MvNormal(mean = m_x[k], cov = V_x[k]) # goal\n",
    "        s_k_min = s[k]\n",
    "    end\n",
    "    \n",
    "    return (s, )\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7675b236",
   "metadata": {},
   "source": [
    "After specifying the generative model, let's create an Active Inference(AI) agent for the car. \n",
    "Technically, the agent goes through three phases: **Act-Execute-Observe**, **Infer** and **Slide**.\n",
    "1. **Act-Execute-Observe**: \n",
    "    In this phase, the agent performs an action onto the environment at time $t$ and gets $T$ observations in exchange. These observations are basically the prediction of the agent on how the environment evolves over the next $T$ time step. \n",
    "2. **Infer**:\n",
    "    After receiving observations, the agent starts updating its internal probabilistic model by doing inference. Particularly, it finds the posterior distributions over the state $s_t$ and control $u_t$, i.e. $p(s_t\\mid x_t)$ and $p(u_t\\mid x_t)$.\n",
    "3. **Slide**:\n",
    "    After updating its internal belief, the agent moves to the next time step and uses the inferred action $u_t$ in the previous time step to interact with the environment.  \n",
    "\n",
    "In the cell below, we create the agent through the `create_agent` function, which includes `infer`, `act`, `slide` and `future` functions:\n",
    "- The `act` function selects the next action based on the inferred policy. On the other hand, the `future` function predicts the next $T$ positions based on the current action. These two function implement the **Act-Execute-Observe** phase.\n",
    "- The `infer` function infers the policy (which is a set of actions for the next $T$ time steps) and the agent's state using the agent internal model. This function implements the **Infer** phase.\n",
    "- The `slide` function implements the **Slide** phase, which moves the agent internal model to the next time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b9d130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use some private functionality from ReactiveMP, \n",
    "# in the future we should expose a proper API for this\n",
    "import RxInfer.ReactiveMP: getrecent, messageout\n",
    "\n",
    "function create_agent(; T = 20, Fg, Fa, Ff, engine_force_limit, x_target, initial_position, initial_velocity)\n",
    "    Epsilon = fill(huge, 1, 1)                # Control prior variance\n",
    "    m_u = Vector{Float64}[ [ 0.0] for k=1:T ] # Set control priors\n",
    "    V_u = Matrix{Float64}[ Epsilon for k=1:T ]\n",
    "\n",
    "    Sigma    = 1e-4*diageye(2) # Goal prior variance\n",
    "    m_x      = [zeros(2) for k=1:T]\n",
    "    V_x      = [huge*diageye(2) for k=1:T]\n",
    "    V_x[end] = Sigma # Set prior to reach goal at t=T\n",
    "\n",
    "    # Set initial brain state prior\n",
    "    m_s_t_min = [initial_position, initial_velocity] \n",
    "    V_s_t_min = tiny * diageye(2)\n",
    "    \n",
    "    # Set current inference results\n",
    "    result = nothing\n",
    "\n",
    "    # The `infer` function is the heart of the agent\n",
    "    # It calls the `RxInfer.inference` function to perform Bayesian inference by message passing\n",
    "    infer = (upsilon_t::Float64, y_hat_t::Vector{Float64}) -> begin\n",
    "        m_u[1] = [ upsilon_t ] # Register action with the generative model\n",
    "        V_u[1] = fill(tiny, 1, 1) # Clamp control prior to performed action\n",
    "\n",
    "        m_x[1] = y_hat_t # Register observation with the generative model\n",
    "        V_x[1] = tiny*diageye(2) # Clamp goal prior to observation\n",
    "\n",
    "        data = Dict(:m_u       => m_u, \n",
    "                    :V_u       => V_u, \n",
    "                    :m_x       => m_x, \n",
    "                    :V_x       => V_x,\n",
    "                    :m_s_t_min => m_s_t_min,\n",
    "                    :V_s_t_min => V_s_t_min)\n",
    "        \n",
    "        model  = mountain_car(; T = T, Fg = Fg, Fa = Fa, Ff = Ff, engine_force_limit = engine_force_limit) \n",
    "        result = inference(model = model, data = data)\n",
    "    end\n",
    "    \n",
    "    # The `act` function returns the inferred best possible action\n",
    "    act = () -> begin\n",
    "        if result !== nothing\n",
    "            return mode(result.posteriors[:u][2])[1]\n",
    "        else\n",
    "            return 0.0 # Without inference result we return some 'random' action\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # The `future` function returns the inferred future states\n",
    "    future = () -> begin \n",
    "        if result !== nothing \n",
    "            return getindex.(mode.(result.posteriors[:s]), 1)\n",
    "        else\n",
    "            return zeros(T)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # The `slide` function modifies the `(m_s_t_min, V_s_t_min)` for the next step\n",
    "    # and shifts (or slides) the array of future goals `(m_x, V_x)` and inferred actions `(m_u, V_u)`\n",
    "    slide = () -> begin\n",
    "        (s, ) = result.returnval\n",
    "        \n",
    "        slide_msg_idx = 3 # This index is model dependend\n",
    "        (m_s_t_min, V_s_t_min) = mean_cov(getrecent(messageout(s[2], slide_msg_idx)))\n",
    "\n",
    "        m_u = circshift(m_u, -1)\n",
    "        m_u[end] = [0.0]\n",
    "        V_u = circshift(V_u, -1)\n",
    "        V_u[end] = Epsilon\n",
    "\n",
    "        m_x = circshift(m_x, -1)\n",
    "        m_x[end] = x_target\n",
    "        V_x = circshift(V_x, -1)\n",
    "        V_x[end] = Sigma\n",
    "    end\n",
    "\n",
    "    return (infer, act, slide, future)    \n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87bd2fb8",
   "metadata": {},
   "source": [
    "Now it's time to see if we can help our friends arrive at the camping site by midnight?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df06c331",
   "metadata": {},
   "outputs": [],
   "source": [
    "(execute_ai, observe_ai) = create_world(\n",
    "    Fg = Fg, Ff = Ff, Fa = Fa, \n",
    "    initial_position = initial_position, \n",
    "    initial_velocity = initial_velocity\n",
    ") # Let there be a world\n",
    "\n",
    "T_ai = 50\n",
    "\n",
    "(infer_ai, act_ai, slide_ai, future_ai) = create_agent(; # Let there be an agent\n",
    "    T  = T_ai, \n",
    "    Fa = Fa,\n",
    "    Fg = Fg, \n",
    "    Ff = Ff, \n",
    "    engine_force_limit = engine_force_limit,\n",
    "    x_target = x_target,\n",
    "    initial_position = initial_position,\n",
    "    initial_velocity = initial_velocity\n",
    ") \n",
    "\n",
    "N_ai = 100\n",
    "\n",
    "# Step through experimental protocol\n",
    "agent_a = Vector{Float64}(undef, N_ai) # Actions\n",
    "agent_f = Vector{Vector{Float64}}(undef, N_ai) # Predicted future\n",
    "agent_x = Vector{Vector{Float64}}(undef, N_ai) # Observations\n",
    "\n",
    "for t=1:N_ai\n",
    "    agent_a[t] = act_ai()            # Invoke an action from the agent\n",
    "    agent_f[t] = future_ai()         # Fetch the predicted future states\n",
    "    execute_ai(agent_a[t])           # The action influences hidden external states\n",
    "    agent_x[t] = observe_ai()        # Observe the current environmental outcome (update p)\n",
    "    infer_ai(agent_a[t], agent_x[t]) # Infer beliefs from current model state (update q)\n",
    "    slide_ai()                       # Prepare for next iteration\n",
    "end\n",
    "\n",
    "animation_ai = @animate for i in 1:N_ai\n",
    "    # pls - plot landscape\n",
    "    pls = plot(valley_x, valley_y, title = \"Active inference results\", label = \"Landscape\", color = \"black\")\n",
    "    pls = scatter!(pls, [agent_x[i][1]], [height(agent_x[i][1])], label=\"car\")\n",
    "    pls = scatter!(pls, [x_target[1]], [height(x_target[1])], label=\"goal\")   \n",
    "    pls = scatter!(pls, agent_f[i], height.(agent_f[i]), label = \"Predicted future\", alpha = map(i -> 0.5 / i, 1:T_ai))\n",
    "    \n",
    "    # pef - plot engine force\n",
    "    pef = plot(Fa.(agent_a[1:i]), title = \"Engine force (agents actions)\", xlim = (0, N_ai), ylim = (-0.05, 0.05))\n",
    "    \n",
    "    plot(pls, pef, size = (800, 400))\n",
    "end\n",
    "    \n",
    "# The animation is saved and displayed as markdown picture for the automatic HTML generation\n",
    "gif(animation_ai, \"../pics/ai-mountain-car-ai.gif\", fps = 24, show_msg = false); "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5cecb75",
   "metadata": {},
   "source": [
    "![](../pics/ai-mountain-car-ai.gif)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9eda2db4",
   "metadata": {},
   "source": [
    "Voila! The car now is able to reach the camping site with a smart strategy.\n",
    "\n",
    "The left figure shows the agent reached its goal by swinging and the right one shows the corresponding engine force. As we can see, at the beginning the agent tried to reach the goal directly (with full engine force) but after some trials it realized that's not possible. Since the agent looks ahead for 50 time steps, it has enough time to explore other policies, helping it learn to move back to get more momentum to reach the goal.\n",
    "\n",
    "Now our friends can enjoy their trip at the camping site!. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "67d68a45",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "We refer reader to the Thijs van de Laar (2019) \"Simulating active inference processes by message passing\" original paper with more in-depth overview and explanation of the active inference agent implementation by message passing.\n",
    "The original environment/task description is from Ueltzhoeffer (2017) \"Deep active inference\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.1",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing People’s Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `c:\\Users\\tvdlaar\\.julia\\dev\\RxInfer\\examples`\n",
      "┌ Warning: The project dependencies or compat requirements have changed since the manifest was last resolved.\n",
      "│ It is recommended to `Pkg.resolve()` or consider `Pkg.update()` if necessary.\n",
      "└ @ Pkg.API C:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.8\\Pkg\\src\\API.jl:1532\n"
     ]
    }
   ],
   "source": [
    "# Activate local environment, see `Project.toml`\n",
    "import Pkg; Pkg.activate(\"..\"); Pkg.instantiate(); "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this demo is to demonstrate the use of the `@node` and `@rule` macros, which allow the user to define custom factor nodes and associated update rules respectively. We will introduce these macros in the context of a root cause analysis on a student's test results. This demo is inspired by Chapter 2 of \"Model-Based Machine Learning\" by Winn et al.\n",
    "\n",
    "## Problem Statement\n",
    "We consider a student who takes a test that consists of three questions. Answering each question correctly requires a combination of skill and attitude. More precisely, has the student studied for the test, and have they partied the night before?\n",
    "\n",
    "We model the result for question $i$ as a continuous variable $r_i\\in[0,1]$, and skill/attitude as a binary variable $s_i \\in \\{0, 1\\}$, where $s_1$ represents whether the student has partied, and $s_2$ and $s_3$ represent whether the student has studied the chapters for the corresponding questions.\n",
    "\n",
    "We assume the following logic:\n",
    "- If the student is alert (has not partied), then they will score on the first question;\n",
    "- If the student is alert or has studied chapter two, then they will score on question two;\n",
    "- If the student can answer question two and has studied chapter three, then they will score on question three.\n",
    "\n",
    "## Generative Model Definition\n",
    "To model the probability for correct answers, we assume a latent state variable $t_i \\in \\{0,1\\}$. The dependencies between the variables can then be modeled by the following Bayesian network:\n",
    "\n",
    "```\n",
    "(s_1)   (s_2)   (s_3)\n",
    "  |       |       |\n",
    "  v       v       v\n",
    "(t_1)-->(t_2)-->(t_3)\n",
    "  |       |       |\n",
    "  v       v       v\n",
    "(r_1)   (r_2)   (r_3)\n",
    "```\n",
    "\n",
    "As prior beliefs, we assume that a student is equally likely to study/party or not:\n",
    "$$s_i \\sim Ber(0.5)\\,,$$\n",
    "for all $i$. Next, we model the domain logic as\n",
    "$$\\begin{aligned}\n",
    "  t_1 &= ¬s_1\\\\\n",
    "  t_2 &= t_1 ∨ s_2\\\\\n",
    "  t_3 &= t_2 ∧ s_3\\,.\n",
    "\\end{aligned}$$\n",
    "For the scoring results we might not have a specific forward model in mind. However, we can define a backward mapping, from continuous results to discrete latent variables, as \n",
    "$$t_i \\sim Ber(s_i)\\,,$$\n",
    "for all $i$.\n",
    "\n",
    "## Custom Nodes and Rules\n",
    "\n",
    "The backward mapping from results to latents is quite specific to our application. Moreover, it does not define a proper generative forward model. In order to still define a full generative model for our application, we can define a custom `Score` node and define an update rule that implements the backward mapping from scores to latents as a message.\n",
    "\n",
    "In RxInfer, the `@node` macro defines a factor node. This macro accepts the new node type, an indicator for a stochastic or deterministic relationship, and a list of interfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using RxInfer, Random\n",
    "\n",
    "# Create Score node\n",
    "struct Score end\n",
    "\n",
    "@node Score Stochastic [out, in]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define the backward mapping as a sum-product message through the `@rule` macro. This macro accepts the node type, the (outbound) interface on which the message is sent, any relevant constraints, and the message/distribution types on the remaining (inbound) interfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding update rule for the Score node\n",
    "@rule Score(:in, Marginalisation) (q_out::PointMass,) = begin     \n",
    "    return Bernoulli(mean(q_out))\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Model Specification\n",
    "We can now build the full generative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphPPL.jl exports the `@model` macro for model specification\n",
    "# It accepts a regular Julia function and builds an FFG under the hood\n",
    "@model function skill_model()\n",
    "    s = randomvar(3)\n",
    "    t = randomvar(3)\n",
    "    r = datavar(Float64, 3)\n",
    "\n",
    "    # Priors\n",
    "    for i=1:3\n",
    "        s[i] ~ Bernoulli(0.5)\n",
    "    end\n",
    "\n",
    "    # Domain logic\n",
    "    t[1] ~ ¬s[1]\n",
    "    t[2] ~ t[1] || s[2]\n",
    "    t[3] ~ t[2] && s[3]\n",
    "    \n",
    "    # Results\n",
    "    for i=1:3\n",
    "        r[i] ~ Score(t[i])\n",
    "    end\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Specification\n",
    "Let us assume that a student scored very low on all questions and set up and execute an inference algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inference results:\n",
       "  Posteriors       | available for (s, t)\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results = [0.1, 0.1, 0.1]\n",
    "inference_result = infer(\n",
    "    model = skill_model(),\n",
    "    data  = (r = test_results, )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Tuple{Float64}}:\n",
       " (0.9872448979591837,)\n",
       " (0.06377551020408162,)\n",
       " (0.4719387755102041,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the results\n",
    "map(params, inference_result.posteriors[:s])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results suggest that this particular student was very likely out on the town last night."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.9.1",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

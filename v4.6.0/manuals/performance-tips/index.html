<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Performance Tips · RxInfer.jl</title><meta name="title" content="Performance Tips · RxInfer.jl"/><meta property="og:title" content="Performance Tips · RxInfer.jl"/><meta property="twitter:title" content="Performance Tips · RxInfer.jl"/><meta name="description" content="Julia package for automated Bayesian inference on a factor graph with reactive message passing"/><meta property="og:description" content="Julia package for automated Bayesian inference on a factor graph with reactive message passing"/><meta property="twitter:description" content="Julia package for automated Bayesian inference on a factor graph with reactive message passing"/><meta property="og:url" content="https://docs.rxinfer.com/stable/manuals/performance-tips/"/><meta property="twitter:url" content="https://docs.rxinfer.com/stable/manuals/performance-tips/"/><link rel="canonical" href="https://docs.rxinfer.com/stable/manuals/performance-tips/"/><script async src="https://www.googletagmanager.com/gtag/js?id=G-X4PH160GMF"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-X4PH160GMF', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/theme.css" rel="stylesheet" type="text/css"/><link href="../../assets/header.css" rel="stylesheet" type="text/css"/><script src="../../assets/header.js"></script><script src="../../assets/chat.js"></script><link href="../../assets/favicon.ico" rel="icon" type="image/x-icon"/><meta name="keywords" content="Julia, Bayesian inference, factor graph, message passing, probabilistic programming, reactive programming, RxInfer">
<link rel="sitemap" type="application/xml" title="Sitemap" href="https://docs.rxinfer.com/stable/sitemap.xml"></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.svg" alt="RxInfer.jl logo"/><img class="docs-dark-only" src="../../assets/logo-dark.svg" alt="RxInfer.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">User guide</span><ul><li><a class="tocitem" href="../getting-started/">Getting started</a></li><li><a class="tocitem" href="../comparison/">RxInfer.jl vs. Others</a></li><li><a class="tocitem" href="../how-to-use-rxinfer-from-python/">Using RxInfer from Python</a></li><li><a class="tocitem" href="../model-specification/">Model specification</a></li><li><a class="tocitem" href="../constraints-specification/">Constraints specification</a></li><li><a class="tocitem" href="../meta-specification/">Meta specification</a></li><li><input class="collapse-toggle" id="menuitem-2-7" type="checkbox"/><label class="tocitem" for="menuitem-2-7"><span class="docs-label">Inference specification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../inference/overview/">Overview</a></li><li><a class="tocitem" href="../inference/static/">Static inference</a></li><li><a class="tocitem" href="../inference/streamlined/">Streamline inference</a></li><li><a class="tocitem" href="../inference/static-vs-streamlined/">Static vs. Streamlined</a></li><li><a class="tocitem" href="../inference/initialization/">Initialization</a></li><li><a class="tocitem" href="../inference/autoupdates/">Auto-updates</a></li><li><a class="tocitem" href="../inference/delta-node/">Deterministic nodes</a></li><li><a class="tocitem" href="../inference/nonconjugate/">Non-conjugate inference</a></li><li><a class="tocitem" href="../inference/undefinedrules/">Undefined message update rules</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-2-8" type="checkbox"/><label class="tocitem" for="menuitem-2-8"><span class="docs-label">Inference customization</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../customization/custom-node/">Defining a custom node and rules</a></li><li><a class="tocitem" href="../customization/postprocess/">Inference results postprocessing</a></li></ul></li><li class="is-active"><a class="tocitem" href>Performance Tips</a><ul class="internal"><li><a class="tocitem" href="#Julia-Compilation-Latency"><span>Julia Compilation Latency</span></a></li><li><a class="tocitem" href="#Model-Structure-Optimization"><span>Model Structure Optimization</span></a></li><li><a class="tocitem" href="#Inference-Procedure-Optimization"><span>Inference Procedure Optimization</span></a></li><li><a class="tocitem" href="#Getting-Help"><span>Getting Help</span></a></li></ul></li><li><a class="tocitem" href="../faq/">FAQ</a></li><li><a class="tocitem" href="../debugging/">Debugging</a></li><li><a class="tocitem" href="../session_summary/">Session summary</a></li><li><a class="tocitem" href="../telemetry/">Sharing sessions &amp; telemetry</a></li><li><a class="tocitem" href="../migration-guide-v2-v3/">Migration from v2 to v3</a></li><li><input class="collapse-toggle" id="menuitem-2-15" type="checkbox"/><label class="tocitem" for="menuitem-2-15"><span class="docs-label">Sharp bits of RxInfer</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../sharpbits/overview/">Overview</a></li><li><a class="tocitem" href="../sharpbits/rule-not-found/">Rule Not Found Error</a></li><li><a class="tocitem" href="../sharpbits/stack-overflow-inference/">Stack Overflow in Message Computations</a></li><li><a class="tocitem" href="../sharpbits/usage-colon-equality/">Using <code>=</code> instead of <code>:=</code> for deterministic nodes</a></li></ul></li></ul></li><li><span class="tocitem">Library</span><ul><li><a class="tocitem" href="../../library/model-construction/">Model construction</a></li><li><a class="tocitem" href="../../library/bethe-free-energy/">Bethe Free Energy</a></li><li><a class="tocitem" href="../../library/functional-forms/">Functional form constraints</a></li><li><a class="tocitem" href="../../library/exported-methods/">Exported methods</a></li></ul></li><li><a class="tocitem" href="../../examples/overview/">Examples</a></li><li><span class="tocitem">Contributing</span><ul><li><a class="tocitem" href="../../contributing/guide/">Contribution guide</a></li><li><a class="tocitem" href="../../contributing/guidelines/">Contribution guidelines</a></li><li><a class="tocitem" href="../../contributing/new-documentation/">Contributing to the documentation</a></li><li><a class="tocitem" href="../../contributing/new-example/">Contributing to the examples</a></li><li><a class="tocitem" href="../../contributing/new-release/">Publishing a new release</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">User guide</a></li><li class="is-active"><a href>Performance Tips</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Performance Tips</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/ReactiveBayes/RxInfer.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/ReactiveBayes/RxInfer.jl/blob/main/docs/src/manuals/performance-tips.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="user-guide-performance-tips"><a class="docs-heading-anchor" href="#user-guide-performance-tips">Performance Tips</a><a id="user-guide-performance-tips-1"></a><a class="docs-heading-anchor-permalink" href="#user-guide-performance-tips" title="Permalink"></a></h1><p>This section provides practical advice and best practices for optimizing the performance of your RxInfer models. Following these guidelines can significantly improve inference speed and memory efficiency.</p><div class="admonition is-info" id="Note-4399f243a1109170"><header class="admonition-header">Note<a class="admonition-anchor" href="#Note-4399f243a1109170" title="Permalink"></a></header><div class="admonition-body"><p>Before diving into RxInfer-specific optimizations, we strongly recommend reading Julia&#39;s official <a href="https://docs.julialang.org/en/v1/manual/performance-tips/">Performance Tips</a> guide. Many performance improvements come from following Julia&#39;s general best practices, such as avoiding global variables, using type stability, and minimizing allocations. The tips in this section build upon those fundamental principles.</p></div></div><h2 id="Julia-Compilation-Latency"><a class="docs-heading-anchor" href="#Julia-Compilation-Latency">Julia Compilation Latency</a><a id="Julia-Compilation-Latency-1"></a><a class="docs-heading-anchor-permalink" href="#Julia-Compilation-Latency" title="Permalink"></a></h2><p>Julia uses <strong>Just-In-Time (JIT)</strong> compilation. The <strong>first time</strong> you run a model and inference procedure, Julia compiles the specialized machine code. This can cause noticeable delays <strong>only once</strong>. Afterward, execution becomes much faster. This might be especially problematic for models and factor nodes that accept a dynamic number of arguments. Such nodes include mixture nodes (where the number of components is only known at compilation time) as well as deterministic nodes representing non-linear transformations (since those transformations can be arbitrary, their signature is only known at compilation time).</p><p><strong>Tips:</strong> </p><ul><li>Don&#39;t worry about long first-run times during development — focus on steady-state performance.</li><li>Use the <code>@time</code> macro from Julia to investigate the time spent on compilation and execution.</li></ul><h2 id="Model-Structure-Optimization"><a class="docs-heading-anchor" href="#Model-Structure-Optimization">Model Structure Optimization</a><a id="Model-Structure-Optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Model-Structure-Optimization" title="Permalink"></a></h2><p>RxInfer is designed for fast inference on factor graphs and leverages the model structure to optimize the inference procedure. However, it is always possible to create a huge model with complex dependencies between variables and make inference slow with RxInfer. </p><p><strong>General guidelines for model structure optimization:</strong></p><h3 id="Choose-Appropriate-Parametrization-for-Your-Nodes"><a class="docs-heading-anchor" href="#Choose-Appropriate-Parametrization-for-Your-Nodes">Choose Appropriate Parametrization for Your Nodes</a><a id="Choose-Appropriate-Parametrization-for-Your-Nodes-1"></a><a class="docs-heading-anchor-permalink" href="#Choose-Appropriate-Parametrization-for-Your-Nodes" title="Permalink"></a></h3><p>While confusing at first glance, the choice of parametrization for your nodes can have a significant impact on the performance of the inference procedure. For this reason, RxInfer allows you to choose, for example, between <code>NormalMeanPrecision</code> and <code>NormalMeanVariance</code> parametrizations for <code>Normal</code> nodes. Or, you can choose between <code>MvNormalMeanPrecision</code> and <code>MvNormalMeanScalePrecision</code> parametrizations for <code>MvNormal</code> nodes. The difference between these parametrizations is that the former needs to store the entire precision matrix, while the latter uses a single number to store the scale of the diagonal of the precision matrix.</p><h3 id="Use-Conjugate-Pairs"><a class="docs-heading-anchor" href="#Use-Conjugate-Pairs">Use Conjugate Pairs</a><a id="Use-Conjugate-Pairs-1"></a><a class="docs-heading-anchor-permalink" href="#Use-Conjugate-Pairs" title="Permalink"></a></h3><p><a href="https://en.wikipedia.org/wiki/Conjugate_prior">Conjugate pairs</a> enable analytical message updates. For example, a <code>Gamma</code> prior is appropriate for a <code>NormalMeanPrecision</code> node, but an <code>InverseGamma</code> is not. Conversely, an <code>InverseGamma</code> prior is appropriate for a <code>NormalMeanVariance</code> node, but a <code>Gamma</code> is not. Another example is that a <code>Beta</code> prior is appropriate for a <code>Bernoulli</code> node, but a <code>Binomial</code> is not. A <code>Wishart</code> prior is appropriate for an <code>MvNormalMeanPrecision</code> node, and an <code>InverseWishart</code> is appropriate for an <code>MvNormalMeanCovariance</code> node.  Note that the conjugacy also depends on the local factorization of your model. If you place priors on both the mean and the precision in <code>MvNormalMeanPrecision</code>, you must enforce independence (e.g., <code>q(μ,Λ)=q(μ)q(Λ)</code>) to make the model conditionally conjugate.</p><h3 id="Be-Aware-of-the-Computational-Overhead-of-Deterministic-Nodes"><a class="docs-heading-anchor" href="#Be-Aware-of-the-Computational-Overhead-of-Deterministic-Nodes">Be Aware of the Computational Overhead of Deterministic Nodes</a><a id="Be-Aware-of-the-Computational-Overhead-of-Deterministic-Nodes-1"></a><a class="docs-heading-anchor-permalink" href="#Be-Aware-of-the-Computational-Overhead-of-Deterministic-Nodes" title="Permalink"></a></h3><p>Each deterministic node adds computational overhead and requires approximation method specification. Read more about approximation methods in the <a href="../inference/delta-node/#delta-node-manual">Deterministic nodes</a> section. In some situations, however, it is possible to use specialized factor nodes instead of deterministic nodes. For example, the <code>SoftDot</code> node is a specialized factor node for computing the dot product of two vectors where the result is passed to a <code>Normal</code> node. Using <code>SoftDot</code> directly instead of <code>Normal(mean = dot(...), ...)</code> can significantly improve both the performance and accuracy of the inference procedure. Similar applies to <code>ContinuousTransition</code> node.</p><p>If a specialized node is not available, you can either <a href="../customization/custom-node/#create-node">create one yourself</a> or choose an appropriate approximation method for the deterministic node. For example, if all inputs to the non-linear transformation are known to be Gaussian, the fastest approximation method is probably <code>Linearization</code>. However, it requires the function to be differentiable and &quot;nice&quot; enough. More computationally expensive methods, such as <code>Unscented</code> or <code>CVIProjection</code>, are more robust and can be used in more general cases. We also suggest you to check <a href="../inference/undefinedrules/#inference-undefinedrules-fusedelta">Fusing deterministic transformations with stochastic nodes</a> example that provides additional tricks.</p><h3 id="Smoothing-vs.-Filtering"><a class="docs-heading-anchor" href="#Smoothing-vs.-Filtering">Smoothing vs. Filtering</a><a id="Smoothing-vs.-Filtering-1"></a><a class="docs-heading-anchor-permalink" href="#Smoothing-vs.-Filtering" title="Permalink"></a></h3><p>It might be appropriate to convert your model from operating on the whole dataset (smoothing) to operating on one observation at a time (filtering). Read more about smoothing in the <a href="../inference/static/#manual-static-inference">Static Inference</a> section and about filtering in the <a href="../inference/streamlined/#manual-online-inference">Online Inference</a> section. It is also possible to combine both approaches and process data in batches.</p><h2 id="Inference-Procedure-Optimization"><a class="docs-heading-anchor" href="#Inference-Procedure-Optimization">Inference Procedure Optimization</a><a id="Inference-Procedure-Optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Inference-Procedure-Optimization" title="Permalink"></a></h2><p>The <a href="../inference/overview/#RxInfer.infer"><code>infer</code></a> function is the main entry point for inference in RxInfer.jl. It is a wrapper around the inference procedure and allows you to specify the inference algorithm, the number of iterations, the initial values for the parameters, and more. The default parameters are chosen to be a good compromise between speed and accuracy. However, in some situations, it is possible to improve the performance of the inference procedure by tuning the parameters.</p><h3 id="Use-free_energy-Float64-Instead-of-free_energy-true"><a class="docs-heading-anchor" href="#Use-free_energy-Float64-Instead-of-free_energy-true">Use <code>free_energy = Float64</code> Instead of <code>free_energy = true</code></a><a id="Use-free_energy-Float64-Instead-of-free_energy-true-1"></a><a class="docs-heading-anchor-permalink" href="#Use-free_energy-Float64-Instead-of-free_energy-true" title="Permalink"></a></h3><p>By default, when computing free energy values, they are stored as an abstract type <code>Real</code> and are converted to <code>Float64</code> only when they are returned. This can be a significant overhead (read Julia&#39;s <a href="https://docs.julialang.org/en/v1/manual/performance-tips/#Avoid-unnecessary-type-conversions">Performance Tips</a>), especially for large models. The reason for this choice is that in this case, the inference procedure can be auto-differentiated where free energy values serve as the objective function. If you do not plan to auto-differentiate the inference procedure, you can set <code>free_energy = Float64</code> to avoid the overhead of type conversions.</p><h3 id="Be-Aware-of-the-Computational-Overhead-of-the-limit_stack_depth-Option"><a class="docs-heading-anchor" href="#Be-Aware-of-the-Computational-Overhead-of-the-limit_stack_depth-Option">Be Aware of the Computational Overhead of the <code>limit_stack_depth</code> Option</a><a id="Be-Aware-of-the-Computational-Overhead-of-the-limit_stack_depth-Option-1"></a><a class="docs-heading-anchor-permalink" href="#Be-Aware-of-the-Computational-Overhead-of-the-limit_stack_depth-Option" title="Permalink"></a></h3><p>RxInfer provides a <code>limit_stack_depth</code> option to limit the depth of the stack of the inference procedure, which is explained in the <a href="../sharpbits/stack-overflow-inference/#stack-overflow-inference">Stack Overflow during inference</a> section. This can be useful to avoid stack overflows, but it can also significantly degrade the performance of the inference procedure. The larger the value, the less the performance is degraded. You can tune the value based on the size of your model as well as your computer. The optimal value differs for different models and computers.</p><h2 id="Getting-Help"><a class="docs-heading-anchor" href="#Getting-Help">Getting Help</a><a id="Getting-Help-1"></a><a class="docs-heading-anchor-permalink" href="#Getting-Help" title="Permalink"></a></h2><p>If you encounter performance issues:</p><ol><li><strong>Check the documentation</strong>: Review relevant sections for optimization tips</li><li><strong>Use the community</strong>: Open discussions on GitHub for specific issues</li><li><strong>Profile your code</strong>: Use Julia&#39;s profiling tools to identify bottlenecks</li><li><strong>Start simple</strong>: Build complexity gradually to identify performance issues</li></ol><script type="module">import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.esm.min.mjs';
mermaid.initialize({
    startOnLoad: true,
    theme: "neutral"
});
</script></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../customization/postprocess/">« Inference results postprocessing</a><a class="docs-footer-nextpage" href="../faq/">FAQ »</a><div class="flexbox-break"></div><p class="footer-message">Created in <a href="https://biaslab.github.io/">BIASlab</a>, maintained by <a href="https://github.com/ReactiveBayes">ReactiveBayes</a>, powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Wednesday 24 September 2025 08:43">Wednesday 24 September 2025</span>. Using Julia version 1.11.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
